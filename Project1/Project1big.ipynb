{
  "cells": [
    {
      "metadata": {
        "collapsed": true
      },
      "cell_type": "markdown",
      "source": "# Α.Στοιχεία Ομάδας"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#      Ομάδα Α5\n\n   Ορφανουδάκης Φίλιππος (ΑΜ:03113140)\n\n   Μπακούρος Αριστείδης (ΑΜ:03113138)"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Β.Εισαγωγή του Dataset"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Το συγκεκριμένο dataset(ISOLET) προήλθε με τον εξής τρόπο: 150 άνθρωποι (αντικείμενα του δείγματος) κλήθηκαν να πουν κάθε γράμμα της αγγλικής αλφαβήτου από 2 φορές. Έτσι, για κάθε αντικείμενο έχουμε 52 (26 γράμματα επί 2 φορές την αλφαβήτα ο καθένας) rows στο dataset μας και 617 attributes συν 1 για την κλάση. Οι κλάσεις παίρνουν τιμές από 1 ως και το 26 και αντιπροσωπεύουν ένα γράμμα της αλφαβήτου(1 για το A, 2 για το B, 3 για το C κ.ο.κ). Αρχικά ήταν χωρισμένοι σε 5 ομάδες των 30 αντικειμένων, η τελευταία ομάδα ήταν το test set, αλλά το αλλάζουμε στη συνέχεια καθώς η άσκηση ζητά 30% test set. Τα attributes αφορούν στοιχεία του γύρω περιβάλλοντος καθώς και ηχητικά χαραχτηριστικά πριν, μετά και κατά τη διάρκεια της προφοράς κάθε γράμματος. Στόχος είναι να προβλεφθεί με όσο το δυνατόν μεγαλύτερη συνέπεια ποιο πράγμα πρόκειται να ειπωθεί βάσει όλων αυτών των χαραχτηριστικών. Δεν υπάρχουν απουσιάζουσες τιμές, έχουν χαθεί μόνο 3 παραδείγματα πιθανώς για προβλήματα στην ηχογράφηση και όλα τα χαραχτηριστικά είναι διατεταγμένα και είναι πραγματικοί αριθμοί από το -1 έως το 1.\n"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Ξεκινάμε κάνοντας upgrade τις βιβλιοθήκες που θα χρειαστούμε."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "!pip install --upgrade pip #upgrade pip package installer\n!pip install scikit-learn --upgrade #upgrade scikit-learn package\n!pip install numpy --upgrade #upgrade numpy package\n!pip install pandas --upgrade #upgrade pandas package",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Requirement already up-to-date: pip in /home/nbuser/anaconda3_501/lib/python3.6/site-packages (18.1)\nRequirement already up-to-date: scikit-learn in /home/nbuser/anaconda3_501/lib/python3.6/site-packages (0.20.1)\nRequirement already satisfied, skipping upgrade: numpy>=1.8.2 in /home/nbuser/anaconda3_501/lib/python3.6/site-packages (from scikit-learn) (1.15.4)\nRequirement already satisfied, skipping upgrade: scipy>=0.13.3 in /home/nbuser/anaconda3_501/lib/python3.6/site-packages (from scikit-learn) (1.1.0)\nRequirement already up-to-date: numpy in /home/nbuser/anaconda3_501/lib/python3.6/site-packages (1.15.4)\nRequirement already up-to-date: pandas in /home/nbuser/anaconda3_501/lib/python3.6/site-packages (0.23.4)\nRequirement already satisfied, skipping upgrade: python-dateutil>=2.5.0 in /home/nbuser/anaconda3_501/lib/python3.6/site-packages (from pandas) (2.7.5)\nRequirement already satisfied, skipping upgrade: pytz>=2011k in /home/nbuser/anaconda3_501/lib/python3.6/site-packages (from pandas) (2018.7)\nRequirement already satisfied, skipping upgrade: numpy>=1.9.0 in /home/nbuser/anaconda3_501/lib/python3.6/site-packages (from pandas) (1.15.4)\nRequirement already satisfied, skipping upgrade: six>=1.5 in /home/nbuser/anaconda3_501/lib/python3.6/site-packages (from python-dateutil>=2.5.0->pandas) (1.11.0)\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Για να εισάγουμε το dataset μας κατεβάζουμε τα αρχεία isolet1+2+3+4.data και isolet5.data και τα ανεβάζουμε στο ιδιο directory με αυτό το notebook."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "!ls",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": "1.1_Classification.ipynb\t   Lab 6 Clustering exercise solution.ipynb\r\n1.2_Pima_Indians_exercise_A.ipynb  Lab 6 Clustering.ipynb\r\n1.3_Classification.ipynb\t   Lab 7 Text mining exercise.ipynb\r\n2.0 Classification.ipynb\t   Lab_7_Text_Mining.ipynb\r\n2.1_Classification.ipynb\t   mydoc.txt\r\naskhsh1_small-Copy.ipynb\t   pythoncode.py\r\naskhsh1_small.ipynb\t\t   sample.csv\r\naskhsh1_task_small.ipynb\t   Small.ipynb\r\nClassification_3.ipynb\t\t   Solution_1.2_Pima_Indians_exercise_Á.ipynb\r\ncrx.data\t\t\t   SOM_with_Somoclu.ipynb\r\ndoc.txt\t\t\t\t   task1clone.ipynb\r\nexample.csv\t\t\t   task1.ipynb\r\nfinal1.2.ipynb\t\t\t   task_big.ipynb\r\ngobig.ipynb\t\t\t   task.ipynb\r\nhepatitis.data\t\t\t   task_small.ipynb\r\nisolet1.data\t\t\t   tmp\r\nisolet5.data\t\t\t   Untitled.ipynb\r\nLab_2_Notebooks_intro.ipynb\t   wdbc.data\r\nLab_2_Python_Intro.ipynb\t   wdbc.names\r\nLab_6_Clustering_exercise.ipynb    wpbc.data\r\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Ακολουθεί μία πρώτη εμφάνιση των 2 αυτών datasets προτού γίνει το concatenation.Επειδή περιέχεται πληροφορία στην 1η γραμμή διαπιστώνουμε ότι δεν έχουμε headers."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import pandas as pd\n\ndf = pd.read_csv(\"isolet1.data\", header=None,na_values = [\"?\"])\ndf",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 3,
          "data": {
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>608</th>\n      <th>609</th>\n      <th>610</th>\n      <th>611</th>\n      <th>612</th>\n      <th>613</th>\n      <th>614</th>\n      <th>615</th>\n      <th>616</th>\n      <th>617</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-0.4394</td>\n      <td>-0.0930</td>\n      <td>0.1718</td>\n      <td>0.4620</td>\n      <td>0.6226</td>\n      <td>0.4704</td>\n      <td>0.3578</td>\n      <td>0.0478</td>\n      <td>-0.1184</td>\n      <td>-0.2310</td>\n      <td>...</td>\n      <td>0.4102</td>\n      <td>0.2052</td>\n      <td>0.3846</td>\n      <td>0.3590</td>\n      <td>0.5898</td>\n      <td>0.3334</td>\n      <td>0.6410</td>\n      <td>0.5898</td>\n      <td>-0.4872</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-0.4348</td>\n      <td>-0.1198</td>\n      <td>0.2474</td>\n      <td>0.4036</td>\n      <td>0.5026</td>\n      <td>0.6328</td>\n      <td>0.4948</td>\n      <td>0.0338</td>\n      <td>-0.0520</td>\n      <td>-0.1302</td>\n      <td>...</td>\n      <td>0.0000</td>\n      <td>0.2954</td>\n      <td>0.2046</td>\n      <td>0.4772</td>\n      <td>0.0454</td>\n      <td>0.2046</td>\n      <td>0.4318</td>\n      <td>0.4546</td>\n      <td>-0.0910</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-0.2330</td>\n      <td>0.2124</td>\n      <td>0.5014</td>\n      <td>0.5222</td>\n      <td>-0.3422</td>\n      <td>-0.5840</td>\n      <td>-0.7168</td>\n      <td>-0.6342</td>\n      <td>-0.8614</td>\n      <td>-0.8318</td>\n      <td>...</td>\n      <td>-0.1112</td>\n      <td>-0.0476</td>\n      <td>-0.1746</td>\n      <td>0.0318</td>\n      <td>-0.0476</td>\n      <td>0.1112</td>\n      <td>0.2540</td>\n      <td>0.1588</td>\n      <td>-0.4762</td>\n      <td>2.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-0.3808</td>\n      <td>-0.0096</td>\n      <td>0.2602</td>\n      <td>0.2554</td>\n      <td>-0.4290</td>\n      <td>-0.6746</td>\n      <td>-0.6868</td>\n      <td>-0.6650</td>\n      <td>-0.8410</td>\n      <td>-0.9614</td>\n      <td>...</td>\n      <td>-0.0504</td>\n      <td>-0.0360</td>\n      <td>-0.1224</td>\n      <td>0.1366</td>\n      <td>0.2950</td>\n      <td>0.0792</td>\n      <td>-0.0072</td>\n      <td>0.0936</td>\n      <td>-0.1510</td>\n      <td>2.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-0.3412</td>\n      <td>0.0946</td>\n      <td>0.6082</td>\n      <td>0.6216</td>\n      <td>-0.1622</td>\n      <td>-0.3784</td>\n      <td>-0.4324</td>\n      <td>-0.4358</td>\n      <td>-0.4966</td>\n      <td>-0.5406</td>\n      <td>...</td>\n      <td>0.1562</td>\n      <td>0.3124</td>\n      <td>0.2500</td>\n      <td>-0.0938</td>\n      <td>0.1562</td>\n      <td>0.3124</td>\n      <td>0.3124</td>\n      <td>0.2188</td>\n      <td>-0.2500</td>\n      <td>3.0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>-0.4634</td>\n      <td>0.0306</td>\n      <td>0.3546</td>\n      <td>0.4448</td>\n      <td>-0.1022</td>\n      <td>-0.4184</td>\n      <td>-0.6388</td>\n      <td>-0.4370</td>\n      <td>-0.4396</td>\n      <td>-0.6654</td>\n      <td>...</td>\n      <td>0.6626</td>\n      <td>0.7350</td>\n      <td>0.3734</td>\n      <td>0.6626</td>\n      <td>0.3012</td>\n      <td>0.1808</td>\n      <td>0.2290</td>\n      <td>0.6144</td>\n      <td>0.3254</td>\n      <td>3.0</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>-0.3364</td>\n      <td>-0.0102</td>\n      <td>0.2132</td>\n      <td>0.2018</td>\n      <td>-0.6146</td>\n      <td>-0.8380</td>\n      <td>-0.8130</td>\n      <td>-0.7240</td>\n      <td>-0.8062</td>\n      <td>-0.8996</td>\n      <td>...</td>\n      <td>0.0526</td>\n      <td>-0.0702</td>\n      <td>-0.0350</td>\n      <td>0.0702</td>\n      <td>0.1578</td>\n      <td>0.1930</td>\n      <td>0.4562</td>\n      <td>0.4562</td>\n      <td>-0.3860</td>\n      <td>4.0</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>-0.4798</td>\n      <td>-0.1580</td>\n      <td>0.1764</td>\n      <td>0.1820</td>\n      <td>-0.6378</td>\n      <td>-0.8400</td>\n      <td>-0.7280</td>\n      <td>-0.6654</td>\n      <td>-0.7978</td>\n      <td>-0.7904</td>\n      <td>...</td>\n      <td>0.2912</td>\n      <td>-0.1646</td>\n      <td>0.1140</td>\n      <td>0.0126</td>\n      <td>-0.0380</td>\n      <td>0.0886</td>\n      <td>0.2912</td>\n      <td>0.3670</td>\n      <td>0.1646</td>\n      <td>4.0</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>-0.3928</td>\n      <td>0.0424</td>\n      <td>0.2166</td>\n      <td>0.2124</td>\n      <td>-0.4564</td>\n      <td>-0.6200</td>\n      <td>-0.7112</td>\n      <td>-0.6602</td>\n      <td>-0.6942</td>\n      <td>-0.7920</td>\n      <td>...</td>\n      <td>0.8868</td>\n      <td>0.8868</td>\n      <td>0.6792</td>\n      <td>0.6038</td>\n      <td>0.2264</td>\n      <td>0.7924</td>\n      <td>1.0000</td>\n      <td>0.9246</td>\n      <td>0.5284</td>\n      <td>5.0</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>-0.5494</td>\n      <td>-0.0940</td>\n      <td>0.2868</td>\n      <td>0.2964</td>\n      <td>-0.5326</td>\n      <td>-0.7204</td>\n      <td>-0.7518</td>\n      <td>-0.7398</td>\n      <td>-0.8482</td>\n      <td>-0.8386</td>\n      <td>...</td>\n      <td>0.6130</td>\n      <td>0.6130</td>\n      <td>0.6130</td>\n      <td>0.3226</td>\n      <td>0.6130</td>\n      <td>0.2904</td>\n      <td>0.5484</td>\n      <td>0.5162</td>\n      <td>0.3548</td>\n      <td>5.0</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>-0.3646</td>\n      <td>0.0904</td>\n      <td>0.2374</td>\n      <td>0.3044</td>\n      <td>0.6188</td>\n      <td>0.7592</td>\n      <td>0.9264</td>\n      <td>0.6522</td>\n      <td>0.6054</td>\n      <td>0.2442</td>\n      <td>...</td>\n      <td>0.9178</td>\n      <td>1.0000</td>\n      <td>0.8082</td>\n      <td>0.8220</td>\n      <td>0.7672</td>\n      <td>0.7534</td>\n      <td>0.6986</td>\n      <td>0.4932</td>\n      <td>-0.0548</td>\n      <td>6.0</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>-0.6926</td>\n      <td>-0.3318</td>\n      <td>-0.1414</td>\n      <td>-0.1512</td>\n      <td>0.0098</td>\n      <td>0.2926</td>\n      <td>0.2440</td>\n      <td>0.3074</td>\n      <td>-0.0098</td>\n      <td>-0.0536</td>\n      <td>...</td>\n      <td>0.7460</td>\n      <td>0.8888</td>\n      <td>0.7460</td>\n      <td>0.6984</td>\n      <td>0.8254</td>\n      <td>1.0000</td>\n      <td>0.9842</td>\n      <td>0.5238</td>\n      <td>0.1746</td>\n      <td>6.0</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>-0.2532</td>\n      <td>0.1670</td>\n      <td>0.3470</td>\n      <td>0.3734</td>\n      <td>-0.2570</td>\n      <td>-0.7260</td>\n      <td>-0.8086</td>\n      <td>-0.6360</td>\n      <td>-0.7748</td>\n      <td>-0.8536</td>\n      <td>...</td>\n      <td>0.5302</td>\n      <td>0.2886</td>\n      <td>0.2618</td>\n      <td>0.1276</td>\n      <td>-0.0738</td>\n      <td>0.2618</td>\n      <td>0.3288</td>\n      <td>0.1276</td>\n      <td>-0.1812</td>\n      <td>7.0</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>-0.4030</td>\n      <td>-0.1284</td>\n      <td>0.2986</td>\n      <td>0.2358</td>\n      <td>-0.9134</td>\n      <td>-0.7552</td>\n      <td>-0.7642</td>\n      <td>-0.7134</td>\n      <td>-0.7014</td>\n      <td>-0.7940</td>\n      <td>...</td>\n      <td>0.4666</td>\n      <td>0.5200</td>\n      <td>0.6266</td>\n      <td>0.3866</td>\n      <td>0.4666</td>\n      <td>0.4134</td>\n      <td>0.5466</td>\n      <td>0.2000</td>\n      <td>-0.3600</td>\n      <td>7.0</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>-0.3216</td>\n      <td>0.1860</td>\n      <td>0.1248</td>\n      <td>0.3654</td>\n      <td>0.4880</td>\n      <td>0.4618</td>\n      <td>-0.1816</td>\n      <td>-0.0854</td>\n      <td>-0.2516</td>\n      <td>-0.4530</td>\n      <td>...</td>\n      <td>0.8222</td>\n      <td>0.9778</td>\n      <td>0.8444</td>\n      <td>0.7666</td>\n      <td>0.7888</td>\n      <td>0.8000</td>\n      <td>0.6556</td>\n      <td>0.5778</td>\n      <td>0.3000</td>\n      <td>8.0</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>-0.5368</td>\n      <td>-0.2516</td>\n      <td>0.1448</td>\n      <td>0.1848</td>\n      <td>0.3184</td>\n      <td>0.1982</td>\n      <td>0.1046</td>\n      <td>-0.2472</td>\n      <td>-0.3898</td>\n      <td>-0.3364</td>\n      <td>...</td>\n      <td>0.7806</td>\n      <td>0.8580</td>\n      <td>0.8968</td>\n      <td>0.8322</td>\n      <td>0.8322</td>\n      <td>1.0000</td>\n      <td>0.7936</td>\n      <td>0.6646</td>\n      <td>-0.2000</td>\n      <td>8.0</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>-0.4550</td>\n      <td>-0.1044</td>\n      <td>-0.0924</td>\n      <td>-0.0300</td>\n      <td>0.1068</td>\n      <td>0.3782</td>\n      <td>0.8248</td>\n      <td>0.9856</td>\n      <td>1.0000</td>\n      <td>0.9640</td>\n      <td>...</td>\n      <td>0.4588</td>\n      <td>0.4824</td>\n      <td>0.3412</td>\n      <td>0.2236</td>\n      <td>0.4118</td>\n      <td>0.5294</td>\n      <td>0.6942</td>\n      <td>0.6000</td>\n      <td>0.3648</td>\n      <td>9.0</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>-0.4804</td>\n      <td>-0.1238</td>\n      <td>0.0792</td>\n      <td>0.1114</td>\n      <td>0.0700</td>\n      <td>0.4096</td>\n      <td>0.4450</td>\n      <td>0.7326</td>\n      <td>0.8448</td>\n      <td>0.9616</td>\n      <td>...</td>\n      <td>0.2766</td>\n      <td>0.2554</td>\n      <td>0.1914</td>\n      <td>0.2978</td>\n      <td>0.2978</td>\n      <td>-0.0638</td>\n      <td>0.0212</td>\n      <td>0.1276</td>\n      <td>-0.2340</td>\n      <td>9.0</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>-0.3368</td>\n      <td>0.0026</td>\n      <td>0.5802</td>\n      <td>0.4706</td>\n      <td>0.2514</td>\n      <td>-0.0910</td>\n      <td>-0.3610</td>\n      <td>-0.3664</td>\n      <td>-0.2326</td>\n      <td>-0.4866</td>\n      <td>...</td>\n      <td>-0.1162</td>\n      <td>0.0388</td>\n      <td>0.0698</td>\n      <td>-0.1472</td>\n      <td>0.0388</td>\n      <td>0.2558</td>\n      <td>0.1162</td>\n      <td>0.1008</td>\n      <td>-0.3644</td>\n      <td>10.0</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>-0.5880</td>\n      <td>-0.2028</td>\n      <td>0.1304</td>\n      <td>0.2278</td>\n      <td>0.2132</td>\n      <td>-0.1822</td>\n      <td>-0.2960</td>\n      <td>-0.3084</td>\n      <td>-0.7722</td>\n      <td>-0.4224</td>\n      <td>...</td>\n      <td>0.7024</td>\n      <td>0.5702</td>\n      <td>0.4710</td>\n      <td>0.6694</td>\n      <td>0.8348</td>\n      <td>0.8182</td>\n      <td>0.6860</td>\n      <td>0.4380</td>\n      <td>-0.1240</td>\n      <td>10.0</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>-0.2560</td>\n      <td>0.3910</td>\n      <td>0.2630</td>\n      <td>0.4948</td>\n      <td>0.7854</td>\n      <td>0.8132</td>\n      <td>0.3460</td>\n      <td>0.2180</td>\n      <td>0.0830</td>\n      <td>-0.1418</td>\n      <td>...</td>\n      <td>0.0378</td>\n      <td>0.0944</td>\n      <td>0.0188</td>\n      <td>-0.2642</td>\n      <td>-0.3774</td>\n      <td>0.0754</td>\n      <td>0.5284</td>\n      <td>0.5094</td>\n      <td>-0.3018</td>\n      <td>11.0</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>-0.3896</td>\n      <td>0.1042</td>\n      <td>0.3190</td>\n      <td>0.4478</td>\n      <td>0.8252</td>\n      <td>0.7576</td>\n      <td>0.5982</td>\n      <td>0.3190</td>\n      <td>0.2300</td>\n      <td>0.0184</td>\n      <td>...</td>\n      <td>0.7846</td>\n      <td>0.4770</td>\n      <td>0.5076</td>\n      <td>0.6924</td>\n      <td>0.3846</td>\n      <td>0.3538</td>\n      <td>0.5692</td>\n      <td>0.6308</td>\n      <td>0.4770</td>\n      <td>11.0</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>-0.3384</td>\n      <td>0.1034</td>\n      <td>0.2260</td>\n      <td>0.3206</td>\n      <td>0.7114</td>\n      <td>0.6372</td>\n      <td>0.6808</td>\n      <td>0.3026</td>\n      <td>0.1468</td>\n      <td>-0.0932</td>\n      <td>...</td>\n      <td>0.8410</td>\n      <td>0.3864</td>\n      <td>-0.1136</td>\n      <td>0.3636</td>\n      <td>0.3182</td>\n      <td>0.2272</td>\n      <td>0.4546</td>\n      <td>-0.2500</td>\n      <td>-0.5228</td>\n      <td>12.0</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>-0.4810</td>\n      <td>-0.1146</td>\n      <td>0.2654</td>\n      <td>0.2890</td>\n      <td>0.5102</td>\n      <td>0.5514</td>\n      <td>0.6180</td>\n      <td>0.2360</td>\n      <td>0.0696</td>\n      <td>-0.0324</td>\n      <td>...</td>\n      <td>0.1384</td>\n      <td>0.3538</td>\n      <td>0.0462</td>\n      <td>0.4770</td>\n      <td>0.3538</td>\n      <td>-0.1692</td>\n      <td>0.2616</td>\n      <td>0.0770</td>\n      <td>-0.3230</td>\n      <td>12.0</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>-0.2116</td>\n      <td>0.2024</td>\n      <td>0.4222</td>\n      <td>0.4596</td>\n      <td>0.7216</td>\n      <td>0.5954</td>\n      <td>0.4340</td>\n      <td>0.0690</td>\n      <td>0.0036</td>\n      <td>-0.2234</td>\n      <td>...</td>\n      <td>1.0000</td>\n      <td>0.8832</td>\n      <td>0.9708</td>\n      <td>0.9124</td>\n      <td>0.7810</td>\n      <td>0.8686</td>\n      <td>0.7664</td>\n      <td>0.4598</td>\n      <td>0.2700</td>\n      <td>13.0</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>-0.5672</td>\n      <td>-0.2202</td>\n      <td>0.0220</td>\n      <td>0.0734</td>\n      <td>0.1058</td>\n      <td>0.1516</td>\n      <td>0.2240</td>\n      <td>-0.1058</td>\n      <td>-0.1898</td>\n      <td>-0.1020</td>\n      <td>...</td>\n      <td>0.3056</td>\n      <td>0.0278</td>\n      <td>0.3056</td>\n      <td>0.3334</td>\n      <td>0.1666</td>\n      <td>0.2500</td>\n      <td>0.3334</td>\n      <td>-0.3612</td>\n      <td>-0.7500</td>\n      <td>13.0</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>-0.5160</td>\n      <td>-0.2120</td>\n      <td>0.1686</td>\n      <td>0.1976</td>\n      <td>0.1562</td>\n      <td>-0.0176</td>\n      <td>-0.0362</td>\n      <td>-0.0672</td>\n      <td>-0.1540</td>\n      <td>-0.6360</td>\n      <td>...</td>\n      <td>0.2632</td>\n      <td>0.0526</td>\n      <td>0.1316</td>\n      <td>0.3158</td>\n      <td>0.3158</td>\n      <td>0.5000</td>\n      <td>0.3948</td>\n      <td>0.3684</td>\n      <td>0.1578</td>\n      <td>14.0</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>-0.5330</td>\n      <td>-0.0868</td>\n      <td>0.1838</td>\n      <td>0.2394</td>\n      <td>0.4428</td>\n      <td>0.2694</td>\n      <td>-0.0150</td>\n      <td>-0.2924</td>\n      <td>-0.4220</td>\n      <td>-0.5654</td>\n      <td>...</td>\n      <td>0.5164</td>\n      <td>0.2748</td>\n      <td>0.1648</td>\n      <td>0.3626</td>\n      <td>0.2968</td>\n      <td>0.4506</td>\n      <td>0.2748</td>\n      <td>-0.0550</td>\n      <td>-0.8462</td>\n      <td>14.0</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>-0.3656</td>\n      <td>0.0296</td>\n      <td>0.3160</td>\n      <td>0.5526</td>\n      <td>0.9030</td>\n      <td>0.9574</td>\n      <td>0.9834</td>\n      <td>0.8556</td>\n      <td>0.9314</td>\n      <td>0.9692</td>\n      <td>...</td>\n      <td>0.1904</td>\n      <td>0.2380</td>\n      <td>0.3810</td>\n      <td>0.4762</td>\n      <td>0.5952</td>\n      <td>0.5714</td>\n      <td>0.8096</td>\n      <td>0.3334</td>\n      <td>-0.4762</td>\n      <td>15.0</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>-0.5154</td>\n      <td>-0.1740</td>\n      <td>0.3140</td>\n      <td>0.5392</td>\n      <td>0.8566</td>\n      <td>0.9488</td>\n      <td>0.7884</td>\n      <td>0.7508</td>\n      <td>0.8464</td>\n      <td>1.0000</td>\n      <td>...</td>\n      <td>0.1154</td>\n      <td>0.0962</td>\n      <td>0.0000</td>\n      <td>-0.2308</td>\n      <td>-0.1730</td>\n      <td>0.2116</td>\n      <td>0.1924</td>\n      <td>0.4038</td>\n      <td>0.1346</td>\n      <td>15.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>6208</th>\n      <td>-0.6188</td>\n      <td>-0.0210</td>\n      <td>0.5006</td>\n      <td>0.9438</td>\n      <td>1.0000</td>\n      <td>0.6510</td>\n      <td>0.3140</td>\n      <td>0.3200</td>\n      <td>0.3160</td>\n      <td>0.6470</td>\n      <td>...</td>\n      <td>0.4242</td>\n      <td>0.0000</td>\n      <td>0.3636</td>\n      <td>0.3030</td>\n      <td>0.3334</td>\n      <td>0.3940</td>\n      <td>0.3334</td>\n      <td>-0.4242</td>\n      <td>-0.7878</td>\n      <td>23.0</td>\n    </tr>\n    <tr>\n      <th>6209</th>\n      <td>-0.6160</td>\n      <td>-0.1920</td>\n      <td>0.1346</td>\n      <td>0.6618</td>\n      <td>0.7250</td>\n      <td>0.4900</td>\n      <td>0.1002</td>\n      <td>-0.0430</td>\n      <td>-0.0372</td>\n      <td>-0.0258</td>\n      <td>...</td>\n      <td>0.5104</td>\n      <td>0.6224</td>\n      <td>0.3986</td>\n      <td>0.2028</td>\n      <td>0.4966</td>\n      <td>0.4406</td>\n      <td>0.3706</td>\n      <td>0.4686</td>\n      <td>0.1468</td>\n      <td>24.0</td>\n    </tr>\n    <tr>\n      <th>6210</th>\n      <td>0.1044</td>\n      <td>0.6778</td>\n      <td>1.0000</td>\n      <td>0.8300</td>\n      <td>0.3770</td>\n      <td>0.4300</td>\n      <td>0.3310</td>\n      <td>-0.0620</td>\n      <td>-0.5398</td>\n      <td>-0.4974</td>\n      <td>...</td>\n      <td>0.1590</td>\n      <td>-0.0682</td>\n      <td>-0.2046</td>\n      <td>0.0000</td>\n      <td>-0.1364</td>\n      <td>-0.0682</td>\n      <td>0.0228</td>\n      <td>-0.0682</td>\n      <td>-0.7500</td>\n      <td>25.0</td>\n    </tr>\n    <tr>\n      <th>6211</th>\n      <td>-0.3994</td>\n      <td>0.3448</td>\n      <td>0.7816</td>\n      <td>0.6178</td>\n      <td>-0.1332</td>\n      <td>-0.3310</td>\n      <td>-0.5154</td>\n      <td>-0.7202</td>\n      <td>-0.6588</td>\n      <td>-0.6314</td>\n      <td>...</td>\n      <td>0.6364</td>\n      <td>0.7402</td>\n      <td>0.4806</td>\n      <td>0.6104</td>\n      <td>0.6364</td>\n      <td>0.6104</td>\n      <td>0.6104</td>\n      <td>-0.1428</td>\n      <td>-0.3506</td>\n      <td>26.0</td>\n    </tr>\n    <tr>\n      <th>6212</th>\n      <td>-0.5710</td>\n      <td>-0.2096</td>\n      <td>0.2578</td>\n      <td>0.5350</td>\n      <td>0.6072</td>\n      <td>0.1180</td>\n      <td>-0.2868</td>\n      <td>-0.1350</td>\n      <td>-0.2964</td>\n      <td>-0.2964</td>\n      <td>...</td>\n      <td>0.2820</td>\n      <td>0.4358</td>\n      <td>0.5384</td>\n      <td>1.0000</td>\n      <td>0.8206</td>\n      <td>0.3334</td>\n      <td>-0.0256</td>\n      <td>0.2564</td>\n      <td>-0.5642</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>6213</th>\n      <td>-0.5008</td>\n      <td>0.1016</td>\n      <td>0.6328</td>\n      <td>0.5436</td>\n      <td>-0.4724</td>\n      <td>-0.4474</td>\n      <td>-0.6256</td>\n      <td>-0.6684</td>\n      <td>-0.7254</td>\n      <td>-0.6970</td>\n      <td>...</td>\n      <td>0.5714</td>\n      <td>0.5476</td>\n      <td>0.8810</td>\n      <td>1.0000</td>\n      <td>0.6904</td>\n      <td>0.7858</td>\n      <td>0.5000</td>\n      <td>0.7142</td>\n      <td>0.4762</td>\n      <td>2.0</td>\n    </tr>\n    <tr>\n      <th>6214</th>\n      <td>-0.5470</td>\n      <td>0.0108</td>\n      <td>0.5378</td>\n      <td>0.5224</td>\n      <td>0.0262</td>\n      <td>-0.1926</td>\n      <td>-0.3498</td>\n      <td>-0.4144</td>\n      <td>-0.4730</td>\n      <td>-0.4454</td>\n      <td>...</td>\n      <td>0.3600</td>\n      <td>0.3334</td>\n      <td>0.9466</td>\n      <td>1.0000</td>\n      <td>0.8400</td>\n      <td>0.7066</td>\n      <td>0.6000</td>\n      <td>0.8134</td>\n      <td>-0.0134</td>\n      <td>3.0</td>\n    </tr>\n    <tr>\n      <th>6215</th>\n      <td>-0.5508</td>\n      <td>0.1606</td>\n      <td>0.4000</td>\n      <td>0.2558</td>\n      <td>-0.2000</td>\n      <td>-0.3050</td>\n      <td>-0.3378</td>\n      <td>-0.5836</td>\n      <td>-0.5606</td>\n      <td>-0.5114</td>\n      <td>...</td>\n      <td>0.8934</td>\n      <td>0.4400</td>\n      <td>1.0000</td>\n      <td>0.8400</td>\n      <td>0.5734</td>\n      <td>0.5200</td>\n      <td>0.4934</td>\n      <td>0.3866</td>\n      <td>0.1200</td>\n      <td>4.0</td>\n    </tr>\n    <tr>\n      <th>6216</th>\n      <td>-0.5874</td>\n      <td>0.1226</td>\n      <td>0.0852</td>\n      <td>-0.0562</td>\n      <td>-0.3940</td>\n      <td>-0.6278</td>\n      <td>-0.7864</td>\n      <td>-0.7056</td>\n      <td>-0.8384</td>\n      <td>-0.7432</td>\n      <td>...</td>\n      <td>0.5172</td>\n      <td>0.7242</td>\n      <td>1.0000</td>\n      <td>0.8620</td>\n      <td>0.6552</td>\n      <td>0.6782</td>\n      <td>0.9310</td>\n      <td>0.8850</td>\n      <td>0.2644</td>\n      <td>5.0</td>\n    </tr>\n    <tr>\n      <th>6217</th>\n      <td>-0.6648</td>\n      <td>-0.2948</td>\n      <td>0.1330</td>\n      <td>0.4740</td>\n      <td>0.6242</td>\n      <td>0.3988</td>\n      <td>0.1272</td>\n      <td>-0.0346</td>\n      <td>-0.1966</td>\n      <td>-0.1446</td>\n      <td>...</td>\n      <td>0.6892</td>\n      <td>0.8514</td>\n      <td>0.9190</td>\n      <td>0.8378</td>\n      <td>0.8784</td>\n      <td>0.9864</td>\n      <td>1.0000</td>\n      <td>0.8378</td>\n      <td>0.7028</td>\n      <td>6.0</td>\n    </tr>\n    <tr>\n      <th>6218</th>\n      <td>-0.4406</td>\n      <td>0.4478</td>\n      <td>0.4734</td>\n      <td>0.4406</td>\n      <td>0.0896</td>\n      <td>-0.2286</td>\n      <td>-0.3016</td>\n      <td>-0.2724</td>\n      <td>-0.6270</td>\n      <td>-0.3784</td>\n      <td>...</td>\n      <td>0.3392</td>\n      <td>0.5000</td>\n      <td>0.7322</td>\n      <td>0.6786</td>\n      <td>0.5714</td>\n      <td>0.6250</td>\n      <td>0.5536</td>\n      <td>0.8572</td>\n      <td>0.5536</td>\n      <td>7.0</td>\n    </tr>\n    <tr>\n      <th>6219</th>\n      <td>-0.5870</td>\n      <td>-0.1630</td>\n      <td>0.3478</td>\n      <td>0.3696</td>\n      <td>0.3804</td>\n      <td>0.0760</td>\n      <td>-0.2718</td>\n      <td>-0.2718</td>\n      <td>-0.4348</td>\n      <td>-0.3642</td>\n      <td>...</td>\n      <td>0.7514</td>\n      <td>0.7870</td>\n      <td>0.7160</td>\n      <td>0.7752</td>\n      <td>0.7514</td>\n      <td>0.6214</td>\n      <td>0.8816</td>\n      <td>0.6450</td>\n      <td>0.2662</td>\n      <td>8.0</td>\n    </tr>\n    <tr>\n      <th>6220</th>\n      <td>-0.6376</td>\n      <td>-0.4144</td>\n      <td>-0.0392</td>\n      <td>0.3334</td>\n      <td>0.7642</td>\n      <td>0.9416</td>\n      <td>1.0000</td>\n      <td>0.7718</td>\n      <td>0.9112</td>\n      <td>0.8580</td>\n      <td>...</td>\n      <td>0.0336</td>\n      <td>0.0202</td>\n      <td>-0.0738</td>\n      <td>-0.0068</td>\n      <td>0.1276</td>\n      <td>0.1276</td>\n      <td>-0.0604</td>\n      <td>-0.0068</td>\n      <td>-0.3960</td>\n      <td>9.0</td>\n    </tr>\n    <tr>\n      <th>6221</th>\n      <td>-0.5384</td>\n      <td>0.0736</td>\n      <td>0.6120</td>\n      <td>0.5284</td>\n      <td>0.1740</td>\n      <td>-0.0904</td>\n      <td>-0.1672</td>\n      <td>-0.3546</td>\n      <td>-0.4248</td>\n      <td>-0.2576</td>\n      <td>...</td>\n      <td>0.1962</td>\n      <td>0.4392</td>\n      <td>0.5514</td>\n      <td>0.4954</td>\n      <td>0.5888</td>\n      <td>0.2336</td>\n      <td>0.3832</td>\n      <td>0.5888</td>\n      <td>0.3458</td>\n      <td>10.0</td>\n    </tr>\n    <tr>\n      <th>6222</th>\n      <td>-0.4842</td>\n      <td>-0.1468</td>\n      <td>0.4642</td>\n      <td>0.8214</td>\n      <td>0.8650</td>\n      <td>0.2936</td>\n      <td>0.2342</td>\n      <td>-0.0278</td>\n      <td>-0.3850</td>\n      <td>-0.1746</td>\n      <td>...</td>\n      <td>0.3636</td>\n      <td>0.3410</td>\n      <td>0.8410</td>\n      <td>1.0000</td>\n      <td>0.7728</td>\n      <td>0.5910</td>\n      <td>0.7728</td>\n      <td>0.7046</td>\n      <td>0.2272</td>\n      <td>11.0</td>\n    </tr>\n    <tr>\n      <th>6223</th>\n      <td>-0.6018</td>\n      <td>-0.2306</td>\n      <td>0.1438</td>\n      <td>0.6736</td>\n      <td>0.9222</td>\n      <td>0.6826</td>\n      <td>0.3024</td>\n      <td>0.2366</td>\n      <td>-0.0480</td>\n      <td>-0.0120</td>\n      <td>...</td>\n      <td>0.6806</td>\n      <td>0.4118</td>\n      <td>0.5294</td>\n      <td>0.5798</td>\n      <td>0.6638</td>\n      <td>0.2774</td>\n      <td>0.4958</td>\n      <td>0.5294</td>\n      <td>-0.0252</td>\n      <td>12.0</td>\n    </tr>\n    <tr>\n      <th>6224</th>\n      <td>-0.6228</td>\n      <td>-0.1886</td>\n      <td>0.2256</td>\n      <td>0.5252</td>\n      <td>0.5622</td>\n      <td>0.3368</td>\n      <td>0.0506</td>\n      <td>-0.0538</td>\n      <td>-0.3232</td>\n      <td>-0.2896</td>\n      <td>...</td>\n      <td>0.0910</td>\n      <td>0.2468</td>\n      <td>0.4026</td>\n      <td>0.3246</td>\n      <td>0.2728</td>\n      <td>0.3896</td>\n      <td>0.3116</td>\n      <td>-0.2728</td>\n      <td>-0.4156</td>\n      <td>13.0</td>\n    </tr>\n    <tr>\n      <th>6225</th>\n      <td>-0.6334</td>\n      <td>-0.2178</td>\n      <td>0.1516</td>\n      <td>0.4834</td>\n      <td>0.5122</td>\n      <td>0.2582</td>\n      <td>0.1024</td>\n      <td>-0.0938</td>\n      <td>-0.3160</td>\n      <td>-0.4516</td>\n      <td>...</td>\n      <td>0.6508</td>\n      <td>0.6666</td>\n      <td>0.4444</td>\n      <td>0.5080</td>\n      <td>0.3810</td>\n      <td>0.2064</td>\n      <td>0.4126</td>\n      <td>0.4126</td>\n      <td>0.0634</td>\n      <td>14.0</td>\n    </tr>\n    <tr>\n      <th>6226</th>\n      <td>-0.7072</td>\n      <td>-0.2148</td>\n      <td>0.2120</td>\n      <td>0.8332</td>\n      <td>1.0000</td>\n      <td>0.7346</td>\n      <td>0.6908</td>\n      <td>0.8276</td>\n      <td>0.8660</td>\n      <td>0.7838</td>\n      <td>...</td>\n      <td>0.2500</td>\n      <td>0.2142</td>\n      <td>0.1786</td>\n      <td>0.2500</td>\n      <td>0.0892</td>\n      <td>-0.0714</td>\n      <td>-0.0178</td>\n      <td>-0.0714</td>\n      <td>-0.2142</td>\n      <td>15.0</td>\n    </tr>\n    <tr>\n      <th>6227</th>\n      <td>-0.2736</td>\n      <td>0.5246</td>\n      <td>0.7938</td>\n      <td>0.4888</td>\n      <td>0.0000</td>\n      <td>-0.2826</td>\n      <td>-0.4170</td>\n      <td>-0.6008</td>\n      <td>-0.7174</td>\n      <td>-0.6772</td>\n      <td>...</td>\n      <td>0.7684</td>\n      <td>0.5158</td>\n      <td>0.6632</td>\n      <td>0.7264</td>\n      <td>0.3264</td>\n      <td>0.3894</td>\n      <td>0.7052</td>\n      <td>0.7684</td>\n      <td>-0.0106</td>\n      <td>16.0</td>\n    </tr>\n    <tr>\n      <th>6228</th>\n      <td>-0.2158</td>\n      <td>0.5066</td>\n      <td>0.8150</td>\n      <td>0.3172</td>\n      <td>-0.3744</td>\n      <td>-0.5374</td>\n      <td>-0.7180</td>\n      <td>-0.7798</td>\n      <td>-0.8898</td>\n      <td>-0.6564</td>\n      <td>...</td>\n      <td>0.3726</td>\n      <td>0.3726</td>\n      <td>0.5686</td>\n      <td>0.9412</td>\n      <td>0.2942</td>\n      <td>0.5294</td>\n      <td>0.2942</td>\n      <td>0.1764</td>\n      <td>-0.3138</td>\n      <td>17.0</td>\n    </tr>\n    <tr>\n      <th>6229</th>\n      <td>-0.5844</td>\n      <td>-0.2138</td>\n      <td>0.0194</td>\n      <td>0.7130</td>\n      <td>0.9074</td>\n      <td>0.9432</td>\n      <td>0.9492</td>\n      <td>1.0000</td>\n      <td>0.5754</td>\n      <td>0.1808</td>\n      <td>...</td>\n      <td>0.1926</td>\n      <td>0.3028</td>\n      <td>0.1010</td>\n      <td>0.2844</td>\n      <td>0.0642</td>\n      <td>0.3212</td>\n      <td>0.3944</td>\n      <td>0.0458</td>\n      <td>-0.1192</td>\n      <td>18.0</td>\n    </tr>\n    <tr>\n      <th>6230</th>\n      <td>-0.7400</td>\n      <td>-0.3666</td>\n      <td>0.0800</td>\n      <td>0.4734</td>\n      <td>0.6466</td>\n      <td>0.4866</td>\n      <td>0.1600</td>\n      <td>0.0134</td>\n      <td>-0.2134</td>\n      <td>-0.1734</td>\n      <td>...</td>\n      <td>0.6574</td>\n      <td>0.6132</td>\n      <td>0.6796</td>\n      <td>0.8784</td>\n      <td>0.9448</td>\n      <td>0.9116</td>\n      <td>0.7790</td>\n      <td>0.8454</td>\n      <td>0.6574</td>\n      <td>19.0</td>\n    </tr>\n    <tr>\n      <th>6231</th>\n      <td>-0.3802</td>\n      <td>0.4374</td>\n      <td>0.5296</td>\n      <td>0.5252</td>\n      <td>-0.1164</td>\n      <td>-0.5032</td>\n      <td>-0.6616</td>\n      <td>-0.8154</td>\n      <td>-0.8770</td>\n      <td>-0.8638</td>\n      <td>...</td>\n      <td>0.7592</td>\n      <td>0.8334</td>\n      <td>0.9444</td>\n      <td>1.0000</td>\n      <td>0.9630</td>\n      <td>0.6482</td>\n      <td>0.6666</td>\n      <td>0.5556</td>\n      <td>0.3518</td>\n      <td>20.0</td>\n    </tr>\n    <tr>\n      <th>6232</th>\n      <td>-0.5894</td>\n      <td>0.2200</td>\n      <td>0.1524</td>\n      <td>-0.0704</td>\n      <td>-0.3636</td>\n      <td>-0.6628</td>\n      <td>-0.7390</td>\n      <td>-0.9120</td>\n      <td>-0.8886</td>\n      <td>-0.6568</td>\n      <td>...</td>\n      <td>-0.2308</td>\n      <td>-0.1624</td>\n      <td>-0.0770</td>\n      <td>0.2308</td>\n      <td>0.1966</td>\n      <td>0.2478</td>\n      <td>0.1624</td>\n      <td>-0.3504</td>\n      <td>-0.5556</td>\n      <td>21.0</td>\n    </tr>\n    <tr>\n      <th>6233</th>\n      <td>-0.5742</td>\n      <td>0.1050</td>\n      <td>0.4936</td>\n      <td>0.3986</td>\n      <td>-0.2058</td>\n      <td>-0.4130</td>\n      <td>-0.4188</td>\n      <td>-0.5194</td>\n      <td>-0.5080</td>\n      <td>-0.4878</td>\n      <td>...</td>\n      <td>0.5000</td>\n      <td>0.6800</td>\n      <td>0.8200</td>\n      <td>0.8400</td>\n      <td>0.8400</td>\n      <td>0.7400</td>\n      <td>0.8200</td>\n      <td>0.6400</td>\n      <td>0.3200</td>\n      <td>22.0</td>\n    </tr>\n    <tr>\n      <th>6234</th>\n      <td>-0.4520</td>\n      <td>0.0154</td>\n      <td>0.5078</td>\n      <td>0.8978</td>\n      <td>0.7956</td>\n      <td>0.4366</td>\n      <td>0.2352</td>\n      <td>0.1300</td>\n      <td>0.0682</td>\n      <td>0.3004</td>\n      <td>...</td>\n      <td>0.5000</td>\n      <td>0.2250</td>\n      <td>0.7500</td>\n      <td>0.8750</td>\n      <td>0.6750</td>\n      <td>0.6000</td>\n      <td>0.4500</td>\n      <td>-0.1250</td>\n      <td>-0.2250</td>\n      <td>23.0</td>\n    </tr>\n    <tr>\n      <th>6235</th>\n      <td>-0.5824</td>\n      <td>-0.1646</td>\n      <td>0.1406</td>\n      <td>0.6224</td>\n      <td>0.6626</td>\n      <td>0.3172</td>\n      <td>0.0924</td>\n      <td>0.0120</td>\n      <td>-0.1646</td>\n      <td>-0.1326</td>\n      <td>...</td>\n      <td>0.8068</td>\n      <td>0.7392</td>\n      <td>0.7392</td>\n      <td>0.6908</td>\n      <td>0.7294</td>\n      <td>0.7004</td>\n      <td>0.6812</td>\n      <td>0.5170</td>\n      <td>0.3430</td>\n      <td>24.0</td>\n    </tr>\n    <tr>\n      <th>6236</th>\n      <td>0.0160</td>\n      <td>0.8168</td>\n      <td>1.0000</td>\n      <td>0.7814</td>\n      <td>0.4084</td>\n      <td>0.2122</td>\n      <td>-0.2218</td>\n      <td>-0.6848</td>\n      <td>-0.8424</td>\n      <td>-0.7588</td>\n      <td>...</td>\n      <td>0.0344</td>\n      <td>0.0344</td>\n      <td>-0.0344</td>\n      <td>0.4252</td>\n      <td>0.2874</td>\n      <td>-0.0114</td>\n      <td>0.1034</td>\n      <td>-0.1954</td>\n      <td>-0.8620</td>\n      <td>25.0</td>\n    </tr>\n    <tr>\n      <th>6237</th>\n      <td>-0.6116</td>\n      <td>-0.1040</td>\n      <td>0.2566</td>\n      <td>0.2316</td>\n      <td>-0.0568</td>\n      <td>-0.3648</td>\n      <td>-0.3870</td>\n      <td>-0.4868</td>\n      <td>-0.4674</td>\n      <td>-0.3232</td>\n      <td>...</td>\n      <td>-0.0178</td>\n      <td>-0.0536</td>\n      <td>0.5178</td>\n      <td>1.0000</td>\n      <td>0.9464</td>\n      <td>0.2500</td>\n      <td>-0.0536</td>\n      <td>0.0714</td>\n      <td>-0.0892</td>\n      <td>26.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>6238 rows × 618 columns</p>\n</div>",
            "text/plain": "         0       1       2       3       4       5       6       7       8    \\\n0    -0.4394 -0.0930  0.1718  0.4620  0.6226  0.4704  0.3578  0.0478 -0.1184   \n1    -0.4348 -0.1198  0.2474  0.4036  0.5026  0.6328  0.4948  0.0338 -0.0520   \n2    -0.2330  0.2124  0.5014  0.5222 -0.3422 -0.5840 -0.7168 -0.6342 -0.8614   \n3    -0.3808 -0.0096  0.2602  0.2554 -0.4290 -0.6746 -0.6868 -0.6650 -0.8410   \n4    -0.3412  0.0946  0.6082  0.6216 -0.1622 -0.3784 -0.4324 -0.4358 -0.4966   \n5    -0.4634  0.0306  0.3546  0.4448 -0.1022 -0.4184 -0.6388 -0.4370 -0.4396   \n6    -0.3364 -0.0102  0.2132  0.2018 -0.6146 -0.8380 -0.8130 -0.7240 -0.8062   \n7    -0.4798 -0.1580  0.1764  0.1820 -0.6378 -0.8400 -0.7280 -0.6654 -0.7978   \n8    -0.3928  0.0424  0.2166  0.2124 -0.4564 -0.6200 -0.7112 -0.6602 -0.6942   \n9    -0.5494 -0.0940  0.2868  0.2964 -0.5326 -0.7204 -0.7518 -0.7398 -0.8482   \n10   -0.3646  0.0904  0.2374  0.3044  0.6188  0.7592  0.9264  0.6522  0.6054   \n11   -0.6926 -0.3318 -0.1414 -0.1512  0.0098  0.2926  0.2440  0.3074 -0.0098   \n12   -0.2532  0.1670  0.3470  0.3734 -0.2570 -0.7260 -0.8086 -0.6360 -0.7748   \n13   -0.4030 -0.1284  0.2986  0.2358 -0.9134 -0.7552 -0.7642 -0.7134 -0.7014   \n14   -0.3216  0.1860  0.1248  0.3654  0.4880  0.4618 -0.1816 -0.0854 -0.2516   \n15   -0.5368 -0.2516  0.1448  0.1848  0.3184  0.1982  0.1046 -0.2472 -0.3898   \n16   -0.4550 -0.1044 -0.0924 -0.0300  0.1068  0.3782  0.8248  0.9856  1.0000   \n17   -0.4804 -0.1238  0.0792  0.1114  0.0700  0.4096  0.4450  0.7326  0.8448   \n18   -0.3368  0.0026  0.5802  0.4706  0.2514 -0.0910 -0.3610 -0.3664 -0.2326   \n19   -0.5880 -0.2028  0.1304  0.2278  0.2132 -0.1822 -0.2960 -0.3084 -0.7722   \n20   -0.2560  0.3910  0.2630  0.4948  0.7854  0.8132  0.3460  0.2180  0.0830   \n21   -0.3896  0.1042  0.3190  0.4478  0.8252  0.7576  0.5982  0.3190  0.2300   \n22   -0.3384  0.1034  0.2260  0.3206  0.7114  0.6372  0.6808  0.3026  0.1468   \n23   -0.4810 -0.1146  0.2654  0.2890  0.5102  0.5514  0.6180  0.2360  0.0696   \n24   -0.2116  0.2024  0.4222  0.4596  0.7216  0.5954  0.4340  0.0690  0.0036   \n25   -0.5672 -0.2202  0.0220  0.0734  0.1058  0.1516  0.2240 -0.1058 -0.1898   \n26   -0.5160 -0.2120  0.1686  0.1976  0.1562 -0.0176 -0.0362 -0.0672 -0.1540   \n27   -0.5330 -0.0868  0.1838  0.2394  0.4428  0.2694 -0.0150 -0.2924 -0.4220   \n28   -0.3656  0.0296  0.3160  0.5526  0.9030  0.9574  0.9834  0.8556  0.9314   \n29   -0.5154 -0.1740  0.3140  0.5392  0.8566  0.9488  0.7884  0.7508  0.8464   \n...      ...     ...     ...     ...     ...     ...     ...     ...     ...   \n6208 -0.6188 -0.0210  0.5006  0.9438  1.0000  0.6510  0.3140  0.3200  0.3160   \n6209 -0.6160 -0.1920  0.1346  0.6618  0.7250  0.4900  0.1002 -0.0430 -0.0372   \n6210  0.1044  0.6778  1.0000  0.8300  0.3770  0.4300  0.3310 -0.0620 -0.5398   \n6211 -0.3994  0.3448  0.7816  0.6178 -0.1332 -0.3310 -0.5154 -0.7202 -0.6588   \n6212 -0.5710 -0.2096  0.2578  0.5350  0.6072  0.1180 -0.2868 -0.1350 -0.2964   \n6213 -0.5008  0.1016  0.6328  0.5436 -0.4724 -0.4474 -0.6256 -0.6684 -0.7254   \n6214 -0.5470  0.0108  0.5378  0.5224  0.0262 -0.1926 -0.3498 -0.4144 -0.4730   \n6215 -0.5508  0.1606  0.4000  0.2558 -0.2000 -0.3050 -0.3378 -0.5836 -0.5606   \n6216 -0.5874  0.1226  0.0852 -0.0562 -0.3940 -0.6278 -0.7864 -0.7056 -0.8384   \n6217 -0.6648 -0.2948  0.1330  0.4740  0.6242  0.3988  0.1272 -0.0346 -0.1966   \n6218 -0.4406  0.4478  0.4734  0.4406  0.0896 -0.2286 -0.3016 -0.2724 -0.6270   \n6219 -0.5870 -0.1630  0.3478  0.3696  0.3804  0.0760 -0.2718 -0.2718 -0.4348   \n6220 -0.6376 -0.4144 -0.0392  0.3334  0.7642  0.9416  1.0000  0.7718  0.9112   \n6221 -0.5384  0.0736  0.6120  0.5284  0.1740 -0.0904 -0.1672 -0.3546 -0.4248   \n6222 -0.4842 -0.1468  0.4642  0.8214  0.8650  0.2936  0.2342 -0.0278 -0.3850   \n6223 -0.6018 -0.2306  0.1438  0.6736  0.9222  0.6826  0.3024  0.2366 -0.0480   \n6224 -0.6228 -0.1886  0.2256  0.5252  0.5622  0.3368  0.0506 -0.0538 -0.3232   \n6225 -0.6334 -0.2178  0.1516  0.4834  0.5122  0.2582  0.1024 -0.0938 -0.3160   \n6226 -0.7072 -0.2148  0.2120  0.8332  1.0000  0.7346  0.6908  0.8276  0.8660   \n6227 -0.2736  0.5246  0.7938  0.4888  0.0000 -0.2826 -0.4170 -0.6008 -0.7174   \n6228 -0.2158  0.5066  0.8150  0.3172 -0.3744 -0.5374 -0.7180 -0.7798 -0.8898   \n6229 -0.5844 -0.2138  0.0194  0.7130  0.9074  0.9432  0.9492  1.0000  0.5754   \n6230 -0.7400 -0.3666  0.0800  0.4734  0.6466  0.4866  0.1600  0.0134 -0.2134   \n6231 -0.3802  0.4374  0.5296  0.5252 -0.1164 -0.5032 -0.6616 -0.8154 -0.8770   \n6232 -0.5894  0.2200  0.1524 -0.0704 -0.3636 -0.6628 -0.7390 -0.9120 -0.8886   \n6233 -0.5742  0.1050  0.4936  0.3986 -0.2058 -0.4130 -0.4188 -0.5194 -0.5080   \n6234 -0.4520  0.0154  0.5078  0.8978  0.7956  0.4366  0.2352  0.1300  0.0682   \n6235 -0.5824 -0.1646  0.1406  0.6224  0.6626  0.3172  0.0924  0.0120 -0.1646   \n6236  0.0160  0.8168  1.0000  0.7814  0.4084  0.2122 -0.2218 -0.6848 -0.8424   \n6237 -0.6116 -0.1040  0.2566  0.2316 -0.0568 -0.3648 -0.3870 -0.4868 -0.4674   \n\n         9    ...      608     609     610     611     612     613     614  \\\n0    -0.2310  ...   0.4102  0.2052  0.3846  0.3590  0.5898  0.3334  0.6410   \n1    -0.1302  ...   0.0000  0.2954  0.2046  0.4772  0.0454  0.2046  0.4318   \n2    -0.8318  ...  -0.1112 -0.0476 -0.1746  0.0318 -0.0476  0.1112  0.2540   \n3    -0.9614  ...  -0.0504 -0.0360 -0.1224  0.1366  0.2950  0.0792 -0.0072   \n4    -0.5406  ...   0.1562  0.3124  0.2500 -0.0938  0.1562  0.3124  0.3124   \n5    -0.6654  ...   0.6626  0.7350  0.3734  0.6626  0.3012  0.1808  0.2290   \n6    -0.8996  ...   0.0526 -0.0702 -0.0350  0.0702  0.1578  0.1930  0.4562   \n7    -0.7904  ...   0.2912 -0.1646  0.1140  0.0126 -0.0380  0.0886  0.2912   \n8    -0.7920  ...   0.8868  0.8868  0.6792  0.6038  0.2264  0.7924  1.0000   \n9    -0.8386  ...   0.6130  0.6130  0.6130  0.3226  0.6130  0.2904  0.5484   \n10    0.2442  ...   0.9178  1.0000  0.8082  0.8220  0.7672  0.7534  0.6986   \n11   -0.0536  ...   0.7460  0.8888  0.7460  0.6984  0.8254  1.0000  0.9842   \n12   -0.8536  ...   0.5302  0.2886  0.2618  0.1276 -0.0738  0.2618  0.3288   \n13   -0.7940  ...   0.4666  0.5200  0.6266  0.3866  0.4666  0.4134  0.5466   \n14   -0.4530  ...   0.8222  0.9778  0.8444  0.7666  0.7888  0.8000  0.6556   \n15   -0.3364  ...   0.7806  0.8580  0.8968  0.8322  0.8322  1.0000  0.7936   \n16    0.9640  ...   0.4588  0.4824  0.3412  0.2236  0.4118  0.5294  0.6942   \n17    0.9616  ...   0.2766  0.2554  0.1914  0.2978  0.2978 -0.0638  0.0212   \n18   -0.4866  ...  -0.1162  0.0388  0.0698 -0.1472  0.0388  0.2558  0.1162   \n19   -0.4224  ...   0.7024  0.5702  0.4710  0.6694  0.8348  0.8182  0.6860   \n20   -0.1418  ...   0.0378  0.0944  0.0188 -0.2642 -0.3774  0.0754  0.5284   \n21    0.0184  ...   0.7846  0.4770  0.5076  0.6924  0.3846  0.3538  0.5692   \n22   -0.0932  ...   0.8410  0.3864 -0.1136  0.3636  0.3182  0.2272  0.4546   \n23   -0.0324  ...   0.1384  0.3538  0.0462  0.4770  0.3538 -0.1692  0.2616   \n24   -0.2234  ...   1.0000  0.8832  0.9708  0.9124  0.7810  0.8686  0.7664   \n25   -0.1020  ...   0.3056  0.0278  0.3056  0.3334  0.1666  0.2500  0.3334   \n26   -0.6360  ...   0.2632  0.0526  0.1316  0.3158  0.3158  0.5000  0.3948   \n27   -0.5654  ...   0.5164  0.2748  0.1648  0.3626  0.2968  0.4506  0.2748   \n28    0.9692  ...   0.1904  0.2380  0.3810  0.4762  0.5952  0.5714  0.8096   \n29    1.0000  ...   0.1154  0.0962  0.0000 -0.2308 -0.1730  0.2116  0.1924   \n...      ...  ...      ...     ...     ...     ...     ...     ...     ...   \n6208  0.6470  ...   0.4242  0.0000  0.3636  0.3030  0.3334  0.3940  0.3334   \n6209 -0.0258  ...   0.5104  0.6224  0.3986  0.2028  0.4966  0.4406  0.3706   \n6210 -0.4974  ...   0.1590 -0.0682 -0.2046  0.0000 -0.1364 -0.0682  0.0228   \n6211 -0.6314  ...   0.6364  0.7402  0.4806  0.6104  0.6364  0.6104  0.6104   \n6212 -0.2964  ...   0.2820  0.4358  0.5384  1.0000  0.8206  0.3334 -0.0256   \n6213 -0.6970  ...   0.5714  0.5476  0.8810  1.0000  0.6904  0.7858  0.5000   \n6214 -0.4454  ...   0.3600  0.3334  0.9466  1.0000  0.8400  0.7066  0.6000   \n6215 -0.5114  ...   0.8934  0.4400  1.0000  0.8400  0.5734  0.5200  0.4934   \n6216 -0.7432  ...   0.5172  0.7242  1.0000  0.8620  0.6552  0.6782  0.9310   \n6217 -0.1446  ...   0.6892  0.8514  0.9190  0.8378  0.8784  0.9864  1.0000   \n6218 -0.3784  ...   0.3392  0.5000  0.7322  0.6786  0.5714  0.6250  0.5536   \n6219 -0.3642  ...   0.7514  0.7870  0.7160  0.7752  0.7514  0.6214  0.8816   \n6220  0.8580  ...   0.0336  0.0202 -0.0738 -0.0068  0.1276  0.1276 -0.0604   \n6221 -0.2576  ...   0.1962  0.4392  0.5514  0.4954  0.5888  0.2336  0.3832   \n6222 -0.1746  ...   0.3636  0.3410  0.8410  1.0000  0.7728  0.5910  0.7728   \n6223 -0.0120  ...   0.6806  0.4118  0.5294  0.5798  0.6638  0.2774  0.4958   \n6224 -0.2896  ...   0.0910  0.2468  0.4026  0.3246  0.2728  0.3896  0.3116   \n6225 -0.4516  ...   0.6508  0.6666  0.4444  0.5080  0.3810  0.2064  0.4126   \n6226  0.7838  ...   0.2500  0.2142  0.1786  0.2500  0.0892 -0.0714 -0.0178   \n6227 -0.6772  ...   0.7684  0.5158  0.6632  0.7264  0.3264  0.3894  0.7052   \n6228 -0.6564  ...   0.3726  0.3726  0.5686  0.9412  0.2942  0.5294  0.2942   \n6229  0.1808  ...   0.1926  0.3028  0.1010  0.2844  0.0642  0.3212  0.3944   \n6230 -0.1734  ...   0.6574  0.6132  0.6796  0.8784  0.9448  0.9116  0.7790   \n6231 -0.8638  ...   0.7592  0.8334  0.9444  1.0000  0.9630  0.6482  0.6666   \n6232 -0.6568  ...  -0.2308 -0.1624 -0.0770  0.2308  0.1966  0.2478  0.1624   \n6233 -0.4878  ...   0.5000  0.6800  0.8200  0.8400  0.8400  0.7400  0.8200   \n6234  0.3004  ...   0.5000  0.2250  0.7500  0.8750  0.6750  0.6000  0.4500   \n6235 -0.1326  ...   0.8068  0.7392  0.7392  0.6908  0.7294  0.7004  0.6812   \n6236 -0.7588  ...   0.0344  0.0344 -0.0344  0.4252  0.2874 -0.0114  0.1034   \n6237 -0.3232  ...  -0.0178 -0.0536  0.5178  1.0000  0.9464  0.2500 -0.0536   \n\n         615     616   617  \n0     0.5898 -0.4872   1.0  \n1     0.4546 -0.0910   1.0  \n2     0.1588 -0.4762   2.0  \n3     0.0936 -0.1510   2.0  \n4     0.2188 -0.2500   3.0  \n5     0.6144  0.3254   3.0  \n6     0.4562 -0.3860   4.0  \n7     0.3670  0.1646   4.0  \n8     0.9246  0.5284   5.0  \n9     0.5162  0.3548   5.0  \n10    0.4932 -0.0548   6.0  \n11    0.5238  0.1746   6.0  \n12    0.1276 -0.1812   7.0  \n13    0.2000 -0.3600   7.0  \n14    0.5778  0.3000   8.0  \n15    0.6646 -0.2000   8.0  \n16    0.6000  0.3648   9.0  \n17    0.1276 -0.2340   9.0  \n18    0.1008 -0.3644  10.0  \n19    0.4380 -0.1240  10.0  \n20    0.5094 -0.3018  11.0  \n21    0.6308  0.4770  11.0  \n22   -0.2500 -0.5228  12.0  \n23    0.0770 -0.3230  12.0  \n24    0.4598  0.2700  13.0  \n25   -0.3612 -0.7500  13.0  \n26    0.3684  0.1578  14.0  \n27   -0.0550 -0.8462  14.0  \n28    0.3334 -0.4762  15.0  \n29    0.4038  0.1346  15.0  \n...      ...     ...   ...  \n6208 -0.4242 -0.7878  23.0  \n6209  0.4686  0.1468  24.0  \n6210 -0.0682 -0.7500  25.0  \n6211 -0.1428 -0.3506  26.0  \n6212  0.2564 -0.5642   1.0  \n6213  0.7142  0.4762   2.0  \n6214  0.8134 -0.0134   3.0  \n6215  0.3866  0.1200   4.0  \n6216  0.8850  0.2644   5.0  \n6217  0.8378  0.7028   6.0  \n6218  0.8572  0.5536   7.0  \n6219  0.6450  0.2662   8.0  \n6220 -0.0068 -0.3960   9.0  \n6221  0.5888  0.3458  10.0  \n6222  0.7046  0.2272  11.0  \n6223  0.5294 -0.0252  12.0  \n6224 -0.2728 -0.4156  13.0  \n6225  0.4126  0.0634  14.0  \n6226 -0.0714 -0.2142  15.0  \n6227  0.7684 -0.0106  16.0  \n6228  0.1764 -0.3138  17.0  \n6229  0.0458 -0.1192  18.0  \n6230  0.8454  0.6574  19.0  \n6231  0.5556  0.3518  20.0  \n6232 -0.3504 -0.5556  21.0  \n6233  0.6400  0.3200  22.0  \n6234 -0.1250 -0.2250  23.0  \n6235  0.5170  0.3430  24.0  \n6236 -0.1954 -0.8620  25.0  \n6237  0.0714 -0.0892  26.0  \n\n[6238 rows x 618 columns]"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "df = pd.read_csv(\"isolet5.data\", header=None,na_values = [\"?\"])\ndf",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 4,
          "data": {
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>608</th>\n      <th>609</th>\n      <th>610</th>\n      <th>611</th>\n      <th>612</th>\n      <th>613</th>\n      <th>614</th>\n      <th>615</th>\n      <th>616</th>\n      <th>617</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-0.2080</td>\n      <td>0.3480</td>\n      <td>0.3280</td>\n      <td>0.5040</td>\n      <td>0.9320</td>\n      <td>1.0000</td>\n      <td>0.8360</td>\n      <td>0.6680</td>\n      <td>0.2720</td>\n      <td>0.2400</td>\n      <td>...</td>\n      <td>0.2500</td>\n      <td>-0.0624</td>\n      <td>0.2188</td>\n      <td>0.4532</td>\n      <td>0.1094</td>\n      <td>0.1718</td>\n      <td>0.1562</td>\n      <td>0.0468</td>\n      <td>-0.3750</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-0.2864</td>\n      <td>0.1992</td>\n      <td>0.2822</td>\n      <td>0.4398</td>\n      <td>0.7012</td>\n      <td>0.7800</td>\n      <td>1.0000</td>\n      <td>0.9792</td>\n      <td>0.5850</td>\n      <td>0.4066</td>\n      <td>...</td>\n      <td>-0.0078</td>\n      <td>-0.1472</td>\n      <td>-0.1782</td>\n      <td>0.0078</td>\n      <td>0.1162</td>\n      <td>-0.0542</td>\n      <td>-0.0542</td>\n      <td>-0.0388</td>\n      <td>-0.7984</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-0.2348</td>\n      <td>0.3826</td>\n      <td>0.6142</td>\n      <td>0.7492</td>\n      <td>0.0546</td>\n      <td>-0.4020</td>\n      <td>-0.3504</td>\n      <td>-0.2990</td>\n      <td>-0.6848</td>\n      <td>-0.6528</td>\n      <td>...</td>\n      <td>0.2834</td>\n      <td>0.1500</td>\n      <td>0.0834</td>\n      <td>-0.2000</td>\n      <td>-0.1834</td>\n      <td>0.0500</td>\n      <td>-0.0166</td>\n      <td>-0.1834</td>\n      <td>-0.8666</td>\n      <td>2.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-0.1856</td>\n      <td>0.3592</td>\n      <td>0.7126</td>\n      <td>0.7366</td>\n      <td>0.3414</td>\n      <td>0.1018</td>\n      <td>-0.1556</td>\n      <td>-0.2514</td>\n      <td>-0.2514</td>\n      <td>-0.3892</td>\n      <td>...</td>\n      <td>0.2840</td>\n      <td>0.5556</td>\n      <td>0.4568</td>\n      <td>0.4568</td>\n      <td>0.4568</td>\n      <td>0.2098</td>\n      <td>0.0370</td>\n      <td>-0.0618</td>\n      <td>-0.3334</td>\n      <td>2.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-0.1814</td>\n      <td>0.4404</td>\n      <td>0.8394</td>\n      <td>1.0000</td>\n      <td>0.7564</td>\n      <td>0.1866</td>\n      <td>0.0260</td>\n      <td>-0.0726</td>\n      <td>-0.2124</td>\n      <td>-0.3730</td>\n      <td>...</td>\n      <td>0.1688</td>\n      <td>-0.1688</td>\n      <td>0.2728</td>\n      <td>0.2988</td>\n      <td>0.2468</td>\n      <td>0.1948</td>\n      <td>-0.0130</td>\n      <td>-0.2988</td>\n      <td>-0.7662</td>\n      <td>3.0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>-0.3212</td>\n      <td>0.6242</td>\n      <td>0.6424</td>\n      <td>0.6666</td>\n      <td>0.5090</td>\n      <td>0.1454</td>\n      <td>0.0060</td>\n      <td>-0.1454</td>\n      <td>-0.2606</td>\n      <td>-0.1940</td>\n      <td>...</td>\n      <td>0.4528</td>\n      <td>0.3584</td>\n      <td>0.4906</td>\n      <td>0.2830</td>\n      <td>0.3584</td>\n      <td>0.6792</td>\n      <td>0.3018</td>\n      <td>0.1698</td>\n      <td>-0.2642</td>\n      <td>3.0</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>-0.1312</td>\n      <td>0.5178</td>\n      <td>0.5752</td>\n      <td>0.6754</td>\n      <td>0.2984</td>\n      <td>-0.1694</td>\n      <td>-0.1980</td>\n      <td>-0.2028</td>\n      <td>-0.4510</td>\n      <td>-0.5418</td>\n      <td>...</td>\n      <td>0.1314</td>\n      <td>0.0708</td>\n      <td>0.0708</td>\n      <td>0.2930</td>\n      <td>0.0910</td>\n      <td>0.1516</td>\n      <td>-0.0506</td>\n      <td>-0.0910</td>\n      <td>-0.7778</td>\n      <td>4.0</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0.0090</td>\n      <td>0.7374</td>\n      <td>0.6298</td>\n      <td>0.5820</td>\n      <td>0.1880</td>\n      <td>-0.0806</td>\n      <td>-0.2836</td>\n      <td>-0.5284</td>\n      <td>-0.5164</td>\n      <td>-0.7612</td>\n      <td>...</td>\n      <td>-0.1428</td>\n      <td>0.0990</td>\n      <td>0.1868</td>\n      <td>0.4506</td>\n      <td>0.4726</td>\n      <td>0.2528</td>\n      <td>-0.0330</td>\n      <td>-0.1868</td>\n      <td>-0.8022</td>\n      <td>4.0</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>-0.4392</td>\n      <td>0.5782</td>\n      <td>0.6426</td>\n      <td>0.4094</td>\n      <td>-0.1762</td>\n      <td>-0.3896</td>\n      <td>-0.3846</td>\n      <td>-0.3548</td>\n      <td>-0.7022</td>\n      <td>-0.7072</td>\n      <td>...</td>\n      <td>0.6000</td>\n      <td>-0.1810</td>\n      <td>0.1428</td>\n      <td>0.2952</td>\n      <td>0.2380</td>\n      <td>0.2000</td>\n      <td>0.2000</td>\n      <td>0.0096</td>\n      <td>-0.7714</td>\n      <td>5.0</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>-0.3154</td>\n      <td>0.1868</td>\n      <td>0.2614</td>\n      <td>0.3776</td>\n      <td>0.0912</td>\n      <td>-0.1660</td>\n      <td>-0.4854</td>\n      <td>-0.4772</td>\n      <td>-0.4440</td>\n      <td>-0.5186</td>\n      <td>...</td>\n      <td>0.3138</td>\n      <td>0.2550</td>\n      <td>0.2942</td>\n      <td>0.5294</td>\n      <td>0.0196</td>\n      <td>0.1176</td>\n      <td>0.4510</td>\n      <td>0.1764</td>\n      <td>-0.9608</td>\n      <td>5.0</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>-0.2344</td>\n      <td>0.3124</td>\n      <td>0.3204</td>\n      <td>0.2656</td>\n      <td>0.4296</td>\n      <td>0.6328</td>\n      <td>1.0000</td>\n      <td>0.8750</td>\n      <td>0.6484</td>\n      <td>0.2968</td>\n      <td>...</td>\n      <td>0.6590</td>\n      <td>0.6124</td>\n      <td>0.9534</td>\n      <td>1.0000</td>\n      <td>0.8140</td>\n      <td>0.6590</td>\n      <td>0.7520</td>\n      <td>0.5814</td>\n      <td>-0.0078</td>\n      <td>6.0</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>-0.4286</td>\n      <td>0.1160</td>\n      <td>0.2946</td>\n      <td>0.4464</td>\n      <td>0.6250</td>\n      <td>0.7054</td>\n      <td>1.0000</td>\n      <td>1.0000</td>\n      <td>0.8840</td>\n      <td>0.7410</td>\n      <td>...</td>\n      <td>0.6000</td>\n      <td>0.6952</td>\n      <td>0.7142</td>\n      <td>1.0000</td>\n      <td>0.6190</td>\n      <td>0.4666</td>\n      <td>0.4858</td>\n      <td>0.1810</td>\n      <td>-0.0096</td>\n      <td>6.0</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>-0.2070</td>\n      <td>0.2140</td>\n      <td>0.7052</td>\n      <td>0.7192</td>\n      <td>-0.0386</td>\n      <td>-0.2982</td>\n      <td>-0.4808</td>\n      <td>-0.5578</td>\n      <td>-0.5088</td>\n      <td>-0.4666</td>\n      <td>...</td>\n      <td>0.5284</td>\n      <td>0.5284</td>\n      <td>0.3584</td>\n      <td>0.5284</td>\n      <td>0.6226</td>\n      <td>0.5472</td>\n      <td>0.3396</td>\n      <td>0.3208</td>\n      <td>-0.1132</td>\n      <td>7.0</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>-0.3064</td>\n      <td>0.0582</td>\n      <td>0.6050</td>\n      <td>0.6506</td>\n      <td>0.0126</td>\n      <td>-0.2406</td>\n      <td>-0.3924</td>\n      <td>-0.3772</td>\n      <td>-0.4228</td>\n      <td>-0.6860</td>\n      <td>...</td>\n      <td>0.2400</td>\n      <td>0.0800</td>\n      <td>0.2400</td>\n      <td>0.4000</td>\n      <td>0.5600</td>\n      <td>0.5800</td>\n      <td>0.2200</td>\n      <td>-0.1600</td>\n      <td>-0.4200</td>\n      <td>7.0</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>-0.1626</td>\n      <td>0.2808</td>\n      <td>0.8522</td>\n      <td>1.0000</td>\n      <td>0.5566</td>\n      <td>0.3400</td>\n      <td>0.1134</td>\n      <td>-0.1822</td>\n      <td>-0.3104</td>\n      <td>-0.4286</td>\n      <td>...</td>\n      <td>0.6288</td>\n      <td>0.6702</td>\n      <td>0.5980</td>\n      <td>0.6494</td>\n      <td>0.5154</td>\n      <td>0.5980</td>\n      <td>0.3506</td>\n      <td>0.2578</td>\n      <td>-0.1238</td>\n      <td>8.0</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>-0.3312</td>\n      <td>0.0254</td>\n      <td>0.5160</td>\n      <td>0.5606</td>\n      <td>0.1974</td>\n      <td>0.1082</td>\n      <td>0.0636</td>\n      <td>-0.1146</td>\n      <td>-0.1464</td>\n      <td>-0.4586</td>\n      <td>...</td>\n      <td>0.7142</td>\n      <td>0.7402</td>\n      <td>0.3636</td>\n      <td>0.2338</td>\n      <td>0.2858</td>\n      <td>0.2468</td>\n      <td>0.3246</td>\n      <td>-0.0390</td>\n      <td>-0.4806</td>\n      <td>8.0</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>-0.4786</td>\n      <td>-0.2022</td>\n      <td>0.1254</td>\n      <td>0.1282</td>\n      <td>0.1994</td>\n      <td>0.4700</td>\n      <td>0.6924</td>\n      <td>1.0000</td>\n      <td>0.8576</td>\n      <td>0.7122</td>\n      <td>...</td>\n      <td>0.3442</td>\n      <td>0.0656</td>\n      <td>0.0328</td>\n      <td>-0.0164</td>\n      <td>-0.1148</td>\n      <td>0.0492</td>\n      <td>0.0820</td>\n      <td>0.0820</td>\n      <td>-0.5738</td>\n      <td>9.0</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>-0.4178</td>\n      <td>-0.0182</td>\n      <td>0.1196</td>\n      <td>0.1814</td>\n      <td>0.2630</td>\n      <td>0.5134</td>\n      <td>0.7552</td>\n      <td>0.9888</td>\n      <td>1.0000</td>\n      <td>0.6624</td>\n      <td>...</td>\n      <td>0.3334</td>\n      <td>0.1404</td>\n      <td>-0.3158</td>\n      <td>0.0000</td>\n      <td>-0.1052</td>\n      <td>-0.3334</td>\n      <td>-0.2632</td>\n      <td>-0.3334</td>\n      <td>-0.5088</td>\n      <td>9.0</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>-0.2008</td>\n      <td>0.3400</td>\n      <td>0.7336</td>\n      <td>0.8608</td>\n      <td>0.6820</td>\n      <td>0.5030</td>\n      <td>0.2326</td>\n      <td>-0.0218</td>\n      <td>-0.1690</td>\n      <td>-0.2088</td>\n      <td>...</td>\n      <td>0.2500</td>\n      <td>0.1250</td>\n      <td>-0.2292</td>\n      <td>0.2500</td>\n      <td>0.0416</td>\n      <td>-0.1458</td>\n      <td>0.2292</td>\n      <td>-0.1666</td>\n      <td>-0.7500</td>\n      <td>10.0</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>-0.0996</td>\n      <td>0.5974</td>\n      <td>0.6926</td>\n      <td>1.0000</td>\n      <td>0.9308</td>\n      <td>0.7056</td>\n      <td>0.3810</td>\n      <td>0.1992</td>\n      <td>-0.1428</td>\n      <td>-0.0822</td>\n      <td>...</td>\n      <td>0.3388</td>\n      <td>-0.0806</td>\n      <td>0.1452</td>\n      <td>0.2742</td>\n      <td>0.3064</td>\n      <td>0.0968</td>\n      <td>-0.1612</td>\n      <td>-0.2258</td>\n      <td>-0.4032</td>\n      <td>10.0</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>-0.2060</td>\n      <td>0.4034</td>\n      <td>0.3304</td>\n      <td>0.5236</td>\n      <td>0.7982</td>\n      <td>0.9356</td>\n      <td>1.0000</td>\n      <td>0.9700</td>\n      <td>0.5622</td>\n      <td>0.4806</td>\n      <td>...</td>\n      <td>0.4906</td>\n      <td>0.0944</td>\n      <td>0.3208</td>\n      <td>0.3396</td>\n      <td>0.4150</td>\n      <td>0.6792</td>\n      <td>0.3962</td>\n      <td>0.0378</td>\n      <td>-0.3208</td>\n      <td>11.0</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>-0.2948</td>\n      <td>0.4722</td>\n      <td>0.4762</td>\n      <td>0.5258</td>\n      <td>0.5958</td>\n      <td>1.0000</td>\n      <td>0.9010</td>\n      <td>0.8598</td>\n      <td>0.6948</td>\n      <td>0.3196</td>\n      <td>...</td>\n      <td>0.4364</td>\n      <td>0.1454</td>\n      <td>0.1272</td>\n      <td>0.3272</td>\n      <td>0.4728</td>\n      <td>0.0910</td>\n      <td>0.0182</td>\n      <td>-0.2728</td>\n      <td>-0.3818</td>\n      <td>11.0</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>-0.4322</td>\n      <td>-0.0928</td>\n      <td>0.2286</td>\n      <td>0.2500</td>\n      <td>0.4286</td>\n      <td>0.7822</td>\n      <td>1.0000</td>\n      <td>0.9642</td>\n      <td>0.6000</td>\n      <td>0.4358</td>\n      <td>...</td>\n      <td>0.7088</td>\n      <td>0.5922</td>\n      <td>0.6700</td>\n      <td>0.5728</td>\n      <td>0.8058</td>\n      <td>0.6116</td>\n      <td>0.6504</td>\n      <td>0.1456</td>\n      <td>0.0486</td>\n      <td>12.0</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>-0.3334</td>\n      <td>0.1846</td>\n      <td>0.2770</td>\n      <td>0.2666</td>\n      <td>0.5230</td>\n      <td>0.8154</td>\n      <td>0.9692</td>\n      <td>0.8666</td>\n      <td>0.6820</td>\n      <td>0.5744</td>\n      <td>...</td>\n      <td>0.9174</td>\n      <td>0.9670</td>\n      <td>0.7024</td>\n      <td>1.0000</td>\n      <td>0.9338</td>\n      <td>0.6198</td>\n      <td>0.1404</td>\n      <td>0.0578</td>\n      <td>-0.1736</td>\n      <td>12.0</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>-0.3764</td>\n      <td>0.0168</td>\n      <td>0.2758</td>\n      <td>0.2662</td>\n      <td>0.6642</td>\n      <td>0.8514</td>\n      <td>0.8322</td>\n      <td>0.7506</td>\n      <td>0.6020</td>\n      <td>0.3478</td>\n      <td>...</td>\n      <td>0.3832</td>\n      <td>0.1028</td>\n      <td>0.1588</td>\n      <td>0.1776</td>\n      <td>0.0654</td>\n      <td>0.2150</td>\n      <td>-0.0468</td>\n      <td>-0.1402</td>\n      <td>-0.6636</td>\n      <td>13.0</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>-0.4488</td>\n      <td>-0.1472</td>\n      <td>0.1844</td>\n      <td>0.2216</td>\n      <td>0.4674</td>\n      <td>0.6536</td>\n      <td>0.6908</td>\n      <td>0.6350</td>\n      <td>0.4040</td>\n      <td>0.3594</td>\n      <td>...</td>\n      <td>0.2564</td>\n      <td>0.0256</td>\n      <td>0.2820</td>\n      <td>0.3334</td>\n      <td>0.1026</td>\n      <td>0.2052</td>\n      <td>0.0256</td>\n      <td>-0.1794</td>\n      <td>-0.7436</td>\n      <td>13.0</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>-0.3474</td>\n      <td>0.1336</td>\n      <td>0.2214</td>\n      <td>0.2634</td>\n      <td>0.5764</td>\n      <td>0.6564</td>\n      <td>0.7328</td>\n      <td>0.5114</td>\n      <td>0.3702</td>\n      <td>0.3244</td>\n      <td>...</td>\n      <td>0.6924</td>\n      <td>0.3654</td>\n      <td>0.3462</td>\n      <td>0.5962</td>\n      <td>0.6924</td>\n      <td>0.4230</td>\n      <td>0.4038</td>\n      <td>0.0576</td>\n      <td>-0.4808</td>\n      <td>14.0</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>-0.1826</td>\n      <td>0.4748</td>\n      <td>0.4156</td>\n      <td>0.5022</td>\n      <td>0.9132</td>\n      <td>1.0000</td>\n      <td>0.8310</td>\n      <td>0.8812</td>\n      <td>0.5480</td>\n      <td>0.4338</td>\n      <td>...</td>\n      <td>0.1504</td>\n      <td>0.2744</td>\n      <td>0.3098</td>\n      <td>0.2744</td>\n      <td>0.5752</td>\n      <td>0.7168</td>\n      <td>0.5576</td>\n      <td>0.3982</td>\n      <td>0.0974</td>\n      <td>14.0</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>-0.4516</td>\n      <td>-0.0888</td>\n      <td>0.1654</td>\n      <td>0.2178</td>\n      <td>0.4880</td>\n      <td>0.8266</td>\n      <td>0.8952</td>\n      <td>0.6370</td>\n      <td>0.5444</td>\n      <td>0.3186</td>\n      <td>...</td>\n      <td>0.7608</td>\n      <td>0.3914</td>\n      <td>0.2174</td>\n      <td>0.6086</td>\n      <td>0.9348</td>\n      <td>0.7826</td>\n      <td>0.8696</td>\n      <td>0.4566</td>\n      <td>-0.0218</td>\n      <td>15.0</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>-0.2564</td>\n      <td>0.2886</td>\n      <td>0.4596</td>\n      <td>0.5566</td>\n      <td>0.8984</td>\n      <td>0.9030</td>\n      <td>1.0000</td>\n      <td>0.9308</td>\n      <td>0.6166</td>\n      <td>0.4734</td>\n      <td>...</td>\n      <td>0.1590</td>\n      <td>0.5682</td>\n      <td>0.3864</td>\n      <td>0.2272</td>\n      <td>0.3182</td>\n      <td>0.4090</td>\n      <td>0.2500</td>\n      <td>0.1818</td>\n      <td>-0.5228</td>\n      <td>15.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1529</th>\n      <td>-0.6990</td>\n      <td>-0.3870</td>\n      <td>-0.2114</td>\n      <td>0.2940</td>\n      <td>0.6200</td>\n      <td>0.5018</td>\n      <td>0.1936</td>\n      <td>-0.1900</td>\n      <td>-0.0610</td>\n      <td>-0.0430</td>\n      <td>...</td>\n      <td>0.0666</td>\n      <td>-0.0814</td>\n      <td>-0.1112</td>\n      <td>0.0518</td>\n      <td>-0.0074</td>\n      <td>0.0962</td>\n      <td>0.0222</td>\n      <td>-0.4370</td>\n      <td>-0.6888</td>\n      <td>12.0</td>\n    </tr>\n    <tr>\n      <th>1530</th>\n      <td>-0.7204</td>\n      <td>-0.4226</td>\n      <td>-0.3142</td>\n      <td>0.3954</td>\n      <td>0.7354</td>\n      <td>0.6452</td>\n      <td>0.2210</td>\n      <td>-0.0286</td>\n      <td>-0.0766</td>\n      <td>-0.1338</td>\n      <td>...</td>\n      <td>0.6950</td>\n      <td>0.3220</td>\n      <td>0.2204</td>\n      <td>0.7288</td>\n      <td>0.8306</td>\n      <td>0.4238</td>\n      <td>0.6272</td>\n      <td>0.4916</td>\n      <td>-0.1186</td>\n      <td>12.0</td>\n    </tr>\n    <tr>\n      <th>1531</th>\n      <td>-0.6998</td>\n      <td>-0.3716</td>\n      <td>-0.2792</td>\n      <td>-0.1220</td>\n      <td>0.4586</td>\n      <td>0.6270</td>\n      <td>0.4026</td>\n      <td>0.0604</td>\n      <td>-0.0098</td>\n      <td>0.0546</td>\n      <td>...</td>\n      <td>0.5000</td>\n      <td>0.5000</td>\n      <td>0.2826</td>\n      <td>0.0000</td>\n      <td>0.1956</td>\n      <td>0.1086</td>\n      <td>0.2826</td>\n      <td>-0.0434</td>\n      <td>-0.4782</td>\n      <td>13.0</td>\n    </tr>\n    <tr>\n      <th>1532</th>\n      <td>-0.6426</td>\n      <td>-0.3728</td>\n      <td>-0.2142</td>\n      <td>0.2946</td>\n      <td>0.5054</td>\n      <td>0.4178</td>\n      <td>0.0178</td>\n      <td>-0.1408</td>\n      <td>-0.1550</td>\n      <td>-0.0366</td>\n      <td>...</td>\n      <td>0.6632</td>\n      <td>0.3264</td>\n      <td>0.3474</td>\n      <td>0.5158</td>\n      <td>0.1578</td>\n      <td>0.4106</td>\n      <td>0.2210</td>\n      <td>0.3052</td>\n      <td>-0.2632</td>\n      <td>13.0</td>\n    </tr>\n    <tr>\n      <th>1533</th>\n      <td>-0.7230</td>\n      <td>-0.2998</td>\n      <td>-0.2542</td>\n      <td>-0.1112</td>\n      <td>0.4186</td>\n      <td>0.5190</td>\n      <td>0.1354</td>\n      <td>-0.2208</td>\n      <td>-0.1872</td>\n      <td>-0.1628</td>\n      <td>...</td>\n      <td>0.1052</td>\n      <td>-0.1404</td>\n      <td>0.1754</td>\n      <td>0.2632</td>\n      <td>0.1754</td>\n      <td>0.0350</td>\n      <td>-0.0350</td>\n      <td>0.0176</td>\n      <td>-0.7018</td>\n      <td>14.0</td>\n    </tr>\n    <tr>\n      <th>1534</th>\n      <td>-0.6600</td>\n      <td>-0.3630</td>\n      <td>-0.3036</td>\n      <td>-0.0284</td>\n      <td>0.5060</td>\n      <td>0.5654</td>\n      <td>0.4332</td>\n      <td>-0.0122</td>\n      <td>-0.0770</td>\n      <td>-0.0742</td>\n      <td>...</td>\n      <td>0.3904</td>\n      <td>0.5428</td>\n      <td>0.3904</td>\n      <td>0.3334</td>\n      <td>0.3334</td>\n      <td>0.3714</td>\n      <td>0.1048</td>\n      <td>0.4666</td>\n      <td>0.0096</td>\n      <td>14.0</td>\n    </tr>\n    <tr>\n      <th>1535</th>\n      <td>-0.7114</td>\n      <td>-0.3586</td>\n      <td>-0.1546</td>\n      <td>0.3760</td>\n      <td>0.6998</td>\n      <td>0.7872</td>\n      <td>0.3848</td>\n      <td>0.3032</td>\n      <td>0.6968</td>\n      <td>1.0000</td>\n      <td>...</td>\n      <td>0.2958</td>\n      <td>0.3802</td>\n      <td>0.3522</td>\n      <td>0.0986</td>\n      <td>0.5212</td>\n      <td>0.1268</td>\n      <td>0.0422</td>\n      <td>0.1830</td>\n      <td>-0.1830</td>\n      <td>15.0</td>\n    </tr>\n    <tr>\n      <th>1536</th>\n      <td>-0.7338</td>\n      <td>-0.5768</td>\n      <td>-0.4266</td>\n      <td>0.2492</td>\n      <td>0.4574</td>\n      <td>0.4778</td>\n      <td>0.1980</td>\n      <td>0.1638</td>\n      <td>0.9864</td>\n      <td>1.0000</td>\n      <td>...</td>\n      <td>0.4762</td>\n      <td>0.3334</td>\n      <td>0.4762</td>\n      <td>0.7142</td>\n      <td>0.3810</td>\n      <td>0.2620</td>\n      <td>0.0714</td>\n      <td>0.1190</td>\n      <td>-0.5000</td>\n      <td>15.0</td>\n    </tr>\n    <tr>\n      <th>1537</th>\n      <td>0.5888</td>\n      <td>0.4274</td>\n      <td>-0.3870</td>\n      <td>-0.8226</td>\n      <td>-0.8870</td>\n      <td>-0.8630</td>\n      <td>-0.7580</td>\n      <td>-0.7822</td>\n      <td>-0.9516</td>\n      <td>-0.8870</td>\n      <td>...</td>\n      <td>0.1098</td>\n      <td>0.0550</td>\n      <td>0.0110</td>\n      <td>-0.0330</td>\n      <td>0.0330</td>\n      <td>0.0110</td>\n      <td>0.0220</td>\n      <td>0.1208</td>\n      <td>-0.2418</td>\n      <td>16.0</td>\n    </tr>\n    <tr>\n      <th>1538</th>\n      <td>-0.7778</td>\n      <td>-0.2790</td>\n      <td>0.0074</td>\n      <td>0.0518</td>\n      <td>-0.3284</td>\n      <td>-0.6098</td>\n      <td>-0.7976</td>\n      <td>-0.6790</td>\n      <td>-0.6790</td>\n      <td>-0.6790</td>\n      <td>...</td>\n      <td>0.4640</td>\n      <td>0.2784</td>\n      <td>0.4640</td>\n      <td>0.7938</td>\n      <td>0.5876</td>\n      <td>0.3402</td>\n      <td>0.0722</td>\n      <td>0.0310</td>\n      <td>-0.2990</td>\n      <td>16.0</td>\n    </tr>\n    <tr>\n      <th>1539</th>\n      <td>-0.5012</td>\n      <td>0.0272</td>\n      <td>0.3630</td>\n      <td>0.3876</td>\n      <td>-0.3976</td>\n      <td>-0.5210</td>\n      <td>-0.6198</td>\n      <td>-0.6740</td>\n      <td>-0.5704</td>\n      <td>-0.5210</td>\n      <td>...</td>\n      <td>0.7288</td>\n      <td>0.4576</td>\n      <td>0.0170</td>\n      <td>0.2542</td>\n      <td>0.5932</td>\n      <td>0.6272</td>\n      <td>0.5932</td>\n      <td>-0.0848</td>\n      <td>-0.6950</td>\n      <td>17.0</td>\n    </tr>\n    <tr>\n      <th>1540</th>\n      <td>-0.6100</td>\n      <td>-0.2908</td>\n      <td>0.0532</td>\n      <td>0.0602</td>\n      <td>-0.5710</td>\n      <td>-0.8936</td>\n      <td>-0.8510</td>\n      <td>-0.7978</td>\n      <td>-0.8724</td>\n      <td>-0.8298</td>\n      <td>...</td>\n      <td>0.4888</td>\n      <td>0.6222</td>\n      <td>0.0888</td>\n      <td>0.7112</td>\n      <td>0.3334</td>\n      <td>0.6666</td>\n      <td>0.6666</td>\n      <td>0.6444</td>\n      <td>0.2666</td>\n      <td>17.0</td>\n    </tr>\n    <tr>\n      <th>1541</th>\n      <td>-0.7250</td>\n      <td>-0.5844</td>\n      <td>-0.3992</td>\n      <td>-0.1988</td>\n      <td>0.6292</td>\n      <td>1.0000</td>\n      <td>0.9642</td>\n      <td>0.8984</td>\n      <td>0.9162</td>\n      <td>0.9910</td>\n      <td>...</td>\n      <td>0.2874</td>\n      <td>0.5632</td>\n      <td>0.4482</td>\n      <td>0.6782</td>\n      <td>0.3334</td>\n      <td>0.1954</td>\n      <td>0.3104</td>\n      <td>0.0114</td>\n      <td>-0.2644</td>\n      <td>18.0</td>\n    </tr>\n    <tr>\n      <th>1542</th>\n      <td>-0.6846</td>\n      <td>-0.5768</td>\n      <td>-0.4132</td>\n      <td>0.0340</td>\n      <td>0.7486</td>\n      <td>0.9560</td>\n      <td>0.7524</td>\n      <td>0.7844</td>\n      <td>1.0000</td>\n      <td>0.9880</td>\n      <td>...</td>\n      <td>0.4728</td>\n      <td>0.2000</td>\n      <td>0.2364</td>\n      <td>0.2910</td>\n      <td>-0.0182</td>\n      <td>0.1090</td>\n      <td>0.1090</td>\n      <td>0.0910</td>\n      <td>-0.7636</td>\n      <td>18.0</td>\n    </tr>\n    <tr>\n      <th>1543</th>\n      <td>-0.6350</td>\n      <td>-0.3460</td>\n      <td>-0.3004</td>\n      <td>0.2776</td>\n      <td>0.7566</td>\n      <td>0.6122</td>\n      <td>0.3840</td>\n      <td>0.0646</td>\n      <td>-0.2168</td>\n      <td>-0.0494</td>\n      <td>...</td>\n      <td>1.0000</td>\n      <td>0.9488</td>\n      <td>0.8974</td>\n      <td>0.7436</td>\n      <td>0.6512</td>\n      <td>0.5794</td>\n      <td>0.6718</td>\n      <td>0.7538</td>\n      <td>0.4974</td>\n      <td>19.0</td>\n    </tr>\n    <tr>\n      <th>1544</th>\n      <td>-0.7304</td>\n      <td>-0.5392</td>\n      <td>-0.4174</td>\n      <td>0.0086</td>\n      <td>0.6174</td>\n      <td>0.7304</td>\n      <td>0.5130</td>\n      <td>0.0956</td>\n      <td>-0.1044</td>\n      <td>-0.0260</td>\n      <td>...</td>\n      <td>0.7112</td>\n      <td>0.6334</td>\n      <td>0.6112</td>\n      <td>0.7778</td>\n      <td>0.8112</td>\n      <td>0.4000</td>\n      <td>0.5222</td>\n      <td>0.6556</td>\n      <td>0.3666</td>\n      <td>19.0</td>\n    </tr>\n    <tr>\n      <th>1545</th>\n      <td>-0.6022</td>\n      <td>0.0000</td>\n      <td>0.3940</td>\n      <td>0.3372</td>\n      <td>-0.2840</td>\n      <td>-0.4734</td>\n      <td>-0.5910</td>\n      <td>-0.5758</td>\n      <td>-0.5228</td>\n      <td>-0.5000</td>\n      <td>...</td>\n      <td>0.4246</td>\n      <td>0.3698</td>\n      <td>0.4794</td>\n      <td>0.4246</td>\n      <td>0.4246</td>\n      <td>0.0136</td>\n      <td>0.2328</td>\n      <td>-0.4520</td>\n      <td>-0.6164</td>\n      <td>20.0</td>\n    </tr>\n    <tr>\n      <th>1546</th>\n      <td>-0.6666</td>\n      <td>-0.1706</td>\n      <td>0.1746</td>\n      <td>0.1866</td>\n      <td>-0.1468</td>\n      <td>-0.4802</td>\n      <td>-0.5158</td>\n      <td>-0.5754</td>\n      <td>-0.5992</td>\n      <td>-0.5476</td>\n      <td>...</td>\n      <td>0.0878</td>\n      <td>0.7018</td>\n      <td>0.6140</td>\n      <td>0.1052</td>\n      <td>-0.1228</td>\n      <td>-0.2106</td>\n      <td>-0.1930</td>\n      <td>0.0526</td>\n      <td>-0.1052</td>\n      <td>20.0</td>\n    </tr>\n    <tr>\n      <th>1547</th>\n      <td>-0.5570</td>\n      <td>-0.2982</td>\n      <td>-0.2130</td>\n      <td>-0.2948</td>\n      <td>-0.6798</td>\n      <td>-0.9284</td>\n      <td>-0.9966</td>\n      <td>-0.9728</td>\n      <td>-0.9318</td>\n      <td>-0.8604</td>\n      <td>...</td>\n      <td>0.4318</td>\n      <td>0.3410</td>\n      <td>0.4772</td>\n      <td>0.5454</td>\n      <td>0.5000</td>\n      <td>0.3410</td>\n      <td>0.6818</td>\n      <td>0.1818</td>\n      <td>0.1136</td>\n      <td>21.0</td>\n    </tr>\n    <tr>\n      <th>1548</th>\n      <td>-0.4906</td>\n      <td>-0.2496</td>\n      <td>-0.2346</td>\n      <td>-0.4204</td>\n      <td>-0.6888</td>\n      <td>-0.9624</td>\n      <td>-0.8920</td>\n      <td>-0.7666</td>\n      <td>-0.8946</td>\n      <td>-0.8068</td>\n      <td>...</td>\n      <td>0.5366</td>\n      <td>0.5122</td>\n      <td>0.2682</td>\n      <td>0.8536</td>\n      <td>0.8048</td>\n      <td>0.5610</td>\n      <td>0.4634</td>\n      <td>0.3658</td>\n      <td>0.0000</td>\n      <td>21.0</td>\n    </tr>\n    <tr>\n      <th>1549</th>\n      <td>-0.6908</td>\n      <td>-0.3864</td>\n      <td>0.0822</td>\n      <td>0.1112</td>\n      <td>-0.4252</td>\n      <td>-0.7972</td>\n      <td>-0.7150</td>\n      <td>-0.7584</td>\n      <td>-0.7682</td>\n      <td>-0.6522</td>\n      <td>...</td>\n      <td>0.1320</td>\n      <td>0.2264</td>\n      <td>-0.0188</td>\n      <td>0.2264</td>\n      <td>0.2264</td>\n      <td>-0.0566</td>\n      <td>0.0188</td>\n      <td>0.0566</td>\n      <td>-0.4716</td>\n      <td>22.0</td>\n    </tr>\n    <tr>\n      <th>1550</th>\n      <td>-0.6266</td>\n      <td>-0.2286</td>\n      <td>0.3472</td>\n      <td>0.3996</td>\n      <td>-0.4486</td>\n      <td>-0.7906</td>\n      <td>-0.8500</td>\n      <td>-0.7976</td>\n      <td>-0.7382</td>\n      <td>-0.7940</td>\n      <td>...</td>\n      <td>0.0000</td>\n      <td>0.0678</td>\n      <td>0.1016</td>\n      <td>0.2542</td>\n      <td>0.1186</td>\n      <td>-0.1186</td>\n      <td>0.1356</td>\n      <td>-0.1526</td>\n      <td>-0.2712</td>\n      <td>22.0</td>\n    </tr>\n    <tr>\n      <th>1551</th>\n      <td>-0.6060</td>\n      <td>-0.2828</td>\n      <td>0.1010</td>\n      <td>0.4848</td>\n      <td>0.5960</td>\n      <td>0.4748</td>\n      <td>0.1414</td>\n      <td>0.0708</td>\n      <td>0.2424</td>\n      <td>0.3232</td>\n      <td>...</td>\n      <td>-0.0278</td>\n      <td>-0.2084</td>\n      <td>-0.0834</td>\n      <td>0.2638</td>\n      <td>0.1388</td>\n      <td>0.0694</td>\n      <td>-0.0694</td>\n      <td>-0.3888</td>\n      <td>-0.9028</td>\n      <td>23.0</td>\n    </tr>\n    <tr>\n      <th>1552</th>\n      <td>-0.4610</td>\n      <td>-0.2056</td>\n      <td>0.4894</td>\n      <td>0.7730</td>\n      <td>0.9008</td>\n      <td>0.8156</td>\n      <td>0.6312</td>\n      <td>0.2766</td>\n      <td>-0.0922</td>\n      <td>-0.3334</td>\n      <td>...</td>\n      <td>-0.4474</td>\n      <td>-0.2764</td>\n      <td>-0.3026</td>\n      <td>-0.2106</td>\n      <td>-0.1316</td>\n      <td>-0.2500</td>\n      <td>-0.2764</td>\n      <td>-0.4736</td>\n      <td>-0.9210</td>\n      <td>23.0</td>\n    </tr>\n    <tr>\n      <th>1553</th>\n      <td>-0.6232</td>\n      <td>-0.2682</td>\n      <td>-0.1376</td>\n      <td>0.3478</td>\n      <td>0.8478</td>\n      <td>0.8260</td>\n      <td>0.3334</td>\n      <td>0.2028</td>\n      <td>0.1450</td>\n      <td>0.0652</td>\n      <td>...</td>\n      <td>0.8266</td>\n      <td>0.5954</td>\n      <td>0.6532</td>\n      <td>0.6648</td>\n      <td>0.6648</td>\n      <td>0.6070</td>\n      <td>0.6416</td>\n      <td>0.5492</td>\n      <td>0.4104</td>\n      <td>24.0</td>\n    </tr>\n    <tr>\n      <th>1554</th>\n      <td>-0.6842</td>\n      <td>-0.3280</td>\n      <td>-0.1984</td>\n      <td>0.2956</td>\n      <td>0.8786</td>\n      <td>0.8948</td>\n      <td>0.3118</td>\n      <td>0.1822</td>\n      <td>0.1012</td>\n      <td>0.1740</td>\n      <td>...</td>\n      <td>0.7738</td>\n      <td>0.7738</td>\n      <td>0.7142</td>\n      <td>0.6428</td>\n      <td>0.5952</td>\n      <td>0.5714</td>\n      <td>0.3928</td>\n      <td>0.4286</td>\n      <td>0.2858</td>\n      <td>24.0</td>\n    </tr>\n    <tr>\n      <th>1555</th>\n      <td>-0.5912</td>\n      <td>-0.2420</td>\n      <td>0.8174</td>\n      <td>1.0000</td>\n      <td>0.4642</td>\n      <td>0.6428</td>\n      <td>0.6944</td>\n      <td>0.3056</td>\n      <td>-0.3888</td>\n      <td>-0.6826</td>\n      <td>...</td>\n      <td>0.1924</td>\n      <td>-0.1154</td>\n      <td>0.0192</td>\n      <td>0.2116</td>\n      <td>-0.0384</td>\n      <td>0.0192</td>\n      <td>-0.2308</td>\n      <td>-0.4230</td>\n      <td>-0.7116</td>\n      <td>25.0</td>\n    </tr>\n    <tr>\n      <th>1556</th>\n      <td>-0.6696</td>\n      <td>-0.3730</td>\n      <td>0.1584</td>\n      <td>0.8910</td>\n      <td>1.0000</td>\n      <td>0.9762</td>\n      <td>0.9762</td>\n      <td>0.7684</td>\n      <td>0.4106</td>\n      <td>0.0154</td>\n      <td>...</td>\n      <td>0.0910</td>\n      <td>0.1818</td>\n      <td>0.2000</td>\n      <td>0.1454</td>\n      <td>0.0182</td>\n      <td>-0.2910</td>\n      <td>0.0728</td>\n      <td>0.0728</td>\n      <td>-0.5818</td>\n      <td>25.0</td>\n    </tr>\n    <tr>\n      <th>1557</th>\n      <td>-0.5764</td>\n      <td>-0.1764</td>\n      <td>0.5106</td>\n      <td>0.3742</td>\n      <td>-0.1670</td>\n      <td>-0.5858</td>\n      <td>-0.7882</td>\n      <td>-0.7224</td>\n      <td>-0.6330</td>\n      <td>-0.8212</td>\n      <td>...</td>\n      <td>0.4130</td>\n      <td>0.5870</td>\n      <td>0.4348</td>\n      <td>0.5652</td>\n      <td>0.3478</td>\n      <td>-0.0434</td>\n      <td>0.3044</td>\n      <td>-0.0434</td>\n      <td>-0.5000</td>\n      <td>26.0</td>\n    </tr>\n    <tr>\n      <th>1558</th>\n      <td>-0.6624</td>\n      <td>-0.3334</td>\n      <td>0.3666</td>\n      <td>0.4292</td>\n      <td>-0.2084</td>\n      <td>-0.5374</td>\n      <td>-0.4542</td>\n      <td>-0.6208</td>\n      <td>-0.6376</td>\n      <td>-0.5042</td>\n      <td>...</td>\n      <td>0.2520</td>\n      <td>0.2846</td>\n      <td>0.4146</td>\n      <td>0.3170</td>\n      <td>0.2520</td>\n      <td>-0.0244</td>\n      <td>-0.0894</td>\n      <td>-0.1708</td>\n      <td>-0.3170</td>\n      <td>26.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>1559 rows × 618 columns</p>\n</div>",
            "text/plain": "         0       1       2       3       4       5       6       7       8    \\\n0    -0.2080  0.3480  0.3280  0.5040  0.9320  1.0000  0.8360  0.6680  0.2720   \n1    -0.2864  0.1992  0.2822  0.4398  0.7012  0.7800  1.0000  0.9792  0.5850   \n2    -0.2348  0.3826  0.6142  0.7492  0.0546 -0.4020 -0.3504 -0.2990 -0.6848   \n3    -0.1856  0.3592  0.7126  0.7366  0.3414  0.1018 -0.1556 -0.2514 -0.2514   \n4    -0.1814  0.4404  0.8394  1.0000  0.7564  0.1866  0.0260 -0.0726 -0.2124   \n5    -0.3212  0.6242  0.6424  0.6666  0.5090  0.1454  0.0060 -0.1454 -0.2606   \n6    -0.1312  0.5178  0.5752  0.6754  0.2984 -0.1694 -0.1980 -0.2028 -0.4510   \n7     0.0090  0.7374  0.6298  0.5820  0.1880 -0.0806 -0.2836 -0.5284 -0.5164   \n8    -0.4392  0.5782  0.6426  0.4094 -0.1762 -0.3896 -0.3846 -0.3548 -0.7022   \n9    -0.3154  0.1868  0.2614  0.3776  0.0912 -0.1660 -0.4854 -0.4772 -0.4440   \n10   -0.2344  0.3124  0.3204  0.2656  0.4296  0.6328  1.0000  0.8750  0.6484   \n11   -0.4286  0.1160  0.2946  0.4464  0.6250  0.7054  1.0000  1.0000  0.8840   \n12   -0.2070  0.2140  0.7052  0.7192 -0.0386 -0.2982 -0.4808 -0.5578 -0.5088   \n13   -0.3064  0.0582  0.6050  0.6506  0.0126 -0.2406 -0.3924 -0.3772 -0.4228   \n14   -0.1626  0.2808  0.8522  1.0000  0.5566  0.3400  0.1134 -0.1822 -0.3104   \n15   -0.3312  0.0254  0.5160  0.5606  0.1974  0.1082  0.0636 -0.1146 -0.1464   \n16   -0.4786 -0.2022  0.1254  0.1282  0.1994  0.4700  0.6924  1.0000  0.8576   \n17   -0.4178 -0.0182  0.1196  0.1814  0.2630  0.5134  0.7552  0.9888  1.0000   \n18   -0.2008  0.3400  0.7336  0.8608  0.6820  0.5030  0.2326 -0.0218 -0.1690   \n19   -0.0996  0.5974  0.6926  1.0000  0.9308  0.7056  0.3810  0.1992 -0.1428   \n20   -0.2060  0.4034  0.3304  0.5236  0.7982  0.9356  1.0000  0.9700  0.5622   \n21   -0.2948  0.4722  0.4762  0.5258  0.5958  1.0000  0.9010  0.8598  0.6948   \n22   -0.4322 -0.0928  0.2286  0.2500  0.4286  0.7822  1.0000  0.9642  0.6000   \n23   -0.3334  0.1846  0.2770  0.2666  0.5230  0.8154  0.9692  0.8666  0.6820   \n24   -0.3764  0.0168  0.2758  0.2662  0.6642  0.8514  0.8322  0.7506  0.6020   \n25   -0.4488 -0.1472  0.1844  0.2216  0.4674  0.6536  0.6908  0.6350  0.4040   \n26   -0.3474  0.1336  0.2214  0.2634  0.5764  0.6564  0.7328  0.5114  0.3702   \n27   -0.1826  0.4748  0.4156  0.5022  0.9132  1.0000  0.8310  0.8812  0.5480   \n28   -0.4516 -0.0888  0.1654  0.2178  0.4880  0.8266  0.8952  0.6370  0.5444   \n29   -0.2564  0.2886  0.4596  0.5566  0.8984  0.9030  1.0000  0.9308  0.6166   \n...      ...     ...     ...     ...     ...     ...     ...     ...     ...   \n1529 -0.6990 -0.3870 -0.2114  0.2940  0.6200  0.5018  0.1936 -0.1900 -0.0610   \n1530 -0.7204 -0.4226 -0.3142  0.3954  0.7354  0.6452  0.2210 -0.0286 -0.0766   \n1531 -0.6998 -0.3716 -0.2792 -0.1220  0.4586  0.6270  0.4026  0.0604 -0.0098   \n1532 -0.6426 -0.3728 -0.2142  0.2946  0.5054  0.4178  0.0178 -0.1408 -0.1550   \n1533 -0.7230 -0.2998 -0.2542 -0.1112  0.4186  0.5190  0.1354 -0.2208 -0.1872   \n1534 -0.6600 -0.3630 -0.3036 -0.0284  0.5060  0.5654  0.4332 -0.0122 -0.0770   \n1535 -0.7114 -0.3586 -0.1546  0.3760  0.6998  0.7872  0.3848  0.3032  0.6968   \n1536 -0.7338 -0.5768 -0.4266  0.2492  0.4574  0.4778  0.1980  0.1638  0.9864   \n1537  0.5888  0.4274 -0.3870 -0.8226 -0.8870 -0.8630 -0.7580 -0.7822 -0.9516   \n1538 -0.7778 -0.2790  0.0074  0.0518 -0.3284 -0.6098 -0.7976 -0.6790 -0.6790   \n1539 -0.5012  0.0272  0.3630  0.3876 -0.3976 -0.5210 -0.6198 -0.6740 -0.5704   \n1540 -0.6100 -0.2908  0.0532  0.0602 -0.5710 -0.8936 -0.8510 -0.7978 -0.8724   \n1541 -0.7250 -0.5844 -0.3992 -0.1988  0.6292  1.0000  0.9642  0.8984  0.9162   \n1542 -0.6846 -0.5768 -0.4132  0.0340  0.7486  0.9560  0.7524  0.7844  1.0000   \n1543 -0.6350 -0.3460 -0.3004  0.2776  0.7566  0.6122  0.3840  0.0646 -0.2168   \n1544 -0.7304 -0.5392 -0.4174  0.0086  0.6174  0.7304  0.5130  0.0956 -0.1044   \n1545 -0.6022  0.0000  0.3940  0.3372 -0.2840 -0.4734 -0.5910 -0.5758 -0.5228   \n1546 -0.6666 -0.1706  0.1746  0.1866 -0.1468 -0.4802 -0.5158 -0.5754 -0.5992   \n1547 -0.5570 -0.2982 -0.2130 -0.2948 -0.6798 -0.9284 -0.9966 -0.9728 -0.9318   \n1548 -0.4906 -0.2496 -0.2346 -0.4204 -0.6888 -0.9624 -0.8920 -0.7666 -0.8946   \n1549 -0.6908 -0.3864  0.0822  0.1112 -0.4252 -0.7972 -0.7150 -0.7584 -0.7682   \n1550 -0.6266 -0.2286  0.3472  0.3996 -0.4486 -0.7906 -0.8500 -0.7976 -0.7382   \n1551 -0.6060 -0.2828  0.1010  0.4848  0.5960  0.4748  0.1414  0.0708  0.2424   \n1552 -0.4610 -0.2056  0.4894  0.7730  0.9008  0.8156  0.6312  0.2766 -0.0922   \n1553 -0.6232 -0.2682 -0.1376  0.3478  0.8478  0.8260  0.3334  0.2028  0.1450   \n1554 -0.6842 -0.3280 -0.1984  0.2956  0.8786  0.8948  0.3118  0.1822  0.1012   \n1555 -0.5912 -0.2420  0.8174  1.0000  0.4642  0.6428  0.6944  0.3056 -0.3888   \n1556 -0.6696 -0.3730  0.1584  0.8910  1.0000  0.9762  0.9762  0.7684  0.4106   \n1557 -0.5764 -0.1764  0.5106  0.3742 -0.1670 -0.5858 -0.7882 -0.7224 -0.6330   \n1558 -0.6624 -0.3334  0.3666  0.4292 -0.2084 -0.5374 -0.4542 -0.6208 -0.6376   \n\n         9    ...      608     609     610     611     612     613     614  \\\n0     0.2400  ...   0.2500 -0.0624  0.2188  0.4532  0.1094  0.1718  0.1562   \n1     0.4066  ...  -0.0078 -0.1472 -0.1782  0.0078  0.1162 -0.0542 -0.0542   \n2    -0.6528  ...   0.2834  0.1500  0.0834 -0.2000 -0.1834  0.0500 -0.0166   \n3    -0.3892  ...   0.2840  0.5556  0.4568  0.4568  0.4568  0.2098  0.0370   \n4    -0.3730  ...   0.1688 -0.1688  0.2728  0.2988  0.2468  0.1948 -0.0130   \n5    -0.1940  ...   0.4528  0.3584  0.4906  0.2830  0.3584  0.6792  0.3018   \n6    -0.5418  ...   0.1314  0.0708  0.0708  0.2930  0.0910  0.1516 -0.0506   \n7    -0.7612  ...  -0.1428  0.0990  0.1868  0.4506  0.4726  0.2528 -0.0330   \n8    -0.7072  ...   0.6000 -0.1810  0.1428  0.2952  0.2380  0.2000  0.2000   \n9    -0.5186  ...   0.3138  0.2550  0.2942  0.5294  0.0196  0.1176  0.4510   \n10    0.2968  ...   0.6590  0.6124  0.9534  1.0000  0.8140  0.6590  0.7520   \n11    0.7410  ...   0.6000  0.6952  0.7142  1.0000  0.6190  0.4666  0.4858   \n12   -0.4666  ...   0.5284  0.5284  0.3584  0.5284  0.6226  0.5472  0.3396   \n13   -0.6860  ...   0.2400  0.0800  0.2400  0.4000  0.5600  0.5800  0.2200   \n14   -0.4286  ...   0.6288  0.6702  0.5980  0.6494  0.5154  0.5980  0.3506   \n15   -0.4586  ...   0.7142  0.7402  0.3636  0.2338  0.2858  0.2468  0.3246   \n16    0.7122  ...   0.3442  0.0656  0.0328 -0.0164 -0.1148  0.0492  0.0820   \n17    0.6624  ...   0.3334  0.1404 -0.3158  0.0000 -0.1052 -0.3334 -0.2632   \n18   -0.2088  ...   0.2500  0.1250 -0.2292  0.2500  0.0416 -0.1458  0.2292   \n19   -0.0822  ...   0.3388 -0.0806  0.1452  0.2742  0.3064  0.0968 -0.1612   \n20    0.4806  ...   0.4906  0.0944  0.3208  0.3396  0.4150  0.6792  0.3962   \n21    0.3196  ...   0.4364  0.1454  0.1272  0.3272  0.4728  0.0910  0.0182   \n22    0.4358  ...   0.7088  0.5922  0.6700  0.5728  0.8058  0.6116  0.6504   \n23    0.5744  ...   0.9174  0.9670  0.7024  1.0000  0.9338  0.6198  0.1404   \n24    0.3478  ...   0.3832  0.1028  0.1588  0.1776  0.0654  0.2150 -0.0468   \n25    0.3594  ...   0.2564  0.0256  0.2820  0.3334  0.1026  0.2052  0.0256   \n26    0.3244  ...   0.6924  0.3654  0.3462  0.5962  0.6924  0.4230  0.4038   \n27    0.4338  ...   0.1504  0.2744  0.3098  0.2744  0.5752  0.7168  0.5576   \n28    0.3186  ...   0.7608  0.3914  0.2174  0.6086  0.9348  0.7826  0.8696   \n29    0.4734  ...   0.1590  0.5682  0.3864  0.2272  0.3182  0.4090  0.2500   \n...      ...  ...      ...     ...     ...     ...     ...     ...     ...   \n1529 -0.0430  ...   0.0666 -0.0814 -0.1112  0.0518 -0.0074  0.0962  0.0222   \n1530 -0.1338  ...   0.6950  0.3220  0.2204  0.7288  0.8306  0.4238  0.6272   \n1531  0.0546  ...   0.5000  0.5000  0.2826  0.0000  0.1956  0.1086  0.2826   \n1532 -0.0366  ...   0.6632  0.3264  0.3474  0.5158  0.1578  0.4106  0.2210   \n1533 -0.1628  ...   0.1052 -0.1404  0.1754  0.2632  0.1754  0.0350 -0.0350   \n1534 -0.0742  ...   0.3904  0.5428  0.3904  0.3334  0.3334  0.3714  0.1048   \n1535  1.0000  ...   0.2958  0.3802  0.3522  0.0986  0.5212  0.1268  0.0422   \n1536  1.0000  ...   0.4762  0.3334  0.4762  0.7142  0.3810  0.2620  0.0714   \n1537 -0.8870  ...   0.1098  0.0550  0.0110 -0.0330  0.0330  0.0110  0.0220   \n1538 -0.6790  ...   0.4640  0.2784  0.4640  0.7938  0.5876  0.3402  0.0722   \n1539 -0.5210  ...   0.7288  0.4576  0.0170  0.2542  0.5932  0.6272  0.5932   \n1540 -0.8298  ...   0.4888  0.6222  0.0888  0.7112  0.3334  0.6666  0.6666   \n1541  0.9910  ...   0.2874  0.5632  0.4482  0.6782  0.3334  0.1954  0.3104   \n1542  0.9880  ...   0.4728  0.2000  0.2364  0.2910 -0.0182  0.1090  0.1090   \n1543 -0.0494  ...   1.0000  0.9488  0.8974  0.7436  0.6512  0.5794  0.6718   \n1544 -0.0260  ...   0.7112  0.6334  0.6112  0.7778  0.8112  0.4000  0.5222   \n1545 -0.5000  ...   0.4246  0.3698  0.4794  0.4246  0.4246  0.0136  0.2328   \n1546 -0.5476  ...   0.0878  0.7018  0.6140  0.1052 -0.1228 -0.2106 -0.1930   \n1547 -0.8604  ...   0.4318  0.3410  0.4772  0.5454  0.5000  0.3410  0.6818   \n1548 -0.8068  ...   0.5366  0.5122  0.2682  0.8536  0.8048  0.5610  0.4634   \n1549 -0.6522  ...   0.1320  0.2264 -0.0188  0.2264  0.2264 -0.0566  0.0188   \n1550 -0.7940  ...   0.0000  0.0678  0.1016  0.2542  0.1186 -0.1186  0.1356   \n1551  0.3232  ...  -0.0278 -0.2084 -0.0834  0.2638  0.1388  0.0694 -0.0694   \n1552 -0.3334  ...  -0.4474 -0.2764 -0.3026 -0.2106 -0.1316 -0.2500 -0.2764   \n1553  0.0652  ...   0.8266  0.5954  0.6532  0.6648  0.6648  0.6070  0.6416   \n1554  0.1740  ...   0.7738  0.7738  0.7142  0.6428  0.5952  0.5714  0.3928   \n1555 -0.6826  ...   0.1924 -0.1154  0.0192  0.2116 -0.0384  0.0192 -0.2308   \n1556  0.0154  ...   0.0910  0.1818  0.2000  0.1454  0.0182 -0.2910  0.0728   \n1557 -0.8212  ...   0.4130  0.5870  0.4348  0.5652  0.3478 -0.0434  0.3044   \n1558 -0.5042  ...   0.2520  0.2846  0.4146  0.3170  0.2520 -0.0244 -0.0894   \n\n         615     616   617  \n0     0.0468 -0.3750   1.0  \n1    -0.0388 -0.7984   1.0  \n2    -0.1834 -0.8666   2.0  \n3    -0.0618 -0.3334   2.0  \n4    -0.2988 -0.7662   3.0  \n5     0.1698 -0.2642   3.0  \n6    -0.0910 -0.7778   4.0  \n7    -0.1868 -0.8022   4.0  \n8     0.0096 -0.7714   5.0  \n9     0.1764 -0.9608   5.0  \n10    0.5814 -0.0078   6.0  \n11    0.1810 -0.0096   6.0  \n12    0.3208 -0.1132   7.0  \n13   -0.1600 -0.4200   7.0  \n14    0.2578 -0.1238   8.0  \n15   -0.0390 -0.4806   8.0  \n16    0.0820 -0.5738   9.0  \n17   -0.3334 -0.5088   9.0  \n18   -0.1666 -0.7500  10.0  \n19   -0.2258 -0.4032  10.0  \n20    0.0378 -0.3208  11.0  \n21   -0.2728 -0.3818  11.0  \n22    0.1456  0.0486  12.0  \n23    0.0578 -0.1736  12.0  \n24   -0.1402 -0.6636  13.0  \n25   -0.1794 -0.7436  13.0  \n26    0.0576 -0.4808  14.0  \n27    0.3982  0.0974  14.0  \n28    0.4566 -0.0218  15.0  \n29    0.1818 -0.5228  15.0  \n...      ...     ...   ...  \n1529 -0.4370 -0.6888  12.0  \n1530  0.4916 -0.1186  12.0  \n1531 -0.0434 -0.4782  13.0  \n1532  0.3052 -0.2632  13.0  \n1533  0.0176 -0.7018  14.0  \n1534  0.4666  0.0096  14.0  \n1535  0.1830 -0.1830  15.0  \n1536  0.1190 -0.5000  15.0  \n1537  0.1208 -0.2418  16.0  \n1538  0.0310 -0.2990  16.0  \n1539 -0.0848 -0.6950  17.0  \n1540  0.6444  0.2666  17.0  \n1541  0.0114 -0.2644  18.0  \n1542  0.0910 -0.7636  18.0  \n1543  0.7538  0.4974  19.0  \n1544  0.6556  0.3666  19.0  \n1545 -0.4520 -0.6164  20.0  \n1546  0.0526 -0.1052  20.0  \n1547  0.1818  0.1136  21.0  \n1548  0.3658  0.0000  21.0  \n1549  0.0566 -0.4716  22.0  \n1550 -0.1526 -0.2712  22.0  \n1551 -0.3888 -0.9028  23.0  \n1552 -0.4736 -0.9210  23.0  \n1553  0.5492  0.4104  24.0  \n1554  0.4286  0.2858  24.0  \n1555 -0.4230 -0.7116  25.0  \n1556  0.0728 -0.5818  25.0  \n1557 -0.0434 -0.5000  26.0  \n1558 -0.1708 -0.3170  26.0  \n\n[1559 rows x 618 columns]"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Ακολούθως, πάμε να κάνουμε το concatenation των 2 αρχείων σε 1 όπως ζητείται. Ταυτόχρονα, για να γίνει ομαλά αυτό και να υπάρχει μία συνέχεια στην αρίθμηση των γραμμών επιλέγουμε να κάνουμε ignore το Index τους δεδομένου ότι δεν περιέχει και κάποια πληροφορία. Αλλάζουμε και το όνομα των κλάσεων, που είναι η τελευταία στήλη σε Letter καθώς αντιπροσωπεύει γράμμα, και έχει 26 ετικέτες, όπως περιγράφηκε παραπάνω."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "isolet1=pd.read_csv(\"isolet1.data\", header=None,na_values = [\"?\"])\nisolet5=pd.read_csv(\"isolet5.data\", header=None,na_values = [\"?\"])\nmerged_isolet = pd.concat(([isolet1,isolet5]),axis=0,ignore_index=True)\ndf=merged_isolet\ndf = df.rename(columns={617: 'Letter'})\ndf",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 5,
          "data": {
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>608</th>\n      <th>609</th>\n      <th>610</th>\n      <th>611</th>\n      <th>612</th>\n      <th>613</th>\n      <th>614</th>\n      <th>615</th>\n      <th>616</th>\n      <th>Letter</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-0.4394</td>\n      <td>-0.0930</td>\n      <td>0.1718</td>\n      <td>0.4620</td>\n      <td>0.6226</td>\n      <td>0.4704</td>\n      <td>0.3578</td>\n      <td>0.0478</td>\n      <td>-0.1184</td>\n      <td>-0.2310</td>\n      <td>...</td>\n      <td>0.4102</td>\n      <td>0.2052</td>\n      <td>0.3846</td>\n      <td>0.3590</td>\n      <td>0.5898</td>\n      <td>0.3334</td>\n      <td>0.6410</td>\n      <td>0.5898</td>\n      <td>-0.4872</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-0.4348</td>\n      <td>-0.1198</td>\n      <td>0.2474</td>\n      <td>0.4036</td>\n      <td>0.5026</td>\n      <td>0.6328</td>\n      <td>0.4948</td>\n      <td>0.0338</td>\n      <td>-0.0520</td>\n      <td>-0.1302</td>\n      <td>...</td>\n      <td>0.0000</td>\n      <td>0.2954</td>\n      <td>0.2046</td>\n      <td>0.4772</td>\n      <td>0.0454</td>\n      <td>0.2046</td>\n      <td>0.4318</td>\n      <td>0.4546</td>\n      <td>-0.0910</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-0.2330</td>\n      <td>0.2124</td>\n      <td>0.5014</td>\n      <td>0.5222</td>\n      <td>-0.3422</td>\n      <td>-0.5840</td>\n      <td>-0.7168</td>\n      <td>-0.6342</td>\n      <td>-0.8614</td>\n      <td>-0.8318</td>\n      <td>...</td>\n      <td>-0.1112</td>\n      <td>-0.0476</td>\n      <td>-0.1746</td>\n      <td>0.0318</td>\n      <td>-0.0476</td>\n      <td>0.1112</td>\n      <td>0.2540</td>\n      <td>0.1588</td>\n      <td>-0.4762</td>\n      <td>2.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-0.3808</td>\n      <td>-0.0096</td>\n      <td>0.2602</td>\n      <td>0.2554</td>\n      <td>-0.4290</td>\n      <td>-0.6746</td>\n      <td>-0.6868</td>\n      <td>-0.6650</td>\n      <td>-0.8410</td>\n      <td>-0.9614</td>\n      <td>...</td>\n      <td>-0.0504</td>\n      <td>-0.0360</td>\n      <td>-0.1224</td>\n      <td>0.1366</td>\n      <td>0.2950</td>\n      <td>0.0792</td>\n      <td>-0.0072</td>\n      <td>0.0936</td>\n      <td>-0.1510</td>\n      <td>2.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-0.3412</td>\n      <td>0.0946</td>\n      <td>0.6082</td>\n      <td>0.6216</td>\n      <td>-0.1622</td>\n      <td>-0.3784</td>\n      <td>-0.4324</td>\n      <td>-0.4358</td>\n      <td>-0.4966</td>\n      <td>-0.5406</td>\n      <td>...</td>\n      <td>0.1562</td>\n      <td>0.3124</td>\n      <td>0.2500</td>\n      <td>-0.0938</td>\n      <td>0.1562</td>\n      <td>0.3124</td>\n      <td>0.3124</td>\n      <td>0.2188</td>\n      <td>-0.2500</td>\n      <td>3.0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>-0.4634</td>\n      <td>0.0306</td>\n      <td>0.3546</td>\n      <td>0.4448</td>\n      <td>-0.1022</td>\n      <td>-0.4184</td>\n      <td>-0.6388</td>\n      <td>-0.4370</td>\n      <td>-0.4396</td>\n      <td>-0.6654</td>\n      <td>...</td>\n      <td>0.6626</td>\n      <td>0.7350</td>\n      <td>0.3734</td>\n      <td>0.6626</td>\n      <td>0.3012</td>\n      <td>0.1808</td>\n      <td>0.2290</td>\n      <td>0.6144</td>\n      <td>0.3254</td>\n      <td>3.0</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>-0.3364</td>\n      <td>-0.0102</td>\n      <td>0.2132</td>\n      <td>0.2018</td>\n      <td>-0.6146</td>\n      <td>-0.8380</td>\n      <td>-0.8130</td>\n      <td>-0.7240</td>\n      <td>-0.8062</td>\n      <td>-0.8996</td>\n      <td>...</td>\n      <td>0.0526</td>\n      <td>-0.0702</td>\n      <td>-0.0350</td>\n      <td>0.0702</td>\n      <td>0.1578</td>\n      <td>0.1930</td>\n      <td>0.4562</td>\n      <td>0.4562</td>\n      <td>-0.3860</td>\n      <td>4.0</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>-0.4798</td>\n      <td>-0.1580</td>\n      <td>0.1764</td>\n      <td>0.1820</td>\n      <td>-0.6378</td>\n      <td>-0.8400</td>\n      <td>-0.7280</td>\n      <td>-0.6654</td>\n      <td>-0.7978</td>\n      <td>-0.7904</td>\n      <td>...</td>\n      <td>0.2912</td>\n      <td>-0.1646</td>\n      <td>0.1140</td>\n      <td>0.0126</td>\n      <td>-0.0380</td>\n      <td>0.0886</td>\n      <td>0.2912</td>\n      <td>0.3670</td>\n      <td>0.1646</td>\n      <td>4.0</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>-0.3928</td>\n      <td>0.0424</td>\n      <td>0.2166</td>\n      <td>0.2124</td>\n      <td>-0.4564</td>\n      <td>-0.6200</td>\n      <td>-0.7112</td>\n      <td>-0.6602</td>\n      <td>-0.6942</td>\n      <td>-0.7920</td>\n      <td>...</td>\n      <td>0.8868</td>\n      <td>0.8868</td>\n      <td>0.6792</td>\n      <td>0.6038</td>\n      <td>0.2264</td>\n      <td>0.7924</td>\n      <td>1.0000</td>\n      <td>0.9246</td>\n      <td>0.5284</td>\n      <td>5.0</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>-0.5494</td>\n      <td>-0.0940</td>\n      <td>0.2868</td>\n      <td>0.2964</td>\n      <td>-0.5326</td>\n      <td>-0.7204</td>\n      <td>-0.7518</td>\n      <td>-0.7398</td>\n      <td>-0.8482</td>\n      <td>-0.8386</td>\n      <td>...</td>\n      <td>0.6130</td>\n      <td>0.6130</td>\n      <td>0.6130</td>\n      <td>0.3226</td>\n      <td>0.6130</td>\n      <td>0.2904</td>\n      <td>0.5484</td>\n      <td>0.5162</td>\n      <td>0.3548</td>\n      <td>5.0</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>-0.3646</td>\n      <td>0.0904</td>\n      <td>0.2374</td>\n      <td>0.3044</td>\n      <td>0.6188</td>\n      <td>0.7592</td>\n      <td>0.9264</td>\n      <td>0.6522</td>\n      <td>0.6054</td>\n      <td>0.2442</td>\n      <td>...</td>\n      <td>0.9178</td>\n      <td>1.0000</td>\n      <td>0.8082</td>\n      <td>0.8220</td>\n      <td>0.7672</td>\n      <td>0.7534</td>\n      <td>0.6986</td>\n      <td>0.4932</td>\n      <td>-0.0548</td>\n      <td>6.0</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>-0.6926</td>\n      <td>-0.3318</td>\n      <td>-0.1414</td>\n      <td>-0.1512</td>\n      <td>0.0098</td>\n      <td>0.2926</td>\n      <td>0.2440</td>\n      <td>0.3074</td>\n      <td>-0.0098</td>\n      <td>-0.0536</td>\n      <td>...</td>\n      <td>0.7460</td>\n      <td>0.8888</td>\n      <td>0.7460</td>\n      <td>0.6984</td>\n      <td>0.8254</td>\n      <td>1.0000</td>\n      <td>0.9842</td>\n      <td>0.5238</td>\n      <td>0.1746</td>\n      <td>6.0</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>-0.2532</td>\n      <td>0.1670</td>\n      <td>0.3470</td>\n      <td>0.3734</td>\n      <td>-0.2570</td>\n      <td>-0.7260</td>\n      <td>-0.8086</td>\n      <td>-0.6360</td>\n      <td>-0.7748</td>\n      <td>-0.8536</td>\n      <td>...</td>\n      <td>0.5302</td>\n      <td>0.2886</td>\n      <td>0.2618</td>\n      <td>0.1276</td>\n      <td>-0.0738</td>\n      <td>0.2618</td>\n      <td>0.3288</td>\n      <td>0.1276</td>\n      <td>-0.1812</td>\n      <td>7.0</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>-0.4030</td>\n      <td>-0.1284</td>\n      <td>0.2986</td>\n      <td>0.2358</td>\n      <td>-0.9134</td>\n      <td>-0.7552</td>\n      <td>-0.7642</td>\n      <td>-0.7134</td>\n      <td>-0.7014</td>\n      <td>-0.7940</td>\n      <td>...</td>\n      <td>0.4666</td>\n      <td>0.5200</td>\n      <td>0.6266</td>\n      <td>0.3866</td>\n      <td>0.4666</td>\n      <td>0.4134</td>\n      <td>0.5466</td>\n      <td>0.2000</td>\n      <td>-0.3600</td>\n      <td>7.0</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>-0.3216</td>\n      <td>0.1860</td>\n      <td>0.1248</td>\n      <td>0.3654</td>\n      <td>0.4880</td>\n      <td>0.4618</td>\n      <td>-0.1816</td>\n      <td>-0.0854</td>\n      <td>-0.2516</td>\n      <td>-0.4530</td>\n      <td>...</td>\n      <td>0.8222</td>\n      <td>0.9778</td>\n      <td>0.8444</td>\n      <td>0.7666</td>\n      <td>0.7888</td>\n      <td>0.8000</td>\n      <td>0.6556</td>\n      <td>0.5778</td>\n      <td>0.3000</td>\n      <td>8.0</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>-0.5368</td>\n      <td>-0.2516</td>\n      <td>0.1448</td>\n      <td>0.1848</td>\n      <td>0.3184</td>\n      <td>0.1982</td>\n      <td>0.1046</td>\n      <td>-0.2472</td>\n      <td>-0.3898</td>\n      <td>-0.3364</td>\n      <td>...</td>\n      <td>0.7806</td>\n      <td>0.8580</td>\n      <td>0.8968</td>\n      <td>0.8322</td>\n      <td>0.8322</td>\n      <td>1.0000</td>\n      <td>0.7936</td>\n      <td>0.6646</td>\n      <td>-0.2000</td>\n      <td>8.0</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>-0.4550</td>\n      <td>-0.1044</td>\n      <td>-0.0924</td>\n      <td>-0.0300</td>\n      <td>0.1068</td>\n      <td>0.3782</td>\n      <td>0.8248</td>\n      <td>0.9856</td>\n      <td>1.0000</td>\n      <td>0.9640</td>\n      <td>...</td>\n      <td>0.4588</td>\n      <td>0.4824</td>\n      <td>0.3412</td>\n      <td>0.2236</td>\n      <td>0.4118</td>\n      <td>0.5294</td>\n      <td>0.6942</td>\n      <td>0.6000</td>\n      <td>0.3648</td>\n      <td>9.0</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>-0.4804</td>\n      <td>-0.1238</td>\n      <td>0.0792</td>\n      <td>0.1114</td>\n      <td>0.0700</td>\n      <td>0.4096</td>\n      <td>0.4450</td>\n      <td>0.7326</td>\n      <td>0.8448</td>\n      <td>0.9616</td>\n      <td>...</td>\n      <td>0.2766</td>\n      <td>0.2554</td>\n      <td>0.1914</td>\n      <td>0.2978</td>\n      <td>0.2978</td>\n      <td>-0.0638</td>\n      <td>0.0212</td>\n      <td>0.1276</td>\n      <td>-0.2340</td>\n      <td>9.0</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>-0.3368</td>\n      <td>0.0026</td>\n      <td>0.5802</td>\n      <td>0.4706</td>\n      <td>0.2514</td>\n      <td>-0.0910</td>\n      <td>-0.3610</td>\n      <td>-0.3664</td>\n      <td>-0.2326</td>\n      <td>-0.4866</td>\n      <td>...</td>\n      <td>-0.1162</td>\n      <td>0.0388</td>\n      <td>0.0698</td>\n      <td>-0.1472</td>\n      <td>0.0388</td>\n      <td>0.2558</td>\n      <td>0.1162</td>\n      <td>0.1008</td>\n      <td>-0.3644</td>\n      <td>10.0</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>-0.5880</td>\n      <td>-0.2028</td>\n      <td>0.1304</td>\n      <td>0.2278</td>\n      <td>0.2132</td>\n      <td>-0.1822</td>\n      <td>-0.2960</td>\n      <td>-0.3084</td>\n      <td>-0.7722</td>\n      <td>-0.4224</td>\n      <td>...</td>\n      <td>0.7024</td>\n      <td>0.5702</td>\n      <td>0.4710</td>\n      <td>0.6694</td>\n      <td>0.8348</td>\n      <td>0.8182</td>\n      <td>0.6860</td>\n      <td>0.4380</td>\n      <td>-0.1240</td>\n      <td>10.0</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>-0.2560</td>\n      <td>0.3910</td>\n      <td>0.2630</td>\n      <td>0.4948</td>\n      <td>0.7854</td>\n      <td>0.8132</td>\n      <td>0.3460</td>\n      <td>0.2180</td>\n      <td>0.0830</td>\n      <td>-0.1418</td>\n      <td>...</td>\n      <td>0.0378</td>\n      <td>0.0944</td>\n      <td>0.0188</td>\n      <td>-0.2642</td>\n      <td>-0.3774</td>\n      <td>0.0754</td>\n      <td>0.5284</td>\n      <td>0.5094</td>\n      <td>-0.3018</td>\n      <td>11.0</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>-0.3896</td>\n      <td>0.1042</td>\n      <td>0.3190</td>\n      <td>0.4478</td>\n      <td>0.8252</td>\n      <td>0.7576</td>\n      <td>0.5982</td>\n      <td>0.3190</td>\n      <td>0.2300</td>\n      <td>0.0184</td>\n      <td>...</td>\n      <td>0.7846</td>\n      <td>0.4770</td>\n      <td>0.5076</td>\n      <td>0.6924</td>\n      <td>0.3846</td>\n      <td>0.3538</td>\n      <td>0.5692</td>\n      <td>0.6308</td>\n      <td>0.4770</td>\n      <td>11.0</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>-0.3384</td>\n      <td>0.1034</td>\n      <td>0.2260</td>\n      <td>0.3206</td>\n      <td>0.7114</td>\n      <td>0.6372</td>\n      <td>0.6808</td>\n      <td>0.3026</td>\n      <td>0.1468</td>\n      <td>-0.0932</td>\n      <td>...</td>\n      <td>0.8410</td>\n      <td>0.3864</td>\n      <td>-0.1136</td>\n      <td>0.3636</td>\n      <td>0.3182</td>\n      <td>0.2272</td>\n      <td>0.4546</td>\n      <td>-0.2500</td>\n      <td>-0.5228</td>\n      <td>12.0</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>-0.4810</td>\n      <td>-0.1146</td>\n      <td>0.2654</td>\n      <td>0.2890</td>\n      <td>0.5102</td>\n      <td>0.5514</td>\n      <td>0.6180</td>\n      <td>0.2360</td>\n      <td>0.0696</td>\n      <td>-0.0324</td>\n      <td>...</td>\n      <td>0.1384</td>\n      <td>0.3538</td>\n      <td>0.0462</td>\n      <td>0.4770</td>\n      <td>0.3538</td>\n      <td>-0.1692</td>\n      <td>0.2616</td>\n      <td>0.0770</td>\n      <td>-0.3230</td>\n      <td>12.0</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>-0.2116</td>\n      <td>0.2024</td>\n      <td>0.4222</td>\n      <td>0.4596</td>\n      <td>0.7216</td>\n      <td>0.5954</td>\n      <td>0.4340</td>\n      <td>0.0690</td>\n      <td>0.0036</td>\n      <td>-0.2234</td>\n      <td>...</td>\n      <td>1.0000</td>\n      <td>0.8832</td>\n      <td>0.9708</td>\n      <td>0.9124</td>\n      <td>0.7810</td>\n      <td>0.8686</td>\n      <td>0.7664</td>\n      <td>0.4598</td>\n      <td>0.2700</td>\n      <td>13.0</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>-0.5672</td>\n      <td>-0.2202</td>\n      <td>0.0220</td>\n      <td>0.0734</td>\n      <td>0.1058</td>\n      <td>0.1516</td>\n      <td>0.2240</td>\n      <td>-0.1058</td>\n      <td>-0.1898</td>\n      <td>-0.1020</td>\n      <td>...</td>\n      <td>0.3056</td>\n      <td>0.0278</td>\n      <td>0.3056</td>\n      <td>0.3334</td>\n      <td>0.1666</td>\n      <td>0.2500</td>\n      <td>0.3334</td>\n      <td>-0.3612</td>\n      <td>-0.7500</td>\n      <td>13.0</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>-0.5160</td>\n      <td>-0.2120</td>\n      <td>0.1686</td>\n      <td>0.1976</td>\n      <td>0.1562</td>\n      <td>-0.0176</td>\n      <td>-0.0362</td>\n      <td>-0.0672</td>\n      <td>-0.1540</td>\n      <td>-0.6360</td>\n      <td>...</td>\n      <td>0.2632</td>\n      <td>0.0526</td>\n      <td>0.1316</td>\n      <td>0.3158</td>\n      <td>0.3158</td>\n      <td>0.5000</td>\n      <td>0.3948</td>\n      <td>0.3684</td>\n      <td>0.1578</td>\n      <td>14.0</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>-0.5330</td>\n      <td>-0.0868</td>\n      <td>0.1838</td>\n      <td>0.2394</td>\n      <td>0.4428</td>\n      <td>0.2694</td>\n      <td>-0.0150</td>\n      <td>-0.2924</td>\n      <td>-0.4220</td>\n      <td>-0.5654</td>\n      <td>...</td>\n      <td>0.5164</td>\n      <td>0.2748</td>\n      <td>0.1648</td>\n      <td>0.3626</td>\n      <td>0.2968</td>\n      <td>0.4506</td>\n      <td>0.2748</td>\n      <td>-0.0550</td>\n      <td>-0.8462</td>\n      <td>14.0</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>-0.3656</td>\n      <td>0.0296</td>\n      <td>0.3160</td>\n      <td>0.5526</td>\n      <td>0.9030</td>\n      <td>0.9574</td>\n      <td>0.9834</td>\n      <td>0.8556</td>\n      <td>0.9314</td>\n      <td>0.9692</td>\n      <td>...</td>\n      <td>0.1904</td>\n      <td>0.2380</td>\n      <td>0.3810</td>\n      <td>0.4762</td>\n      <td>0.5952</td>\n      <td>0.5714</td>\n      <td>0.8096</td>\n      <td>0.3334</td>\n      <td>-0.4762</td>\n      <td>15.0</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>-0.5154</td>\n      <td>-0.1740</td>\n      <td>0.3140</td>\n      <td>0.5392</td>\n      <td>0.8566</td>\n      <td>0.9488</td>\n      <td>0.7884</td>\n      <td>0.7508</td>\n      <td>0.8464</td>\n      <td>1.0000</td>\n      <td>...</td>\n      <td>0.1154</td>\n      <td>0.0962</td>\n      <td>0.0000</td>\n      <td>-0.2308</td>\n      <td>-0.1730</td>\n      <td>0.2116</td>\n      <td>0.1924</td>\n      <td>0.4038</td>\n      <td>0.1346</td>\n      <td>15.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>7767</th>\n      <td>-0.6990</td>\n      <td>-0.3870</td>\n      <td>-0.2114</td>\n      <td>0.2940</td>\n      <td>0.6200</td>\n      <td>0.5018</td>\n      <td>0.1936</td>\n      <td>-0.1900</td>\n      <td>-0.0610</td>\n      <td>-0.0430</td>\n      <td>...</td>\n      <td>0.0666</td>\n      <td>-0.0814</td>\n      <td>-0.1112</td>\n      <td>0.0518</td>\n      <td>-0.0074</td>\n      <td>0.0962</td>\n      <td>0.0222</td>\n      <td>-0.4370</td>\n      <td>-0.6888</td>\n      <td>12.0</td>\n    </tr>\n    <tr>\n      <th>7768</th>\n      <td>-0.7204</td>\n      <td>-0.4226</td>\n      <td>-0.3142</td>\n      <td>0.3954</td>\n      <td>0.7354</td>\n      <td>0.6452</td>\n      <td>0.2210</td>\n      <td>-0.0286</td>\n      <td>-0.0766</td>\n      <td>-0.1338</td>\n      <td>...</td>\n      <td>0.6950</td>\n      <td>0.3220</td>\n      <td>0.2204</td>\n      <td>0.7288</td>\n      <td>0.8306</td>\n      <td>0.4238</td>\n      <td>0.6272</td>\n      <td>0.4916</td>\n      <td>-0.1186</td>\n      <td>12.0</td>\n    </tr>\n    <tr>\n      <th>7769</th>\n      <td>-0.6998</td>\n      <td>-0.3716</td>\n      <td>-0.2792</td>\n      <td>-0.1220</td>\n      <td>0.4586</td>\n      <td>0.6270</td>\n      <td>0.4026</td>\n      <td>0.0604</td>\n      <td>-0.0098</td>\n      <td>0.0546</td>\n      <td>...</td>\n      <td>0.5000</td>\n      <td>0.5000</td>\n      <td>0.2826</td>\n      <td>0.0000</td>\n      <td>0.1956</td>\n      <td>0.1086</td>\n      <td>0.2826</td>\n      <td>-0.0434</td>\n      <td>-0.4782</td>\n      <td>13.0</td>\n    </tr>\n    <tr>\n      <th>7770</th>\n      <td>-0.6426</td>\n      <td>-0.3728</td>\n      <td>-0.2142</td>\n      <td>0.2946</td>\n      <td>0.5054</td>\n      <td>0.4178</td>\n      <td>0.0178</td>\n      <td>-0.1408</td>\n      <td>-0.1550</td>\n      <td>-0.0366</td>\n      <td>...</td>\n      <td>0.6632</td>\n      <td>0.3264</td>\n      <td>0.3474</td>\n      <td>0.5158</td>\n      <td>0.1578</td>\n      <td>0.4106</td>\n      <td>0.2210</td>\n      <td>0.3052</td>\n      <td>-0.2632</td>\n      <td>13.0</td>\n    </tr>\n    <tr>\n      <th>7771</th>\n      <td>-0.7230</td>\n      <td>-0.2998</td>\n      <td>-0.2542</td>\n      <td>-0.1112</td>\n      <td>0.4186</td>\n      <td>0.5190</td>\n      <td>0.1354</td>\n      <td>-0.2208</td>\n      <td>-0.1872</td>\n      <td>-0.1628</td>\n      <td>...</td>\n      <td>0.1052</td>\n      <td>-0.1404</td>\n      <td>0.1754</td>\n      <td>0.2632</td>\n      <td>0.1754</td>\n      <td>0.0350</td>\n      <td>-0.0350</td>\n      <td>0.0176</td>\n      <td>-0.7018</td>\n      <td>14.0</td>\n    </tr>\n    <tr>\n      <th>7772</th>\n      <td>-0.6600</td>\n      <td>-0.3630</td>\n      <td>-0.3036</td>\n      <td>-0.0284</td>\n      <td>0.5060</td>\n      <td>0.5654</td>\n      <td>0.4332</td>\n      <td>-0.0122</td>\n      <td>-0.0770</td>\n      <td>-0.0742</td>\n      <td>...</td>\n      <td>0.3904</td>\n      <td>0.5428</td>\n      <td>0.3904</td>\n      <td>0.3334</td>\n      <td>0.3334</td>\n      <td>0.3714</td>\n      <td>0.1048</td>\n      <td>0.4666</td>\n      <td>0.0096</td>\n      <td>14.0</td>\n    </tr>\n    <tr>\n      <th>7773</th>\n      <td>-0.7114</td>\n      <td>-0.3586</td>\n      <td>-0.1546</td>\n      <td>0.3760</td>\n      <td>0.6998</td>\n      <td>0.7872</td>\n      <td>0.3848</td>\n      <td>0.3032</td>\n      <td>0.6968</td>\n      <td>1.0000</td>\n      <td>...</td>\n      <td>0.2958</td>\n      <td>0.3802</td>\n      <td>0.3522</td>\n      <td>0.0986</td>\n      <td>0.5212</td>\n      <td>0.1268</td>\n      <td>0.0422</td>\n      <td>0.1830</td>\n      <td>-0.1830</td>\n      <td>15.0</td>\n    </tr>\n    <tr>\n      <th>7774</th>\n      <td>-0.7338</td>\n      <td>-0.5768</td>\n      <td>-0.4266</td>\n      <td>0.2492</td>\n      <td>0.4574</td>\n      <td>0.4778</td>\n      <td>0.1980</td>\n      <td>0.1638</td>\n      <td>0.9864</td>\n      <td>1.0000</td>\n      <td>...</td>\n      <td>0.4762</td>\n      <td>0.3334</td>\n      <td>0.4762</td>\n      <td>0.7142</td>\n      <td>0.3810</td>\n      <td>0.2620</td>\n      <td>0.0714</td>\n      <td>0.1190</td>\n      <td>-0.5000</td>\n      <td>15.0</td>\n    </tr>\n    <tr>\n      <th>7775</th>\n      <td>0.5888</td>\n      <td>0.4274</td>\n      <td>-0.3870</td>\n      <td>-0.8226</td>\n      <td>-0.8870</td>\n      <td>-0.8630</td>\n      <td>-0.7580</td>\n      <td>-0.7822</td>\n      <td>-0.9516</td>\n      <td>-0.8870</td>\n      <td>...</td>\n      <td>0.1098</td>\n      <td>0.0550</td>\n      <td>0.0110</td>\n      <td>-0.0330</td>\n      <td>0.0330</td>\n      <td>0.0110</td>\n      <td>0.0220</td>\n      <td>0.1208</td>\n      <td>-0.2418</td>\n      <td>16.0</td>\n    </tr>\n    <tr>\n      <th>7776</th>\n      <td>-0.7778</td>\n      <td>-0.2790</td>\n      <td>0.0074</td>\n      <td>0.0518</td>\n      <td>-0.3284</td>\n      <td>-0.6098</td>\n      <td>-0.7976</td>\n      <td>-0.6790</td>\n      <td>-0.6790</td>\n      <td>-0.6790</td>\n      <td>...</td>\n      <td>0.4640</td>\n      <td>0.2784</td>\n      <td>0.4640</td>\n      <td>0.7938</td>\n      <td>0.5876</td>\n      <td>0.3402</td>\n      <td>0.0722</td>\n      <td>0.0310</td>\n      <td>-0.2990</td>\n      <td>16.0</td>\n    </tr>\n    <tr>\n      <th>7777</th>\n      <td>-0.5012</td>\n      <td>0.0272</td>\n      <td>0.3630</td>\n      <td>0.3876</td>\n      <td>-0.3976</td>\n      <td>-0.5210</td>\n      <td>-0.6198</td>\n      <td>-0.6740</td>\n      <td>-0.5704</td>\n      <td>-0.5210</td>\n      <td>...</td>\n      <td>0.7288</td>\n      <td>0.4576</td>\n      <td>0.0170</td>\n      <td>0.2542</td>\n      <td>0.5932</td>\n      <td>0.6272</td>\n      <td>0.5932</td>\n      <td>-0.0848</td>\n      <td>-0.6950</td>\n      <td>17.0</td>\n    </tr>\n    <tr>\n      <th>7778</th>\n      <td>-0.6100</td>\n      <td>-0.2908</td>\n      <td>0.0532</td>\n      <td>0.0602</td>\n      <td>-0.5710</td>\n      <td>-0.8936</td>\n      <td>-0.8510</td>\n      <td>-0.7978</td>\n      <td>-0.8724</td>\n      <td>-0.8298</td>\n      <td>...</td>\n      <td>0.4888</td>\n      <td>0.6222</td>\n      <td>0.0888</td>\n      <td>0.7112</td>\n      <td>0.3334</td>\n      <td>0.6666</td>\n      <td>0.6666</td>\n      <td>0.6444</td>\n      <td>0.2666</td>\n      <td>17.0</td>\n    </tr>\n    <tr>\n      <th>7779</th>\n      <td>-0.7250</td>\n      <td>-0.5844</td>\n      <td>-0.3992</td>\n      <td>-0.1988</td>\n      <td>0.6292</td>\n      <td>1.0000</td>\n      <td>0.9642</td>\n      <td>0.8984</td>\n      <td>0.9162</td>\n      <td>0.9910</td>\n      <td>...</td>\n      <td>0.2874</td>\n      <td>0.5632</td>\n      <td>0.4482</td>\n      <td>0.6782</td>\n      <td>0.3334</td>\n      <td>0.1954</td>\n      <td>0.3104</td>\n      <td>0.0114</td>\n      <td>-0.2644</td>\n      <td>18.0</td>\n    </tr>\n    <tr>\n      <th>7780</th>\n      <td>-0.6846</td>\n      <td>-0.5768</td>\n      <td>-0.4132</td>\n      <td>0.0340</td>\n      <td>0.7486</td>\n      <td>0.9560</td>\n      <td>0.7524</td>\n      <td>0.7844</td>\n      <td>1.0000</td>\n      <td>0.9880</td>\n      <td>...</td>\n      <td>0.4728</td>\n      <td>0.2000</td>\n      <td>0.2364</td>\n      <td>0.2910</td>\n      <td>-0.0182</td>\n      <td>0.1090</td>\n      <td>0.1090</td>\n      <td>0.0910</td>\n      <td>-0.7636</td>\n      <td>18.0</td>\n    </tr>\n    <tr>\n      <th>7781</th>\n      <td>-0.6350</td>\n      <td>-0.3460</td>\n      <td>-0.3004</td>\n      <td>0.2776</td>\n      <td>0.7566</td>\n      <td>0.6122</td>\n      <td>0.3840</td>\n      <td>0.0646</td>\n      <td>-0.2168</td>\n      <td>-0.0494</td>\n      <td>...</td>\n      <td>1.0000</td>\n      <td>0.9488</td>\n      <td>0.8974</td>\n      <td>0.7436</td>\n      <td>0.6512</td>\n      <td>0.5794</td>\n      <td>0.6718</td>\n      <td>0.7538</td>\n      <td>0.4974</td>\n      <td>19.0</td>\n    </tr>\n    <tr>\n      <th>7782</th>\n      <td>-0.7304</td>\n      <td>-0.5392</td>\n      <td>-0.4174</td>\n      <td>0.0086</td>\n      <td>0.6174</td>\n      <td>0.7304</td>\n      <td>0.5130</td>\n      <td>0.0956</td>\n      <td>-0.1044</td>\n      <td>-0.0260</td>\n      <td>...</td>\n      <td>0.7112</td>\n      <td>0.6334</td>\n      <td>0.6112</td>\n      <td>0.7778</td>\n      <td>0.8112</td>\n      <td>0.4000</td>\n      <td>0.5222</td>\n      <td>0.6556</td>\n      <td>0.3666</td>\n      <td>19.0</td>\n    </tr>\n    <tr>\n      <th>7783</th>\n      <td>-0.6022</td>\n      <td>0.0000</td>\n      <td>0.3940</td>\n      <td>0.3372</td>\n      <td>-0.2840</td>\n      <td>-0.4734</td>\n      <td>-0.5910</td>\n      <td>-0.5758</td>\n      <td>-0.5228</td>\n      <td>-0.5000</td>\n      <td>...</td>\n      <td>0.4246</td>\n      <td>0.3698</td>\n      <td>0.4794</td>\n      <td>0.4246</td>\n      <td>0.4246</td>\n      <td>0.0136</td>\n      <td>0.2328</td>\n      <td>-0.4520</td>\n      <td>-0.6164</td>\n      <td>20.0</td>\n    </tr>\n    <tr>\n      <th>7784</th>\n      <td>-0.6666</td>\n      <td>-0.1706</td>\n      <td>0.1746</td>\n      <td>0.1866</td>\n      <td>-0.1468</td>\n      <td>-0.4802</td>\n      <td>-0.5158</td>\n      <td>-0.5754</td>\n      <td>-0.5992</td>\n      <td>-0.5476</td>\n      <td>...</td>\n      <td>0.0878</td>\n      <td>0.7018</td>\n      <td>0.6140</td>\n      <td>0.1052</td>\n      <td>-0.1228</td>\n      <td>-0.2106</td>\n      <td>-0.1930</td>\n      <td>0.0526</td>\n      <td>-0.1052</td>\n      <td>20.0</td>\n    </tr>\n    <tr>\n      <th>7785</th>\n      <td>-0.5570</td>\n      <td>-0.2982</td>\n      <td>-0.2130</td>\n      <td>-0.2948</td>\n      <td>-0.6798</td>\n      <td>-0.9284</td>\n      <td>-0.9966</td>\n      <td>-0.9728</td>\n      <td>-0.9318</td>\n      <td>-0.8604</td>\n      <td>...</td>\n      <td>0.4318</td>\n      <td>0.3410</td>\n      <td>0.4772</td>\n      <td>0.5454</td>\n      <td>0.5000</td>\n      <td>0.3410</td>\n      <td>0.6818</td>\n      <td>0.1818</td>\n      <td>0.1136</td>\n      <td>21.0</td>\n    </tr>\n    <tr>\n      <th>7786</th>\n      <td>-0.4906</td>\n      <td>-0.2496</td>\n      <td>-0.2346</td>\n      <td>-0.4204</td>\n      <td>-0.6888</td>\n      <td>-0.9624</td>\n      <td>-0.8920</td>\n      <td>-0.7666</td>\n      <td>-0.8946</td>\n      <td>-0.8068</td>\n      <td>...</td>\n      <td>0.5366</td>\n      <td>0.5122</td>\n      <td>0.2682</td>\n      <td>0.8536</td>\n      <td>0.8048</td>\n      <td>0.5610</td>\n      <td>0.4634</td>\n      <td>0.3658</td>\n      <td>0.0000</td>\n      <td>21.0</td>\n    </tr>\n    <tr>\n      <th>7787</th>\n      <td>-0.6908</td>\n      <td>-0.3864</td>\n      <td>0.0822</td>\n      <td>0.1112</td>\n      <td>-0.4252</td>\n      <td>-0.7972</td>\n      <td>-0.7150</td>\n      <td>-0.7584</td>\n      <td>-0.7682</td>\n      <td>-0.6522</td>\n      <td>...</td>\n      <td>0.1320</td>\n      <td>0.2264</td>\n      <td>-0.0188</td>\n      <td>0.2264</td>\n      <td>0.2264</td>\n      <td>-0.0566</td>\n      <td>0.0188</td>\n      <td>0.0566</td>\n      <td>-0.4716</td>\n      <td>22.0</td>\n    </tr>\n    <tr>\n      <th>7788</th>\n      <td>-0.6266</td>\n      <td>-0.2286</td>\n      <td>0.3472</td>\n      <td>0.3996</td>\n      <td>-0.4486</td>\n      <td>-0.7906</td>\n      <td>-0.8500</td>\n      <td>-0.7976</td>\n      <td>-0.7382</td>\n      <td>-0.7940</td>\n      <td>...</td>\n      <td>0.0000</td>\n      <td>0.0678</td>\n      <td>0.1016</td>\n      <td>0.2542</td>\n      <td>0.1186</td>\n      <td>-0.1186</td>\n      <td>0.1356</td>\n      <td>-0.1526</td>\n      <td>-0.2712</td>\n      <td>22.0</td>\n    </tr>\n    <tr>\n      <th>7789</th>\n      <td>-0.6060</td>\n      <td>-0.2828</td>\n      <td>0.1010</td>\n      <td>0.4848</td>\n      <td>0.5960</td>\n      <td>0.4748</td>\n      <td>0.1414</td>\n      <td>0.0708</td>\n      <td>0.2424</td>\n      <td>0.3232</td>\n      <td>...</td>\n      <td>-0.0278</td>\n      <td>-0.2084</td>\n      <td>-0.0834</td>\n      <td>0.2638</td>\n      <td>0.1388</td>\n      <td>0.0694</td>\n      <td>-0.0694</td>\n      <td>-0.3888</td>\n      <td>-0.9028</td>\n      <td>23.0</td>\n    </tr>\n    <tr>\n      <th>7790</th>\n      <td>-0.4610</td>\n      <td>-0.2056</td>\n      <td>0.4894</td>\n      <td>0.7730</td>\n      <td>0.9008</td>\n      <td>0.8156</td>\n      <td>0.6312</td>\n      <td>0.2766</td>\n      <td>-0.0922</td>\n      <td>-0.3334</td>\n      <td>...</td>\n      <td>-0.4474</td>\n      <td>-0.2764</td>\n      <td>-0.3026</td>\n      <td>-0.2106</td>\n      <td>-0.1316</td>\n      <td>-0.2500</td>\n      <td>-0.2764</td>\n      <td>-0.4736</td>\n      <td>-0.9210</td>\n      <td>23.0</td>\n    </tr>\n    <tr>\n      <th>7791</th>\n      <td>-0.6232</td>\n      <td>-0.2682</td>\n      <td>-0.1376</td>\n      <td>0.3478</td>\n      <td>0.8478</td>\n      <td>0.8260</td>\n      <td>0.3334</td>\n      <td>0.2028</td>\n      <td>0.1450</td>\n      <td>0.0652</td>\n      <td>...</td>\n      <td>0.8266</td>\n      <td>0.5954</td>\n      <td>0.6532</td>\n      <td>0.6648</td>\n      <td>0.6648</td>\n      <td>0.6070</td>\n      <td>0.6416</td>\n      <td>0.5492</td>\n      <td>0.4104</td>\n      <td>24.0</td>\n    </tr>\n    <tr>\n      <th>7792</th>\n      <td>-0.6842</td>\n      <td>-0.3280</td>\n      <td>-0.1984</td>\n      <td>0.2956</td>\n      <td>0.8786</td>\n      <td>0.8948</td>\n      <td>0.3118</td>\n      <td>0.1822</td>\n      <td>0.1012</td>\n      <td>0.1740</td>\n      <td>...</td>\n      <td>0.7738</td>\n      <td>0.7738</td>\n      <td>0.7142</td>\n      <td>0.6428</td>\n      <td>0.5952</td>\n      <td>0.5714</td>\n      <td>0.3928</td>\n      <td>0.4286</td>\n      <td>0.2858</td>\n      <td>24.0</td>\n    </tr>\n    <tr>\n      <th>7793</th>\n      <td>-0.5912</td>\n      <td>-0.2420</td>\n      <td>0.8174</td>\n      <td>1.0000</td>\n      <td>0.4642</td>\n      <td>0.6428</td>\n      <td>0.6944</td>\n      <td>0.3056</td>\n      <td>-0.3888</td>\n      <td>-0.6826</td>\n      <td>...</td>\n      <td>0.1924</td>\n      <td>-0.1154</td>\n      <td>0.0192</td>\n      <td>0.2116</td>\n      <td>-0.0384</td>\n      <td>0.0192</td>\n      <td>-0.2308</td>\n      <td>-0.4230</td>\n      <td>-0.7116</td>\n      <td>25.0</td>\n    </tr>\n    <tr>\n      <th>7794</th>\n      <td>-0.6696</td>\n      <td>-0.3730</td>\n      <td>0.1584</td>\n      <td>0.8910</td>\n      <td>1.0000</td>\n      <td>0.9762</td>\n      <td>0.9762</td>\n      <td>0.7684</td>\n      <td>0.4106</td>\n      <td>0.0154</td>\n      <td>...</td>\n      <td>0.0910</td>\n      <td>0.1818</td>\n      <td>0.2000</td>\n      <td>0.1454</td>\n      <td>0.0182</td>\n      <td>-0.2910</td>\n      <td>0.0728</td>\n      <td>0.0728</td>\n      <td>-0.5818</td>\n      <td>25.0</td>\n    </tr>\n    <tr>\n      <th>7795</th>\n      <td>-0.5764</td>\n      <td>-0.1764</td>\n      <td>0.5106</td>\n      <td>0.3742</td>\n      <td>-0.1670</td>\n      <td>-0.5858</td>\n      <td>-0.7882</td>\n      <td>-0.7224</td>\n      <td>-0.6330</td>\n      <td>-0.8212</td>\n      <td>...</td>\n      <td>0.4130</td>\n      <td>0.5870</td>\n      <td>0.4348</td>\n      <td>0.5652</td>\n      <td>0.3478</td>\n      <td>-0.0434</td>\n      <td>0.3044</td>\n      <td>-0.0434</td>\n      <td>-0.5000</td>\n      <td>26.0</td>\n    </tr>\n    <tr>\n      <th>7796</th>\n      <td>-0.6624</td>\n      <td>-0.3334</td>\n      <td>0.3666</td>\n      <td>0.4292</td>\n      <td>-0.2084</td>\n      <td>-0.5374</td>\n      <td>-0.4542</td>\n      <td>-0.6208</td>\n      <td>-0.6376</td>\n      <td>-0.5042</td>\n      <td>...</td>\n      <td>0.2520</td>\n      <td>0.2846</td>\n      <td>0.4146</td>\n      <td>0.3170</td>\n      <td>0.2520</td>\n      <td>-0.0244</td>\n      <td>-0.0894</td>\n      <td>-0.1708</td>\n      <td>-0.3170</td>\n      <td>26.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>7797 rows × 618 columns</p>\n</div>",
            "text/plain": "           0       1       2       3       4       5       6       7       8  \\\n0    -0.4394 -0.0930  0.1718  0.4620  0.6226  0.4704  0.3578  0.0478 -0.1184   \n1    -0.4348 -0.1198  0.2474  0.4036  0.5026  0.6328  0.4948  0.0338 -0.0520   \n2    -0.2330  0.2124  0.5014  0.5222 -0.3422 -0.5840 -0.7168 -0.6342 -0.8614   \n3    -0.3808 -0.0096  0.2602  0.2554 -0.4290 -0.6746 -0.6868 -0.6650 -0.8410   \n4    -0.3412  0.0946  0.6082  0.6216 -0.1622 -0.3784 -0.4324 -0.4358 -0.4966   \n5    -0.4634  0.0306  0.3546  0.4448 -0.1022 -0.4184 -0.6388 -0.4370 -0.4396   \n6    -0.3364 -0.0102  0.2132  0.2018 -0.6146 -0.8380 -0.8130 -0.7240 -0.8062   \n7    -0.4798 -0.1580  0.1764  0.1820 -0.6378 -0.8400 -0.7280 -0.6654 -0.7978   \n8    -0.3928  0.0424  0.2166  0.2124 -0.4564 -0.6200 -0.7112 -0.6602 -0.6942   \n9    -0.5494 -0.0940  0.2868  0.2964 -0.5326 -0.7204 -0.7518 -0.7398 -0.8482   \n10   -0.3646  0.0904  0.2374  0.3044  0.6188  0.7592  0.9264  0.6522  0.6054   \n11   -0.6926 -0.3318 -0.1414 -0.1512  0.0098  0.2926  0.2440  0.3074 -0.0098   \n12   -0.2532  0.1670  0.3470  0.3734 -0.2570 -0.7260 -0.8086 -0.6360 -0.7748   \n13   -0.4030 -0.1284  0.2986  0.2358 -0.9134 -0.7552 -0.7642 -0.7134 -0.7014   \n14   -0.3216  0.1860  0.1248  0.3654  0.4880  0.4618 -0.1816 -0.0854 -0.2516   \n15   -0.5368 -0.2516  0.1448  0.1848  0.3184  0.1982  0.1046 -0.2472 -0.3898   \n16   -0.4550 -0.1044 -0.0924 -0.0300  0.1068  0.3782  0.8248  0.9856  1.0000   \n17   -0.4804 -0.1238  0.0792  0.1114  0.0700  0.4096  0.4450  0.7326  0.8448   \n18   -0.3368  0.0026  0.5802  0.4706  0.2514 -0.0910 -0.3610 -0.3664 -0.2326   \n19   -0.5880 -0.2028  0.1304  0.2278  0.2132 -0.1822 -0.2960 -0.3084 -0.7722   \n20   -0.2560  0.3910  0.2630  0.4948  0.7854  0.8132  0.3460  0.2180  0.0830   \n21   -0.3896  0.1042  0.3190  0.4478  0.8252  0.7576  0.5982  0.3190  0.2300   \n22   -0.3384  0.1034  0.2260  0.3206  0.7114  0.6372  0.6808  0.3026  0.1468   \n23   -0.4810 -0.1146  0.2654  0.2890  0.5102  0.5514  0.6180  0.2360  0.0696   \n24   -0.2116  0.2024  0.4222  0.4596  0.7216  0.5954  0.4340  0.0690  0.0036   \n25   -0.5672 -0.2202  0.0220  0.0734  0.1058  0.1516  0.2240 -0.1058 -0.1898   \n26   -0.5160 -0.2120  0.1686  0.1976  0.1562 -0.0176 -0.0362 -0.0672 -0.1540   \n27   -0.5330 -0.0868  0.1838  0.2394  0.4428  0.2694 -0.0150 -0.2924 -0.4220   \n28   -0.3656  0.0296  0.3160  0.5526  0.9030  0.9574  0.9834  0.8556  0.9314   \n29   -0.5154 -0.1740  0.3140  0.5392  0.8566  0.9488  0.7884  0.7508  0.8464   \n...      ...     ...     ...     ...     ...     ...     ...     ...     ...   \n7767 -0.6990 -0.3870 -0.2114  0.2940  0.6200  0.5018  0.1936 -0.1900 -0.0610   \n7768 -0.7204 -0.4226 -0.3142  0.3954  0.7354  0.6452  0.2210 -0.0286 -0.0766   \n7769 -0.6998 -0.3716 -0.2792 -0.1220  0.4586  0.6270  0.4026  0.0604 -0.0098   \n7770 -0.6426 -0.3728 -0.2142  0.2946  0.5054  0.4178  0.0178 -0.1408 -0.1550   \n7771 -0.7230 -0.2998 -0.2542 -0.1112  0.4186  0.5190  0.1354 -0.2208 -0.1872   \n7772 -0.6600 -0.3630 -0.3036 -0.0284  0.5060  0.5654  0.4332 -0.0122 -0.0770   \n7773 -0.7114 -0.3586 -0.1546  0.3760  0.6998  0.7872  0.3848  0.3032  0.6968   \n7774 -0.7338 -0.5768 -0.4266  0.2492  0.4574  0.4778  0.1980  0.1638  0.9864   \n7775  0.5888  0.4274 -0.3870 -0.8226 -0.8870 -0.8630 -0.7580 -0.7822 -0.9516   \n7776 -0.7778 -0.2790  0.0074  0.0518 -0.3284 -0.6098 -0.7976 -0.6790 -0.6790   \n7777 -0.5012  0.0272  0.3630  0.3876 -0.3976 -0.5210 -0.6198 -0.6740 -0.5704   \n7778 -0.6100 -0.2908  0.0532  0.0602 -0.5710 -0.8936 -0.8510 -0.7978 -0.8724   \n7779 -0.7250 -0.5844 -0.3992 -0.1988  0.6292  1.0000  0.9642  0.8984  0.9162   \n7780 -0.6846 -0.5768 -0.4132  0.0340  0.7486  0.9560  0.7524  0.7844  1.0000   \n7781 -0.6350 -0.3460 -0.3004  0.2776  0.7566  0.6122  0.3840  0.0646 -0.2168   \n7782 -0.7304 -0.5392 -0.4174  0.0086  0.6174  0.7304  0.5130  0.0956 -0.1044   \n7783 -0.6022  0.0000  0.3940  0.3372 -0.2840 -0.4734 -0.5910 -0.5758 -0.5228   \n7784 -0.6666 -0.1706  0.1746  0.1866 -0.1468 -0.4802 -0.5158 -0.5754 -0.5992   \n7785 -0.5570 -0.2982 -0.2130 -0.2948 -0.6798 -0.9284 -0.9966 -0.9728 -0.9318   \n7786 -0.4906 -0.2496 -0.2346 -0.4204 -0.6888 -0.9624 -0.8920 -0.7666 -0.8946   \n7787 -0.6908 -0.3864  0.0822  0.1112 -0.4252 -0.7972 -0.7150 -0.7584 -0.7682   \n7788 -0.6266 -0.2286  0.3472  0.3996 -0.4486 -0.7906 -0.8500 -0.7976 -0.7382   \n7789 -0.6060 -0.2828  0.1010  0.4848  0.5960  0.4748  0.1414  0.0708  0.2424   \n7790 -0.4610 -0.2056  0.4894  0.7730  0.9008  0.8156  0.6312  0.2766 -0.0922   \n7791 -0.6232 -0.2682 -0.1376  0.3478  0.8478  0.8260  0.3334  0.2028  0.1450   \n7792 -0.6842 -0.3280 -0.1984  0.2956  0.8786  0.8948  0.3118  0.1822  0.1012   \n7793 -0.5912 -0.2420  0.8174  1.0000  0.4642  0.6428  0.6944  0.3056 -0.3888   \n7794 -0.6696 -0.3730  0.1584  0.8910  1.0000  0.9762  0.9762  0.7684  0.4106   \n7795 -0.5764 -0.1764  0.5106  0.3742 -0.1670 -0.5858 -0.7882 -0.7224 -0.6330   \n7796 -0.6624 -0.3334  0.3666  0.4292 -0.2084 -0.5374 -0.4542 -0.6208 -0.6376   \n\n           9   ...       608     609     610     611     612     613     614  \\\n0    -0.2310   ...    0.4102  0.2052  0.3846  0.3590  0.5898  0.3334  0.6410   \n1    -0.1302   ...    0.0000  0.2954  0.2046  0.4772  0.0454  0.2046  0.4318   \n2    -0.8318   ...   -0.1112 -0.0476 -0.1746  0.0318 -0.0476  0.1112  0.2540   \n3    -0.9614   ...   -0.0504 -0.0360 -0.1224  0.1366  0.2950  0.0792 -0.0072   \n4    -0.5406   ...    0.1562  0.3124  0.2500 -0.0938  0.1562  0.3124  0.3124   \n5    -0.6654   ...    0.6626  0.7350  0.3734  0.6626  0.3012  0.1808  0.2290   \n6    -0.8996   ...    0.0526 -0.0702 -0.0350  0.0702  0.1578  0.1930  0.4562   \n7    -0.7904   ...    0.2912 -0.1646  0.1140  0.0126 -0.0380  0.0886  0.2912   \n8    -0.7920   ...    0.8868  0.8868  0.6792  0.6038  0.2264  0.7924  1.0000   \n9    -0.8386   ...    0.6130  0.6130  0.6130  0.3226  0.6130  0.2904  0.5484   \n10    0.2442   ...    0.9178  1.0000  0.8082  0.8220  0.7672  0.7534  0.6986   \n11   -0.0536   ...    0.7460  0.8888  0.7460  0.6984  0.8254  1.0000  0.9842   \n12   -0.8536   ...    0.5302  0.2886  0.2618  0.1276 -0.0738  0.2618  0.3288   \n13   -0.7940   ...    0.4666  0.5200  0.6266  0.3866  0.4666  0.4134  0.5466   \n14   -0.4530   ...    0.8222  0.9778  0.8444  0.7666  0.7888  0.8000  0.6556   \n15   -0.3364   ...    0.7806  0.8580  0.8968  0.8322  0.8322  1.0000  0.7936   \n16    0.9640   ...    0.4588  0.4824  0.3412  0.2236  0.4118  0.5294  0.6942   \n17    0.9616   ...    0.2766  0.2554  0.1914  0.2978  0.2978 -0.0638  0.0212   \n18   -0.4866   ...   -0.1162  0.0388  0.0698 -0.1472  0.0388  0.2558  0.1162   \n19   -0.4224   ...    0.7024  0.5702  0.4710  0.6694  0.8348  0.8182  0.6860   \n20   -0.1418   ...    0.0378  0.0944  0.0188 -0.2642 -0.3774  0.0754  0.5284   \n21    0.0184   ...    0.7846  0.4770  0.5076  0.6924  0.3846  0.3538  0.5692   \n22   -0.0932   ...    0.8410  0.3864 -0.1136  0.3636  0.3182  0.2272  0.4546   \n23   -0.0324   ...    0.1384  0.3538  0.0462  0.4770  0.3538 -0.1692  0.2616   \n24   -0.2234   ...    1.0000  0.8832  0.9708  0.9124  0.7810  0.8686  0.7664   \n25   -0.1020   ...    0.3056  0.0278  0.3056  0.3334  0.1666  0.2500  0.3334   \n26   -0.6360   ...    0.2632  0.0526  0.1316  0.3158  0.3158  0.5000  0.3948   \n27   -0.5654   ...    0.5164  0.2748  0.1648  0.3626  0.2968  0.4506  0.2748   \n28    0.9692   ...    0.1904  0.2380  0.3810  0.4762  0.5952  0.5714  0.8096   \n29    1.0000   ...    0.1154  0.0962  0.0000 -0.2308 -0.1730  0.2116  0.1924   \n...      ...   ...       ...     ...     ...     ...     ...     ...     ...   \n7767 -0.0430   ...    0.0666 -0.0814 -0.1112  0.0518 -0.0074  0.0962  0.0222   \n7768 -0.1338   ...    0.6950  0.3220  0.2204  0.7288  0.8306  0.4238  0.6272   \n7769  0.0546   ...    0.5000  0.5000  0.2826  0.0000  0.1956  0.1086  0.2826   \n7770 -0.0366   ...    0.6632  0.3264  0.3474  0.5158  0.1578  0.4106  0.2210   \n7771 -0.1628   ...    0.1052 -0.1404  0.1754  0.2632  0.1754  0.0350 -0.0350   \n7772 -0.0742   ...    0.3904  0.5428  0.3904  0.3334  0.3334  0.3714  0.1048   \n7773  1.0000   ...    0.2958  0.3802  0.3522  0.0986  0.5212  0.1268  0.0422   \n7774  1.0000   ...    0.4762  0.3334  0.4762  0.7142  0.3810  0.2620  0.0714   \n7775 -0.8870   ...    0.1098  0.0550  0.0110 -0.0330  0.0330  0.0110  0.0220   \n7776 -0.6790   ...    0.4640  0.2784  0.4640  0.7938  0.5876  0.3402  0.0722   \n7777 -0.5210   ...    0.7288  0.4576  0.0170  0.2542  0.5932  0.6272  0.5932   \n7778 -0.8298   ...    0.4888  0.6222  0.0888  0.7112  0.3334  0.6666  0.6666   \n7779  0.9910   ...    0.2874  0.5632  0.4482  0.6782  0.3334  0.1954  0.3104   \n7780  0.9880   ...    0.4728  0.2000  0.2364  0.2910 -0.0182  0.1090  0.1090   \n7781 -0.0494   ...    1.0000  0.9488  0.8974  0.7436  0.6512  0.5794  0.6718   \n7782 -0.0260   ...    0.7112  0.6334  0.6112  0.7778  0.8112  0.4000  0.5222   \n7783 -0.5000   ...    0.4246  0.3698  0.4794  0.4246  0.4246  0.0136  0.2328   \n7784 -0.5476   ...    0.0878  0.7018  0.6140  0.1052 -0.1228 -0.2106 -0.1930   \n7785 -0.8604   ...    0.4318  0.3410  0.4772  0.5454  0.5000  0.3410  0.6818   \n7786 -0.8068   ...    0.5366  0.5122  0.2682  0.8536  0.8048  0.5610  0.4634   \n7787 -0.6522   ...    0.1320  0.2264 -0.0188  0.2264  0.2264 -0.0566  0.0188   \n7788 -0.7940   ...    0.0000  0.0678  0.1016  0.2542  0.1186 -0.1186  0.1356   \n7789  0.3232   ...   -0.0278 -0.2084 -0.0834  0.2638  0.1388  0.0694 -0.0694   \n7790 -0.3334   ...   -0.4474 -0.2764 -0.3026 -0.2106 -0.1316 -0.2500 -0.2764   \n7791  0.0652   ...    0.8266  0.5954  0.6532  0.6648  0.6648  0.6070  0.6416   \n7792  0.1740   ...    0.7738  0.7738  0.7142  0.6428  0.5952  0.5714  0.3928   \n7793 -0.6826   ...    0.1924 -0.1154  0.0192  0.2116 -0.0384  0.0192 -0.2308   \n7794  0.0154   ...    0.0910  0.1818  0.2000  0.1454  0.0182 -0.2910  0.0728   \n7795 -0.8212   ...    0.4130  0.5870  0.4348  0.5652  0.3478 -0.0434  0.3044   \n7796 -0.5042   ...    0.2520  0.2846  0.4146  0.3170  0.2520 -0.0244 -0.0894   \n\n         615     616  Letter  \n0     0.5898 -0.4872     1.0  \n1     0.4546 -0.0910     1.0  \n2     0.1588 -0.4762     2.0  \n3     0.0936 -0.1510     2.0  \n4     0.2188 -0.2500     3.0  \n5     0.6144  0.3254     3.0  \n6     0.4562 -0.3860     4.0  \n7     0.3670  0.1646     4.0  \n8     0.9246  0.5284     5.0  \n9     0.5162  0.3548     5.0  \n10    0.4932 -0.0548     6.0  \n11    0.5238  0.1746     6.0  \n12    0.1276 -0.1812     7.0  \n13    0.2000 -0.3600     7.0  \n14    0.5778  0.3000     8.0  \n15    0.6646 -0.2000     8.0  \n16    0.6000  0.3648     9.0  \n17    0.1276 -0.2340     9.0  \n18    0.1008 -0.3644    10.0  \n19    0.4380 -0.1240    10.0  \n20    0.5094 -0.3018    11.0  \n21    0.6308  0.4770    11.0  \n22   -0.2500 -0.5228    12.0  \n23    0.0770 -0.3230    12.0  \n24    0.4598  0.2700    13.0  \n25   -0.3612 -0.7500    13.0  \n26    0.3684  0.1578    14.0  \n27   -0.0550 -0.8462    14.0  \n28    0.3334 -0.4762    15.0  \n29    0.4038  0.1346    15.0  \n...      ...     ...     ...  \n7767 -0.4370 -0.6888    12.0  \n7768  0.4916 -0.1186    12.0  \n7769 -0.0434 -0.4782    13.0  \n7770  0.3052 -0.2632    13.0  \n7771  0.0176 -0.7018    14.0  \n7772  0.4666  0.0096    14.0  \n7773  0.1830 -0.1830    15.0  \n7774  0.1190 -0.5000    15.0  \n7775  0.1208 -0.2418    16.0  \n7776  0.0310 -0.2990    16.0  \n7777 -0.0848 -0.6950    17.0  \n7778  0.6444  0.2666    17.0  \n7779  0.0114 -0.2644    18.0  \n7780  0.0910 -0.7636    18.0  \n7781  0.7538  0.4974    19.0  \n7782  0.6556  0.3666    19.0  \n7783 -0.4520 -0.6164    20.0  \n7784  0.0526 -0.1052    20.0  \n7785  0.1818  0.1136    21.0  \n7786  0.3658  0.0000    21.0  \n7787  0.0566 -0.4716    22.0  \n7788 -0.1526 -0.2712    22.0  \n7789 -0.3888 -0.9028    23.0  \n7790 -0.4736 -0.9210    23.0  \n7791  0.5492  0.4104    24.0  \n7792  0.4286  0.2858    24.0  \n7793 -0.4230 -0.7116    25.0  \n7794  0.0728 -0.5818    25.0  \n7795 -0.0434 -0.5000    26.0  \n7796 -0.1708 -0.3170    26.0  \n\n[7797 rows x 618 columns]"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "print(\"Έχουμε\", len(df), \"δείγματα.\")\nprint(\"Το καθένα έχει \", df.shape[1]-1, \"χαραχτηριστικά, και 1 για την κλάση.\")",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Έχουμε 7797 δείγματα.\nΤο καθένα έχει  617 χαραχτηριστικά, και 1 για την κλάση.\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Διαδικαστικά τσεκάρουμε για ενδεχόμενες απουσιάζουσες τιμές αν και γνωρίζουμε ότι δεν υπάρχουν από την περιγραφή του dataset"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "null_columns=df.columns[df.isnull().any()]\nprint(df[df.isnull().any(axis=1)][null_columns])",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Empty DataFrame\nColumns: []\nIndex: []\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Σε ό,τι αφορά την ισορροπία του dataset μας, υπάρχει απόλυτη ισορροπία καθώς όλες οι τιμές της κλάσεις εμφανίζονται ακριβώς τις ίδιες φορές."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Προχωράμε με τον χωρισμό του dataset σε train και test, με το test set να είναι το 30% του dataset όπως ζητείται. Εισάγουμε και τη numpy για μετατροπή του dataframe σε numpy array. Αλλάζουμε και τις ετικέτες των κλάσεων."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "label_names = [\"A\",\"B\",\"C\",\"D\",\"E\",\"F\",\"G\",\"H\",\"I\",\"J\",\"K\",\"L\",\"M\",\"N\",\"O\",\"P\",\"Q\",\"R\",\"S\",\"T\",\"U\",\"V\",\"W\",\"X\",\"Y\",\"Z\"]\nlabels_df = df.iloc[:,[617]]\nfeatures_df = df.iloc[:,:617]",
      "execution_count": 8,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import numpy as np\nfeatures_df_t=np.asarray(features_df)\nlabels_df_t=df['Letter']\nlabels_df_t =list(map(lambda x : x, labels_df_t)) \nfrom sklearn.model_selection import train_test_split\n\ntrain, test, train_labels, test_labels = train_test_split(features_df_t,labels_df_t, test_size = 0.3)\n\ntrain_dummy = train\ntrain_labels_dummy = train_labels \ntest_dummy = test\ntest_labels_dummy = test_labels",
      "execution_count": 9,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Γ. Baseline classification"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Είμαστε έτοιμοι να εκπαιδεύσουμε τους ταξινομητές χωρίς βελτιστοποιημένες παραμέτρους (με αρχικές τιμές απλά) ή επεξεργασία σε πρώτη φάση. Σημειώνουμε ότι δεν εκτελούμε τον dummyclassifier με strategy constant καθώς με τις 26 διαφορετικές κλάσεις δεν έχει ιδιαίτερο νόημα. Παρακάτω φαίνονται κατά σειρά όλες οι μέθοδοι (dummy classifiers,kNN,gnb,mlp), τυπώνονται τα confussion matrix, f1-macro average,f1-micro average τους για κάθε estimator και παρουσιάζονται τα αποτελέσματα τους με plots σύγκρισης κάθε estimator σχετικά με τους δείτκες f1 τους."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from sklearn.metrics import classification_report\nfrom sklearn.metrics import precision_recall_fscore_support\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.dummy import DummyClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.metrics import classification_report\nfrom sklearn.naive_bayes import GaussianNB\n\ndef function(train_in,train_labels_in,test_in,test_labels_in):\n    credit_accuracy = {}\n\n    dc_uniform = DummyClassifier(strategy=\"uniform\")\n\n# με τη μέθοδο fit \"εκπαιδεύουμε\" τον ταξινομητή στο σύνολο εκπαίδευσης (τα χαρακτηριστικά και τις ετικέτες τους)\n    model = dc_uniform.fit(train_in, train_labels_in)\n\n# με τη μέθοδο predict παράγουμε προβλέψεις για τα δεδομένα ελέγχου (είσοδος τα χαρακτηριστικά μόνο)\n    preds = dc_uniform.predict(test_in)\n\n# υπολογίζουμε την ακρίβεια του συγκεκριμένου μοντέλου dummy classifier\n    credit_accuracy['uniform (random)'] = dc_uniform.score(test_in, test_labels_in)\n\n\n#################\n    print ('Classification report for Dummy Classifier (uniform)')\n    cr_dummy_uni = classification_report(test_labels_in, preds,target_names = label_names)\n    print (cr_dummy_uni)\n\n    scores_weighted = {}\n    scores_macro = {}\n    scores_micro = {}\n\n    scores_weighted['Dummy-Uniform']=precision_recall_fscore_support(test_labels_in,preds,average='weighted')\n    scores_macro['Dummy-Uniform']=precision_recall_fscore_support(test_labels_in,preds,average='macro')\n    scores_micro['Dummy-Uniform']=precision_recall_fscore_support(test_labels_in,preds,average='micro')\n\n\n    print ('Confusion Matrix for Dummy Classifier (uniform)')\n    print (confusion_matrix(test_labels_in, preds))\n\n    acc_dummy_uni = 100*accuracy_score(test_labels_in,preds)\n    print ('Accuracy percentage of this classifier is %.3f %%\\n' % (acc_dummy_uni))\n    \n    dc_most_frequent = DummyClassifier(strategy=\"most_frequent\")\n    model = dc_most_frequent.fit(train_in, train_labels_in)\n    preds = dc_most_frequent.predict(test_in)\n    credit_accuracy['most_frequent'] = dc_most_frequent.score(test_in, test_labels_in)\n\n#################\n    print ('Classification report for Dummy Classifier (most frequent)')\n    cr_dummy_freq = classification_report(test_labels_in, preds,target_names = label_names)\n    print (cr_dummy_freq)\n\n    scores_weighted['Dummy-Most_Freq']=precision_recall_fscore_support(test_labels_in,preds,average='weighted')\n    scores_macro['Dummy-Most_Freq']=precision_recall_fscore_support(test_labels_in,preds,average='macro')\n    scores_micro['Dummy-Most_Freq']=precision_recall_fscore_support(test_labels_in,preds,average='micro')\n\n    print ('Confusion Matrix for Dummy Classifier (most frequent)')\n    print (confusion_matrix(test_labels_in, preds))\n\n    acc_dummy_freq = 100*accuracy_score(test_labels_in,preds)\n    print ('Accuracy percentage of this classifier is %.3f %%\\n' % (acc_dummy_freq))\n    \n    \n    \n    dc_stratified = DummyClassifier(strategy=\"stratified\")\n    model = dc_stratified.fit(train_in, train_labels_in)\n    preds = dc_stratified.predict(test_in)\n    credit_accuracy['stratified'] = dc_stratified.score(test_in, test_labels_in)\n\n#################\n    print ('Classification report for Dummy Classifier (stratified)')\n    cr_dummy_strat = classification_report(test_labels_in, preds,target_names = label_names)\n    print (cr_dummy_strat)\n\n    scores_weighted['Dummy-Strat']=precision_recall_fscore_support(test_labels_in,preds,average='weighted')\n    scores_macro['Dummy-Strat']=precision_recall_fscore_support(test_labels_in,preds,average='macro')\n    scores_micro['Dummy-Strat']=precision_recall_fscore_support(test_labels_in,preds,average='macro')\n\n    print ('Confusion Matrix for Dummy Classifier (stratified)')\n    print (confusion_matrix(test_labels_in, preds))\n\n    acc_dummy_strat = 100*accuracy_score(test_labels_in,preds)\n    print ('Accuracy percentage of this classifier is %.3f %%\\n' % (acc_dummy_strat))\n    \n    from sklearn.neighbors import KNeighborsClassifier\n    knn = KNeighborsClassifier()\n    knn.fit(train_in, train_labels_in)\n    knn_preds = knn.predict(test_in)\n\n\n#################\n    print ('Classification report for kNN')\n    cr_knn_no = classification_report(test_labels_in,knn_preds, target_names=label_names)\n    print (cr_knn_no)\n\n    scores_weighted['kNN-non-opt']=precision_recall_fscore_support(test_labels_in,knn_preds,average='weighted')\n    scores_macro['kNN-non-opt']=precision_recall_fscore_support(test_labels_in,knn_preds,average='macro')\n    scores_micro['kNN-non-opt']=precision_recall_fscore_support(test_labels_in,knn_preds,average='micro')\n\n    print ('Confusion Matrix for non-optimized kNN ')\n    print (confusion_matrix(test_labels_in, knn_preds))\n\n    acc_knn_no = 100*accuracy_score(test_labels_in,knn_preds)\n    print ('\\nAccuracy percentage of this classifier is %.3f %%\\n' % (acc_knn_no))\n    \n    \n    ###############\n    \n    \n    clf = MLPClassifier()\n    clf.fit(train_in, train_labels_in)\n    preds = clf.predict(test_in)\n\n    print ('Classification report for MLP on initial data')\n    cr_mlp_dummy = classification_report(test_labels_in, preds)\n    print (cr_mlp_dummy)\n\n    scores_weighted['MLP_dummy']=precision_recall_fscore_support(test_labels_in,preds,average='weighted')\n    scores_macro['MLP_dummy']=precision_recall_fscore_support(test_labels_in,preds,average='macro')\n    scores_micro['MLP_dummy']=precision_recall_fscore_support(test_labels_in,preds,average='micro')\n    \n    print ('Confusion matrix for MLP on initial data')\n    print (confusion_matrix(test_labels_in, preds))\n\n    acc_mlp_dummy = 100*accuracy_score(test_labels,preds)\n    print ('Accuracy percentage of this classifier is %.3f %%' % (acc_mlp_dummy))\n    \n    \n    ######\n    \n    gnb = GaussianNB()\n    gnb.fit(train_in, train_labels_in)\n    preds=gnb.predict(test_in)\n\n#################\n    print ('Classification report for non_opt Gaussian Naive Bayes Classifier')\n    cr_gnb = classification_report(test_labels_in, preds)\n    print (cr_gnb)\n\n    scores_weighted['GNB']=precision_recall_fscore_support(test_labels_in,preds,average='weighted')\n    scores_macro['GNB']=precision_recall_fscore_support(test_labels_in,preds,average='macro')\n    scores_micro['GNB']=precision_recall_fscore_support(test_labels_in,preds,average='micro')\n\n    print ('Confusion Matrix for Gaussian Naive Bayes Classifier')\n    print (confusion_matrix(test_labels_in, preds))\n\n    acc_gnb = 100*accuracy_score(test_labels_in,preds)\n    print ('\\nAccuracy percentage of this classifier is %.3f %%' % (acc_gnb))\n    \n    \n    import matplotlib.pyplot as plt\n\n    f1_scores_macro = [item[2] for item in scores_macro.values()]\n    f1_scores_micro = [item[2] for item in scores_micro.values()]\n    f1_scores_weighted = [item[2] for item in scores_weighted.values()]\n\n\n    y_pos = np.arange(len(f1_scores_macro))\n    plt.barh(y_pos, f1_scores_macro, align='center',color='red')\n    plt.yticks(y_pos, scores_macro.keys())\n    plt.title('F1_macro average scores')\n    plt.show()\n\n    y_pos = np.arange(len(f1_scores_micro))\n    plt.barh(y_pos, f1_scores_micro, align='center',color='yellow')\n    plt.yticks(y_pos, scores_micro.keys())\n    plt.title('F1_micro average scores')\n    plt.show()\n\n    \n    y_pos = np.arange(len(f1_scores_weighted))\n    plt.barh(y_pos, f1_scores_weighted, align='center',color='green')\n    plt.yticks(y_pos, scores_weighted.keys())\n    plt.title('F1_weighted average scores')\n    plt.show()\n    \n    \n    return",
      "execution_count": 10,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "function(train,train_labels,test,test_labels)",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Classification report for Dummy Classifier (uniform)\n              precision    recall  f1-score   support\n\n           A       0.06      0.06      0.06        84\n           B       0.06      0.06      0.06        95\n           C       0.03      0.03      0.03       100\n           D       0.06      0.05      0.06        91\n           E       0.02      0.03      0.03        69\n           F       0.04      0.03      0.04        90\n           G       0.08      0.07      0.07        83\n           H       0.02      0.02      0.02        87\n           I       0.01      0.01      0.01        87\n           J       0.05      0.06      0.05        90\n           K       0.09      0.09      0.09        82\n           L       0.06      0.07      0.06        88\n           M       0.02      0.02      0.02        97\n           N       0.07      0.07      0.07        99\n           O       0.01      0.01      0.01        80\n           P       0.04      0.03      0.03        98\n           Q       0.01      0.01      0.01        84\n           R       0.05      0.06      0.06        89\n           S       0.01      0.01      0.01       100\n           T       0.07      0.08      0.07        87\n           U       0.06      0.06      0.06        94\n           V       0.07      0.06      0.06       101\n           W       0.02      0.02      0.02        97\n           X       0.03      0.02      0.02        90\n           Y       0.02      0.02      0.02        90\n           Z       0.02      0.02      0.02        88\n\n   micro avg       0.04      0.04      0.04      2340\n   macro avg       0.04      0.04      0.04      2340\nweighted avg       0.04      0.04      0.04      2340\n\nConfusion Matrix for Dummy Classifier (uniform)\n[[ 5  8  1  5  4  3  3  3  4  2  2  3  2  0  2  3  2  2  4  3  6  4  6  0\n   6  1]\n [ 5  6  7  3 10  3  3  4  0  4  2  4  3  3  4  2  2  4  2  4  3  3  5  4\n   2  3]\n [ 2  1  3  4  4  3  4  2  2  2  5  4  5  5  5  3  3  4  5  7  5  3  3  4\n   6  6]\n [ 3  4  9  5  2  2  1  5  3  5  2  9  5  3  2  1  6  2  4  2  2  3  5  1\n   3  2]\n [ 3  1  4  4  2  1  1  3  2  2  3  4  3  3  2  2  4  4  0  6  1  5  1  2\n   5  1]\n [ 1  1  3  2  8  3  1  2  3  7  1  0  2  5  2  8  4  5  4  2  3  4  5  2\n   7  5]\n [ 2  7  1  4  0  2  6  1  3  4  6  3  5  2  2  0  3  7  5  2  6  1  4  4\n   2  1]\n [ 2  4  2  3  2  5  4  2  2  4  3  1  3  6  4  2  3  2  7  9  3  3  3  3\n   0  5]\n [ 4  4  4  2  2  6  1  3  1  3  4  1  7  5  3  6  2  3  5  6  5  0  4  3\n   2  1]\n [ 3  4  0  5  2  2  6  2  1  5  4  6  4  6  6  3  1  5  5  2  2  1  5  3\n   3  4]\n [ 2  5  1  3  3  2  4  3  5  0  7  6  8  2  4  3  4  0  5  1  3  2  0  0\n   5  4]\n [ 3  4  2  1  1  4  1  2  2  5  3  6  4  8  6  2  3  4  4  3  5  5  3  1\n   3  3]\n [ 3  2  4  3  3  3  3  6  5  1  2  5  2  7  3  6  1  4  3  3  6  9  1  4\n   7  1]\n [ 4  3  1  4  2  3  5  6  3  5  3  5  4  7  2  3  2  2  1  9  3  3  3  7\n   6  3]\n [ 2  3  2  5  3  3  1  9  1  4  2  2  3  4  1  2  3  2  4  3  7  2  4  1\n   4  3]\n [ 7  1  4  3  1  2  1  2  2  3  4  6  4  6  4  3  5  2  2  4  6  5  6  2\n   7  6]\n [ 0  8  3  4  6  7  1  5  3  2  2  5  2  1  5  3  1  7  4  3  0  2  4  1\n   3  2]\n [ 5  5  4  4  2  3  1  1  1  5  0  2  8  5  0  2  4  5  1  4  3  5  2  7\n   4  6]\n [ 4  6  4  2  4  3  4  3  5  4  3  4  7  3  6  5  1  4  1  7  4  3  5  1\n   2  5]\n [ 5  2  5  1  4  2  4  4  1  5  0  5  3  2  4  5  4  3  4  7  3  3  2  3\n   2  4]\n [ 9  5  6  1  5  0  1  5  4  5  5  4  2  5  2  1  1  1  1  3  6  4  4  5\n   4  5]\n [ 2  2  5  3  4  1  2  0  6  5  3  4  4  3  3  4  1  7  3  3  4  6  5  5\n   4 12]\n [ 5  4  3  1  6  2  6  5  5  4  3  3  4  5  3  5  2  3  3  2  3  6  2  5\n   2  5]\n [ 3  3  5  2  2  4  4  4  2  6  2  6  4  2  2  2  3  6  2  4 10  3  2  2\n   0  5]\n [ 2  4  5  5  1  2  7  3  6  3  4  5  4  5  2  2  2  1  6  3  5  1  5  2\n   2  3]\n [ 1  3  7  6  5  7  4  4  4  1  3  6  2  1  2  4  3  2  2  5  4  3  2  3\n   2  2]]\nAccuracy percentage of this classifier is 4.188 %\n\nClassification report for Dummy Classifier (most frequent)\n              precision    recall  f1-score   support\n\n           A       0.00      0.00      0.00        84\n           B       0.00      0.00      0.00        95\n           C       0.00      0.00      0.00       100\n           D       0.00      0.00      0.00        91\n           E       0.03      1.00      0.06        69\n           F       0.00      0.00      0.00        90\n           G       0.00      0.00      0.00        83\n           H       0.00      0.00      0.00        87\n           I       0.00      0.00      0.00        87\n           J       0.00      0.00      0.00        90\n           K       0.00      0.00      0.00        82\n           L       0.00      0.00      0.00        88\n           M       0.00      0.00      0.00        97\n           N       0.00      0.00      0.00        99\n           O       0.00      0.00      0.00        80\n           P       0.00      0.00      0.00        98\n           Q       0.00      0.00      0.00        84\n           R       0.00      0.00      0.00        89\n           S       0.00      0.00      0.00       100\n           T       0.00      0.00      0.00        87\n           U       0.00      0.00      0.00        94\n           V       0.00      0.00      0.00       101\n           W       0.00      0.00      0.00        97\n           X       0.00      0.00      0.00        90\n           Y       0.00      0.00      0.00        90\n           Z       0.00      0.00      0.00        88\n\n   micro avg       0.03      0.03      0.03      2340\n   macro avg       0.00      0.04      0.00      2340\nweighted avg       0.00      0.03      0.00      2340\n\nConfusion Matrix for Dummy Classifier (most frequent)\n[[  0   0   0   0  84   0   0   0   0   0   0   0   0   0   0   0   0   0\n    0   0   0   0   0   0   0   0]\n [  0   0   0   0  95   0   0   0   0   0   0   0   0   0   0   0   0   0\n    0   0   0   0   0   0   0   0]\n [  0   0   0   0 100   0   0   0   0   0   0   0   0   0   0   0   0   0\n    0   0   0   0   0   0   0   0]\n [  0   0   0   0  91   0   0   0   0   0   0   0   0   0   0   0   0   0\n    0   0   0   0   0   0   0   0]\n [  0   0   0   0  69   0   0   0   0   0   0   0   0   0   0   0   0   0\n    0   0   0   0   0   0   0   0]\n [  0   0   0   0  90   0   0   0   0   0   0   0   0   0   0   0   0   0\n    0   0   0   0   0   0   0   0]\n [  0   0   0   0  83   0   0   0   0   0   0   0   0   0   0   0   0   0\n    0   0   0   0   0   0   0   0]\n [  0   0   0   0  87   0   0   0   0   0   0   0   0   0   0   0   0   0\n    0   0   0   0   0   0   0   0]\n [  0   0   0   0  87   0   0   0   0   0   0   0   0   0   0   0   0   0\n    0   0   0   0   0   0   0   0]\n [  0   0   0   0  90   0   0   0   0   0   0   0   0   0   0   0   0   0\n    0   0   0   0   0   0   0   0]\n [  0   0   0   0  82   0   0   0   0   0   0   0   0   0   0   0   0   0\n    0   0   0   0   0   0   0   0]\n [  0   0   0   0  88   0   0   0   0   0   0   0   0   0   0   0   0   0\n    0   0   0   0   0   0   0   0]\n [  0   0   0   0  97   0   0   0   0   0   0   0   0   0   0   0   0   0\n    0   0   0   0   0   0   0   0]\n [  0   0   0   0  99   0   0   0   0   0   0   0   0   0   0   0   0   0\n    0   0   0   0   0   0   0   0]\n [  0   0   0   0  80   0   0   0   0   0   0   0   0   0   0   0   0   0\n    0   0   0   0   0   0   0   0]\n [  0   0   0   0  98   0   0   0   0   0   0   0   0   0   0   0   0   0\n    0   0   0   0   0   0   0   0]\n [  0   0   0   0  84   0   0   0   0   0   0   0   0   0   0   0   0   0\n    0   0   0   0   0   0   0   0]\n [  0   0   0   0  89   0   0   0   0   0   0   0   0   0   0   0   0   0\n    0   0   0   0   0   0   0   0]\n [  0   0   0   0 100   0   0   0   0   0   0   0   0   0   0   0   0   0\n    0   0   0   0   0   0   0   0]\n [  0   0   0   0  87   0   0   0   0   0   0   0   0   0   0   0   0   0\n    0   0   0   0   0   0   0   0]\n [  0   0   0   0  94   0   0   0   0   0   0   0   0   0   0   0   0   0\n    0   0   0   0   0   0   0   0]\n [  0   0   0   0 101   0   0   0   0   0   0   0   0   0   0   0   0   0\n    0   0   0   0   0   0   0   0]\n [  0   0   0   0  97   0   0   0   0   0   0   0   0   0   0   0   0   0\n    0   0   0   0   0   0   0   0]\n [  0   0   0   0  90   0   0   0   0   0   0   0   0   0   0   0   0   0\n    0   0   0   0   0   0   0   0]\n [  0   0   0   0  90   0   0   0   0   0   0   0   0   0   0   0   0   0\n    0   0   0   0   0   0   0   0]\n [  0   0   0   0  88   0   0   0   0   0   0   0   0   0   0   0   0   0\n    0   0   0   0   0   0   0   0]]\nAccuracy percentage of this classifier is 2.949 %\n\nClassification report for Dummy Classifier (stratified)\n              precision    recall  f1-score   support\n\n           A       0.05      0.06      0.06        84\n           B       0.02      0.02      0.02        95\n           C       0.05      0.05      0.05       100\n           D       0.14      0.14      0.14        91\n           E       0.06      0.07      0.06        69\n           F       0.01      0.01      0.01        90\n           G       0.02      0.02      0.02        83\n           H       0.04      0.03      0.04        87\n           I       0.06      0.07      0.07        87\n           J       0.03      0.03      0.03        90\n           K       0.04      0.05      0.04        82\n           L       0.05      0.06      0.05        88\n           M       0.05      0.05      0.05        97\n           N       0.01      0.01      0.01        99\n           O       0.02      0.03      0.02        80\n           P       0.03      0.03      0.03        98\n           Q       0.01      0.01      0.01        84\n           R       0.03      0.02      0.02        89\n           S       0.02      0.02      0.02       100\n           T       0.06      0.07      0.06        87\n           U       0.00      0.00      0.00        94\n           V       0.04      0.04      0.04       101\n           W       0.01      0.01      0.01        97\n           X       0.01      0.01      0.01        90\n           Y       0.03      0.03      0.03        90\n           Z       0.03      0.03      0.03        88\n\n   micro avg       0.04      0.04      0.04      2340\n   macro avg       0.04      0.04      0.04      2340\nweighted avg       0.04      0.04      0.04      2340\n\nConfusion Matrix for Dummy Classifier (stratified)\n[[ 5  3  4  2  2  6 10  1  3  7  2  4  3  3  5  2  2  2  2  7  1  1  2  2\n   2  1]\n [ 6  2  5  4  4  4  3  3  5  4  2  0  3  3  6  5  2  2  3  3  5  7  5  4\n   3  2]\n [ 4  3  5  4  1  5  5  1  7  3  4  6  4  5  3  4  4  6  3  5  5  2  4  2\n   1  4]\n [ 6  5  4 13  1  3  5  3  3  4  3  1  3  4  2  1  4  4  4  4  0  5  2  2\n   4  1]\n [ 1  3  2  1  5  2  5  3  5  1  3  5  4  0  1  3  0  3  2  4  1  0  5  3\n   4  3]\n [ 6  3  3  4  6  1  2  1  3  2  6  3  4  4  2  5  4  3  3  2  6  4  4  2\n   4  3]\n [ 3  6  1  2  4  1  2  3  3  3  2  4  3  4  3  7  1  2  3  6  4  4  4  3\n   3  2]\n [ 2  5  8  2  6  4  2  3  3  6  0  4  2  1  6  2  4  3  3  5  1  4  3  2\n   2  4]\n [ 4  1  3  2  2  5  3  5  6  5  4  5  5  2  4  3  3  4  5  1  4  1  4  1\n   3  2]\n [ 4  4  3  2  1  3  5  1  3  3  3  6  2  2  5  5  1  4  3  4  3  4  4  6\n   3  6]\n [ 2  1  2  5  2  1  4  4  2  5  4  4  4  5  4  5  4  3  3  5  1  1  1  2\n   4  4]\n [ 2  2  1  3  4  0  5  2  2  7  4  5  4  2  4  3  6  0  5  3  5  3  2  6\n   6  2]\n [ 3  1  3  3  6  4  3  3  5  5  1  6  5  0  2  5  5  4  6  5  5  4  2  1\n   6  4]\n [ 6  3  3  3  5  4  4  3  3  2  4  3  5  1  4  7  3  3  4 11  2  3  1  3\n   3  6]\n [ 2  2  3  1  3  2  2  2  1  7  4  5  4  4  2  4  2  4  4  5  2  2  1  2\n   3  7]\n [ 4  4  5  3  5  3  5  5  1  1  3  7  2  8  3  3  3  2  3  4  1  7  5  8\n   1  2]\n [ 5  5  4  5  2  3  5  3  4  0  1  7  3  2  6  4  1  0  2  1  1  4  5  4\n   4  3]\n [ 3  5  2  7  3  3  5  1  4  2 10  1  5  1  2  2  5  2  1  4  4  3  2  2\n   5  5]\n [ 4  2  4  2  6  4  8  4  5  5  4  3  3  1  3  5  3  2  2  4  5  5  5  3\n   5  3]\n [ 2  5  4  2  2  3  5  3  2  3  2  7  2  4  0  2  3  6  1  6  2  3  4  4\n   6  4]\n [ 5  5  7  6  3  8  3  2  3  7  2  4  2  5  7  3  2  1  1  2  0  3  9  1\n   2  1]\n [ 4  5  5  3  2  5  5  3  4  2  8  2  4  2  3  4  1  4  4  6  2  4  6  4\n   3  6]\n [ 3  1  2  7  4  8  3  1 10  0  5  4  7  3  3  4  4  7  1  3  2  2  1  6\n   1  5]\n [ 1  2 10  2  3  3  2  5  4  8  6  1  3  1  5  3  3  1  6  4  2  5  2  1\n   3  4]\n [ 4  4  3  5  4  3  1  3  4  7  8  2  0  4  3  3  6  1  5  2  6  3  3  1\n   3  2]\n [ 2  1  4  2  4  6  1  2  2  5  3  5  9  2  4  4  4  0  2  1  6  6  4  2\n   4  3]]\nAccuracy percentage of this classifier is 3.761 %\n\n",
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": "/home/nbuser/anaconda3_501/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n  'precision', 'predicted', average, warn_for)\n",
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": "Classification report for kNN\n              precision    recall  f1-score   support\n\n           A       0.92      0.95      0.94        84\n           B       0.65      0.87      0.74        95\n           C       0.90      0.95      0.92       100\n           D       0.65      0.81      0.73        91\n           E       0.86      0.71      0.78        69\n           F       0.90      0.94      0.92        90\n           G       0.77      0.95      0.85        83\n           H       1.00      0.99      0.99        87\n           I       0.99      0.99      0.99        87\n           J       0.87      0.97      0.92        90\n           K       0.90      0.87      0.88        82\n           L       0.93      0.98      0.96        88\n           M       0.85      0.79      0.82        97\n           N       0.88      0.83      0.85        99\n           O       0.98      1.00      0.99        80\n           P       0.87      0.82      0.84        98\n           Q       0.91      1.00      0.95        84\n           R       0.99      0.99      0.99        89\n           S       0.95      0.92      0.93       100\n           T       0.90      0.61      0.73        87\n           U       0.97      0.94      0.95        94\n           V       0.95      0.69      0.80       101\n           W       1.00      0.97      0.98        97\n           X       1.00      0.98      0.99        90\n           Y       1.00      1.00      1.00        90\n           Z       0.93      0.81      0.87        88\n\n   micro avg       0.90      0.90      0.90      2340\n   macro avg       0.90      0.90      0.90      2340\nweighted avg       0.90      0.90      0.90      2340\n\nConfusion Matrix for non-optimized kNN \n[[80  1  0  0  0  0  0  0  0  0  3  0  0  0  0  0  0  0  0  0  0  0  0  0\n   0  0]\n [ 1 83  0  6  4  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1  0  0\n   0  0]\n [ 0  0 95  0  0  0  2  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n   0  3]\n [ 1 10  0 74  3  0  2  0  0  0  0  0  0  0  0  1  0  0  0  0  0  0  0  0\n   0  0]\n [ 0 10  0  8 49  0  1  0  0  0  0  0  0  0  0  1  0  0  0  0  0  0  0  0\n   0  0]\n [ 0  0  0  0  0 85  0  0  0  0  0  0  0  0  0  0  0  0  5  0  0  0  0  0\n   0  0]\n [ 0  0  0  2  0  0 79  0  0  1  0  0  0  0  0  0  0  0  0  1  0  0  0  0\n   0  0]\n [ 1  0  0  0  0  0  0 86  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n   0  0]\n [ 0  0  0  0  0  0  0  0 86  0  0  0  0  0  0  0  0  1  0  0  0  0  0  0\n   0  0]\n [ 0  0  0  0  0  0  0  0  0 87  3  0  0  0  0  0  0  0  0  0  0  0  0  0\n   0  0]\n [ 0  0  0  0  0  0  0  0  0 11 71  0  0  0  0  0  0  0  0  0  0  0  0  0\n   0  0]\n [ 0  0  0  0  0  0  0  0  0  0  0 86  0  0  2  0  0  0  0  0  0  0  0  0\n   0  0]\n [ 1  0  0  0  0  0  0  0  0  0  0  6 77 11  0  0  1  0  0  0  1  0  0  0\n   0  0]\n [ 2  0  0  0  0  0  0  0  0  0  0  0 14 82  0  0  0  0  0  0  1  0  0  0\n   0  0]\n [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0 80  0  0  0  0  0  0  0  0  0\n   0  0]\n [ 0  7  0  3  1  0  4  0  0  0  0  0  0  0  0 80  0  0  0  2  0  1  0  0\n   0  0]\n [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 84  0  0  0  0  0  0  0\n   0  0]\n [ 0  0  0  0  0  0  0  0  1  0  0  0  0  0  0  0  0 88  0  0  0  0  0  0\n   0  0]\n [ 0  0  0  0  0  8  0  0  0  0  0  0  0  0  0  0  0  0 92  0  0  0  0  0\n   0  0]\n [ 0  1  0 10  0  0 14  0  0  0  1  0  0  0  0  8  0  0  0 53  0  0  0  0\n   0  0]\n [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  6  0  0  0 88  0  0  0\n   0  0]\n [ 0 15  0  8  0  0  1  0  0  0  1  0  0  0  0  2  0  0  0  2  0 70  0  0\n   0  2]\n [ 0  0  0  0  0  0  0  0  0  1  0  0  0  0  0  0  1  0  0  0  1  0 94  0\n   0  0]\n [ 1  0  0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 88\n   0  0]\n [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n  90  0]\n [ 0  1 11  2  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1  0  2  0  0\n   0 71]]\n\nAccuracy percentage of this classifier is 89.658 %\n\nClassification report for MLP on initial data\n              precision    recall  f1-score   support\n\n         1.0       0.98      0.99      0.98        84\n         2.0       0.90      0.93      0.91        95\n         3.0       0.96      0.95      0.95       100\n         4.0       0.94      0.92      0.93        91\n         5.0       0.93      0.93      0.93        69\n         6.0       0.98      0.98      0.98        90\n         7.0       0.89      0.94      0.91        83\n         8.0       1.00      0.99      0.99        87\n         9.0       0.98      1.00      0.99        87\n        10.0       0.97      0.98      0.97        90\n        11.0       0.98      0.98      0.98        82\n        12.0       1.00      0.94      0.97        88\n        13.0       0.92      0.94      0.93        97\n        14.0       0.95      0.94      0.94        99\n        15.0       0.98      0.99      0.98        80\n        16.0       0.92      0.92      0.92        98\n        17.0       1.00      1.00      1.00        84\n        18.0       0.99      0.98      0.98        89\n        19.0       0.98      0.99      0.99       100\n        20.0       0.94      0.87      0.90        87\n        21.0       1.00      1.00      1.00        94\n        22.0       0.88      0.87      0.88       101\n        23.0       1.00      1.00      1.00        97\n        24.0       0.99      0.99      0.99        90\n        25.0       0.99      1.00      0.99        90\n        26.0       0.91      0.92      0.92        88\n\n   micro avg       0.96      0.96      0.96      2340\n   macro avg       0.96      0.96      0.96      2340\nweighted avg       0.96      0.96      0.96      2340\n\nConfusion matrix for MLP on initial data\n[[83  0  0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n   0  0]\n [ 0 88  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  7  0  0\n   0  0]\n [ 0  0 95  0  0  0  2  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n   0  3]\n [ 0  0  0 84  3  0  1  0  0  0  0  0  0  0  0  1  0  0  0  0  0  2  0  0\n   0  0]\n [ 0  3  0  1 64  0  0  0  0  0  0  0  0  0  0  1  0  0  0  0  0  0  0  0\n   0  0]\n [ 0  0  0  0  0 88  0  0  0  0  0  0  0  0  0  0  0  0  2  0  0  0  0  0\n   0  0]\n [ 0  0  0  0  0  0 78  0  0  1  0  0  0  0  0  0  0  0  0  4  0  0  0  0\n   0  0]\n [ 1  0  0  0  0  0  0 86  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n   0  0]\n [ 0  0  0  0  0  0  0  0 87  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n   0  0]\n [ 0  0  0  0  0  0  0  0  0 88  2  0  0  0  0  0  0  0  0  0  0  0  0  0\n   0  0]\n [ 0  0  0  0  0  0  0  0  0  2 80  0  0  0  0  0  0  0  0  0  0  0  0  0\n   0  0]\n [ 0  0  0  0  0  0  0  0  0  0  0 83  3  0  2  0  0  0  0  0  0  0  0  0\n   0  0]\n [ 0  0  0  0  0  0  0  0  0  0  0  0 91  5  0  0  0  0  0  0  0  0  0  0\n   1  0]\n [ 0  0  0  0  0  0  0  0  0  0  0  0  5 93  0  0  0  0  0  0  0  0  0  1\n   0  0]\n [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0 79  0  0  1  0  0  0  0  0  0\n   0  0]\n [ 0  1  0  1  0  0  3  0  0  0  0  0  0  0  0 90  0  0  0  1  0  1  0  0\n   0  1]\n [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 84  0  0  0  0  0  0  0\n   0  0]\n [ 0  0  0  0  0  0  0  0  2  0  0  0  0  0  0  0  0 87  0  0  0  0  0  0\n   0  0]\n [ 0  0  0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0 99  0  0  0  0  0\n   0  0]\n [ 0  0  1  2  0  0  4  0  0  0  0  0  0  0  0  4  0  0  0 76  0  0  0  0\n   0  0]\n [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 94  0  0  0\n   0  0]\n [ 0  6  0  1  0  0  0  0  0  0  0  0  0  0  0  2  0  0  0  0  0 88  0  0\n   0  4]\n [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 97  0\n   0  0]\n [ 1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 89\n   0  0]\n [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n  90  0]\n [ 0  0  3  0  2  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  2  0  0\n   0 81]]\nAccuracy percentage of this classifier is 95.812 %\nClassification report for non_opt Gaussian Naive Bayes Classifier\n              precision    recall  f1-score   support\n\n         1.0       0.81      0.98      0.89        84\n         2.0       0.75      0.43      0.55        95\n         3.0       0.97      0.90      0.93       100\n         4.0       0.82      0.67      0.74        91\n         5.0       0.49      0.94      0.64        69\n         6.0       0.93      0.56      0.69        90\n         7.0       0.88      0.80      0.84        83\n         8.0       0.97      0.99      0.98        87\n         9.0       0.92      0.95      0.94        87\n        10.0       0.90      0.91      0.91        90\n        11.0       0.88      0.89      0.88        82\n        12.0       0.84      0.97      0.90        88\n        13.0       0.82      0.80      0.81        97\n        14.0       0.82      0.74      0.78        99\n        15.0       0.98      0.80      0.88        80\n        16.0       0.92      0.62      0.74        98\n        17.0       0.92      0.99      0.95        84\n        18.0       0.96      0.90      0.93        89\n        19.0       0.76      0.96      0.85       100\n        20.0       0.68      0.82      0.74        87\n        21.0       0.93      0.95      0.94        94\n        22.0       0.85      0.77      0.81       101\n        23.0       0.83      0.99      0.90        97\n        24.0       0.90      0.98      0.94        90\n        25.0       0.99      0.98      0.98        90\n        26.0       0.86      0.90      0.88        88\n\n   micro avg       0.85      0.85      0.85      2340\n   macro avg       0.86      0.85      0.85      2340\nweighted avg       0.86      0.85      0.85      2340\n\nConfusion Matrix for Gaussian Naive Bayes Classifier\n[[82  0  0  0  0  0  0  1  0  0  0  0  0  1  0  0  0  0  0  0  0  0  0  0\n   0  0]\n [ 4 41  0  4 42  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  3  0  0\n   0  0]\n [ 0  0 90  0  0  0  2  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n   0  8]\n [ 1  4  0 61 16  0  1  0  0  0  0  0  0  0  0  1  0  0  0  2  2  3  0  0\n   0  0]\n [ 1  2  0  1 65  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n   0  0]\n [ 0  0  0  0  0 50  0  0  0  0  0  0  0  1  0  0  0  0 29  0  0  0  0 10\n   0  0]\n [ 0  0  0  0  0  0 66  0  0  2  0  0  0  0  0  0  1  0  0 13  0  0  0  0\n   0  1]\n [ 0  0  0  0  0  0  0 86  0  0  0  0  0  0  0  0  0  0  1  0  0  0  0  0\n   0  0]\n [ 0  0  0  0  0  0  0  0 83  0  0  0  0  1  0  0  0  2  0  0  0  0  1  0\n   0  0]\n [ 0  0  0  0  0  0  0  0  0 82  7  0  0  0  0  0  0  0  0  0  0  0  1  0\n   0  0]\n [ 1  0  0  0  0  0  0  0  0  5 73  0  0  0  0  2  0  0  0  1  0  0  0  0\n   0  0]\n [ 0  0  0  0  0  0  0  0  0  0  0 85  0  0  1  0  0  0  0  0  0  0  1  0\n   1  0]\n [ 0  0  0  0  0  0  0  0  0  0  0  3 78 12  0  0  0  0  0  0  2  0  2  0\n   0  0]\n [ 7  0  0  0  0  0  0  0  0  0  0  0 17 73  0  0  0  0  0  0  1  0  1  0\n   0  0]\n [ 0  0  0  0  0  0  0  0  0  0  0 13  0  0 64  0  0  1  0  0  0  0  2  0\n   0  0]\n [ 3  1  0  1 10  0  0  0  0  0  1  0  0  0  0 61  0  0  0 15  0  3  3  0\n   0  0]\n [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 83  0  0  0  0  0  1  0\n   0  0]\n [ 0  0  0  0  0  0  0  0  7  0  0  0  0  0  0  0  0 80  0  0  0  0  2  0\n   0  0]\n [ 0  0  0  0  0  4  0  0  0  0  0  0  0  0  0  0  0  0 96  0  0  0  0  0\n   0  0]\n [ 1  0  0  2  0  0  5  0  0  0  1  0  0  0  0  2  2  0  0 71  0  2  1  0\n   0  0]\n [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  4  0  0  0 89  0  1  0\n   0  0]\n [ 0  7  0  5  0  0  1  0  0  1  1  0  0  0  0  0  0  0  0  2  1 78  1  0\n   0  4]\n [ 0  0  0  0  0  0  0  0  0  0  0  0  0  1  0  0  0  0  0  0  0  0 96  0\n   0  0]\n [ 1  0  0  0  0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 88\n   0  0]\n [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  2  0\n  88  0]\n [ 0  0  3  0  0  0  0  0  0  1  0  0  0  0  0  0  0  0  0  0  1  3  1  0\n   0 79]]\n\nAccuracy percentage of this classifier is 84.957 %\n",
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": "<Figure size 432x288 with 1 Axes>"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": "<Figure size 432x288 with 1 Axes>"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": "<Figure size 432x288 with 1 Axes>"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Ακρίβεια -Precision- ($P$) είναι ο λόγος των true positives ($T_p$) ως προς τον αριθμό των true positives συν τον αριθμό των false positives ($F_p$). $$P = \\frac{T_p}{T_p+F_p}$$ Ανάκληση -Recall- ($R$) είναι ο λόγος των true positives ($T_p$) ως προς τον αριθμό των true positives συν τον αριθμό των false negatives ($F_n$). $$R = \\frac{T_p}{T_p + F_n}$$ Συχνά χρησιμοποιούμε και το ($F_1$) score, το οποίο είναι ο αρμονικός μέσος της ακρίβειας και της ανάκλησης. $$F1 = 2\\frac{P \\times R}{P+R}$$ Ιδανικά θέλουμε και υψηλή ακρίβεια και υψηλή ανάκληση, ωστόσο μεταξύ της ακρίβειας και της ανάκλησης υπάρχει γενικά trade-off. Στην οριακή περίπτωση του ταξινομητή που επιστρέφει σταθερά μόνο τη θετική κλάση, η ανάκληση θα είναι 1 ($F_n=0$) αλλά η ακρίβεια θα έχει τη μικρότερη δυνατή τιμή της. Γενικά, κατεβάζοντας το κατώφλι της απόφασης του ταξινομητή, αυξάνουμε την ανάκληση και μειώνουμε την ακρίβεια και αντιστρόφως.\n\nΣτην πράξη και ειδικά σε μη ισορροπημένα datasets χρησιμοποιούμε την ακρίβεια, ανάκληση και το F1 πιο συχνά από την πιστότητα."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Βλέπουμε ότι η MLP υπερτερεί των άλλων και μάλιστα βρίσκεται σε πολύ καλό σημείο από πλευράς accuracy,γεγονός αναμενόμενο λόγω και της πολυπλοκότητας του συγκεκριμένου ταξινομητή ως προς τη δομή του. Οι dummies από την άλλη πλευρά είναι αναμενόμενο ότι θα κινούνταν σε ιδιαίτερα χαμηλά επίπεδα δεδομένου του πλήθους των διαφορετικών κλάσεων του dataset. "
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Δ. Βελτιστοποίηση ταξινομητών"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Θα εξετάσουμε μήπως μπορούμε να μειώσουμε τις διαστάσεις του dataset μέσω της επιλογής χαραχτηριστικών που έχουν μηδενικό είτε απειροελάχιστο variance οπότε δεν παίζουν κάποιο ρόλο στην απόφαση για το ποια θα είναι η πρόβλεψή μας."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from sklearn.feature_selection import VarianceThreshold\n# αρχικοποιούμε έναν selector\nselector = VarianceThreshold()\n# όπως κάναμε και με τους ταξινομητές τον κάνουμε fit στα δεδομένα εκπαίδευσης\ntrain_reduced = selector.fit_transform(train)\nmask = selector.get_support()\n#print mask\n\nprint ('Το παλιό size του train dataset ήταν',train.shape,'. Μετά τη μείωση της διαστατικότητας αυτό είναι',train_reduced.shape)\n\n# εφαρμόζουμε τις αντίστοιχες αλλαγές και στο test set\ntest_reduced = test[:,mask]\nprint ('Το παλιό size του test dataset ήταν',test.shape,'. Μετά τη μείωση της διαστατικότητας αυτό είναι',test_reduced.shape)\n\n",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Το παλιό size του train dataset ήταν (5457, 617) . Μετά τη μείωση της διαστατικότητας αυτό είναι (5457, 617)\nΤο παλιό size του test dataset ήταν (2340, 617) . Μετά τη μείωση της διαστατικότητας αυτό είναι (2340, 617)\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Διαπιστώνουμε ότι δεν πετυχαίνει η μείωση της διαστατικότητας μέσω αυτής της μεθόδου οπότε θα στηριχτούμε στην PCA που γίνεται κατά τη βελτιστοποίηση των ταξινομητών."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Για όλους του Dummy Classifiers δεν έχει νόημα να βρούμε βέλτιστες υπερπαραμέτρους για το pipeline, οπότε κάνουμε χρήση του pipeline με τις default παραμέτρους (δεν θα βρούμε βελτιστες με searchgridcv)."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from sklearn import preprocessing\nfrom sklearn.dummy import DummyClassifier\nfrom imblearn.pipeline import Pipeline\nfrom sklearn.decomposition import PCA\nfrom imblearn.over_sampling import RandomOverSampler\nfrom sklearn.preprocessing import MultiLabelBinarizer\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import precision_recall_fscore_support\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\n\nimport time\n\n# αρχικοποιούμε τους εκτιμητές (μετασχηματιστές και ταξινομητή) χωρείς παραμέτρους\nselector = VarianceThreshold()\nscaler = preprocessing.StandardScaler()\nros = RandomOverSampler()\npca = PCA()\n\ndc_uni = DummyClassifier(strategy=\"uniform\")\npipe = Pipeline(steps=[('selector', selector), ('scaler', scaler), ('sampler', ros), ('pca', pca), ('dummy', dc_uni)])\n\nstart_time = time.time()\npipe.fit(train_dummy,train_labels_dummy)\nprint(\"Για τον Dummy Classifier(uniform) : %s seconds\" % (time.time() - start_time))\n\npreds = pipe.predict(test_dummy)\n\n#################\nprint ('Classification report for Dummy Classifier (uniform)')\ncr_dummy_uni = classification_report(test_labels, preds)\nprint (cr_dummy_uni)\n\nscores_micro = {}\nscores_macro = {}\n\nscores_micro['Dummy-Uniform']=precision_recall_fscore_support(test_labels,preds,average='micro')\nscores_macro['Dummy-Uniform']=precision_recall_fscore_support(test_labels,preds,average='macro')\n\nprint ('Confusion Matrix for Dummy Classifier (uniform)')\nprint (confusion_matrix(test_labels, preds))\n\nacc_dummy_uni = 100*accuracy_score(test_labels,preds)\nprint ('\\nAccuracy percentage of this classifier is %.3f %%' % (acc_dummy_uni))",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Για τον Dummy Classifier(uniform) : 1.8156580924987793 seconds\nClassification report for Dummy Classifier (uniform)\n              precision    recall  f1-score   support\n\n         1.0       0.04      0.05      0.05        84\n         2.0       0.04      0.04      0.04        95\n         3.0       0.05      0.04      0.04       100\n         4.0       0.07      0.08      0.07        91\n         5.0       0.03      0.04      0.04        69\n         6.0       0.03      0.03      0.03        90\n         7.0       0.04      0.05      0.05        83\n         8.0       0.05      0.06      0.05        87\n         9.0       0.08      0.09      0.08        87\n        10.0       0.07      0.08      0.07        90\n        11.0       0.05      0.05      0.05        82\n        12.0       0.06      0.06      0.06        88\n        13.0       0.04      0.04      0.04        97\n        14.0       0.06      0.06      0.06        99\n        15.0       0.03      0.03      0.03        80\n        16.0       0.01      0.01      0.01        98\n        17.0       0.05      0.06      0.05        84\n        18.0       0.01      0.01      0.01        89\n        19.0       0.04      0.04      0.04       100\n        20.0       0.02      0.02      0.02        87\n        21.0       0.04      0.03      0.03        94\n        22.0       0.05      0.04      0.04       101\n        23.0       0.02      0.02      0.02        97\n        24.0       0.02      0.02      0.02        90\n        25.0       0.04      0.03      0.03        90\n        26.0       0.03      0.02      0.02        88\n\n   micro avg       0.04      0.04      0.04      2340\n   macro avg       0.04      0.04      0.04      2340\nweighted avg       0.04      0.04      0.04      2340\n\nConfusion Matrix for Dummy Classifier (uniform)\n[[ 4  4  3  3  3  3  3  2  1  2  1  5  6  3  2  2  4  3  5  3  3  4  4  5\n   3  3]\n [ 4  4  4  3  2  0  5  2  5  2  5  6  1  4  4  3  2  7  3  4  6  6  6  4\n   1  2]\n [ 3  6  4  7  9  7  2  3  6  8  2  6  3  3  0  2  3  4  2  1  3  3  4  1\n   3  5]\n [ 8  4  3  7  3  4  2  4  1  2  4  3  4  3  2  4  5  2  3  6  3  1  2  3\n   3  5]\n [ 5  4  0  4  3  0  1  3  5  3  1  0  1  4  1  6  3  2  7  3  1  5  1  2\n   4  0]\n [ 2  3  4  2  6  3  3  3  1  6  6  1  2  5  2  2  3  5  3  4  4  3  6  7\n   4  0]\n [ 1  4  6  6  2  5  4  5  2  1  1  5  3  5  2  5  4  1  3  3  3  1  5  2\n   2  2]\n [ 3  4  2  4  6  2  5  5  6  4  2  4  5  4  2  0  5  1  4  5  0  0  4  2\n   6  2]\n [ 1  3  2  6  2  1  1  3  8  2  2  2  5  5  5  2  4  3  5  2  5  1  2  5\n   6  4]\n [ 4  3  4  4  4  1  5  1  3  7  4  6  3  6  4  3  2  4  2  3  2  3  1  7\n   1  3]\n [ 3  5  3  2  0  3  4  6  5  4  4  2  4  5  2  1  6  6  2  2  0  4  1  4\n   2  2]\n [ 2  3  4  0  5  2  4  1  6  6  5  5  1  4  3  0  5  4  2  3  0  4  5  6\n   3  5]\n [ 3  4  7  0  4  4  5  5  4  4  4  3  4  5  3  4  8  5  3  3  2  4  1  3\n   0  5]\n [ 1  3  0  3  1  2  5  4  7  5  5  2  1  6  1  3  8  3  5  8  3  5  8  4\n   2  4]\n [ 2  5  5  3  6  3  2  2  2  3  2  3  5  2  2  1  5  6  2  4  4  2  2  3\n   2  2]\n [ 1  4  5  4  3  5  5  9  5  2  3  4  3  2  5  1  5  3  7  5  3  0  2  5\n   2  5]\n [ 2  2  5  8  5  3  3  1  5  4  4  2  8  6  2  2  5  1  3  6  3  0  0  0\n   2  2]\n [ 2  6  1  5  2  2  5  5  2  3  3  2  5  3  1  6  1  1  2  2  4  5 11  5\n   2  3]\n [ 4  4  2  7  1  5  2  6  3  6  1  3  2  4  5  4  3  1  4  5  3 11  3  5\n   3  3]\n [ 8  7  3  0  0  4  5  2  3  3  1  1  4  2  7  5  4  1  8  2  3  1  2  1\n   6  4]\n [ 8  8  5  1  2  5  7  6  2  3  2  1  3  4  1  4  4  1  1  5  3  2  3  3\n   7  3]\n [ 4  8  3  4  6  2  5  1  2  4  1  5  4  5  2  2  4  3  4  3  8  4  2  4\n   7  4]\n [ 2  3  6  5  2  5  2  4  6  8  3  1  6  2  3  4  2  5  7  4  3  4  2  3\n   3  2]\n [ 4  2  1  2  5  5  2  4  6  6  3  3  0  4  8  3  6  4  5  2  5  2  2  2\n   2  2]\n [ 3  4  3  5  3  6  2  6  4  2  6  5  6  1  4  6  4  3  1  3  2  1  3  2\n   3  2]\n [ 5  1  2  7  2  5  2  2  2  4  8  4  1  2  0  4  4  2  4  2  7  5  1  7\n   3  2]]\n\nAccuracy percentage of this classifier is 4.231 %\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "dc_freq = DummyClassifier(strategy=\"most_frequent\")\npipe = Pipeline(steps=[('selector', selector), ('scaler', scaler), ('sampler', ros), ('pca', pca), ('dummy', dc_freq)])\n\nstart_time = time.time()\npipe.fit(train_dummy,train_labels_dummy)\nprint(\"Για τον Dummy Classifier(Most Frequent) : %s seconds\" % (time.time() - start_time))\n\npreds = pipe.predict(test_dummy)\n\n#################\nprint ('Classification report for Dummy Classifier (most frequent)')\ncr_dummy_const_p1 = classification_report(test_labels, preds)\nprint (cr_dummy_const_p1)\n\nscores_micro['Dummy-Most_Freq']=precision_recall_fscore_support(test_labels,preds,average='micro')\nscores_macro['Dummy-Most_Freq']=precision_recall_fscore_support(test_labels,preds,average='macro')\n\nprint ('Confusion Matrix for Dummy Classifier (most frequent)')\nprint (confusion_matrix(test_labels, preds))\n\nacc_dummy_freq = 100*accuracy_score(test_labels,preds)\nprint ('\\nAccuracy percentage of this classifier is %.3f %%' % (acc_dummy_freq))",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Για τον Dummy Classifier(Most Frequent) : 1.63983154296875 seconds\nClassification report for Dummy Classifier (most frequent)\n              precision    recall  f1-score   support\n\n         1.0       0.04      1.00      0.07        84\n         2.0       0.00      0.00      0.00        95\n         3.0       0.00      0.00      0.00       100\n         4.0       0.00      0.00      0.00        91\n         5.0       0.00      0.00      0.00        69\n         6.0       0.00      0.00      0.00        90\n         7.0       0.00      0.00      0.00        83\n         8.0       0.00      0.00      0.00        87\n         9.0       0.00      0.00      0.00        87\n        10.0       0.00      0.00      0.00        90\n        11.0       0.00      0.00      0.00        82\n        12.0       0.00      0.00      0.00        88\n        13.0       0.00      0.00      0.00        97\n        14.0       0.00      0.00      0.00        99\n        15.0       0.00      0.00      0.00        80\n        16.0       0.00      0.00      0.00        98\n        17.0       0.00      0.00      0.00        84\n        18.0       0.00      0.00      0.00        89\n        19.0       0.00      0.00      0.00       100\n        20.0       0.00      0.00      0.00        87\n        21.0       0.00      0.00      0.00        94\n        22.0       0.00      0.00      0.00       101\n        23.0       0.00      0.00      0.00        97\n        24.0       0.00      0.00      0.00        90\n        25.0       0.00      0.00      0.00        90\n        26.0       0.00      0.00      0.00        88\n\n   micro avg       0.04      0.04      0.04      2340\n   macro avg       0.00      0.04      0.00      2340\nweighted avg       0.00      0.04      0.00      2340\n\nConfusion Matrix for Dummy Classifier (most frequent)\n[[ 84   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n    0   0   0   0   0   0   0   0]\n [ 95   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n    0   0   0   0   0   0   0   0]\n [100   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n    0   0   0   0   0   0   0   0]\n [ 91   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n    0   0   0   0   0   0   0   0]\n [ 69   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n    0   0   0   0   0   0   0   0]\n [ 90   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n    0   0   0   0   0   0   0   0]\n [ 83   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n    0   0   0   0   0   0   0   0]\n [ 87   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n    0   0   0   0   0   0   0   0]\n [ 87   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n    0   0   0   0   0   0   0   0]\n [ 90   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n    0   0   0   0   0   0   0   0]\n [ 82   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n    0   0   0   0   0   0   0   0]\n [ 88   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n    0   0   0   0   0   0   0   0]\n [ 97   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n    0   0   0   0   0   0   0   0]\n [ 99   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n    0   0   0   0   0   0   0   0]\n [ 80   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n    0   0   0   0   0   0   0   0]\n [ 98   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n    0   0   0   0   0   0   0   0]\n [ 84   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n    0   0   0   0   0   0   0   0]\n [ 89   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n    0   0   0   0   0   0   0   0]\n [100   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n    0   0   0   0   0   0   0   0]\n [ 87   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n    0   0   0   0   0   0   0   0]\n [ 94   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n    0   0   0   0   0   0   0   0]\n [101   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n    0   0   0   0   0   0   0   0]\n [ 97   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n    0   0   0   0   0   0   0   0]\n [ 90   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n    0   0   0   0   0   0   0   0]\n [ 90   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n    0   0   0   0   0   0   0   0]\n [ 88   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n    0   0   0   0   0   0   0   0]]\n\nAccuracy percentage of this classifier is 3.590 %\n",
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": "/home/nbuser/anaconda3_501/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n  'precision', 'predicted', average, warn_for)\n",
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "dc_strat = DummyClassifier(strategy=\"stratified\")\npipe = Pipeline(steps=[('selector', selector), ('scaler', scaler), ('sampler', ros), ('pca', pca), ('dummy', dc_strat)])\n\nstart_time = time.time()\npipe.fit(train_dummy,train_labels_dummy)\nprint(\"Για τον Dummy Classifier(Stratified) : %s seconds\" % (time.time() - start_time))\n\npreds = pipe.predict(test_dummy)\n\n#################\nprint ('Classification report for Dummy Classifier (stratified)')\ncr_dummy_strat = classification_report(test_labels, preds)\nprint (cr_dummy_strat)\n\nscores_micro['Dummy-Most_Freq']=precision_recall_fscore_support(test_labels,preds,average='micro')\nscores_macro['Dummy-Most_Freq']=precision_recall_fscore_support(test_labels,preds,average='macro')\n\nprint ('Confusion Matrix for Dummy Classifier (stratified)')\nprint (confusion_matrix(test_labels, preds))\n\nacc_dummy_strat = 100*accuracy_score(test_labels,preds)\nprint ('\\nAccuracy percentage of this classifier is %.3f %%' % (acc_dummy_strat))",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Για τον Dummy Classifier(Stratified) : 1.9720463752746582 seconds\nClassification report for Dummy Classifier (stratified)\n              precision    recall  f1-score   support\n\n         1.0       0.01      0.01      0.01        84\n         2.0       0.04      0.04      0.04        95\n         3.0       0.03      0.03      0.03       100\n         4.0       0.03      0.03      0.03        91\n         5.0       0.02      0.03      0.02        69\n         6.0       0.01      0.01      0.01        90\n         7.0       0.02      0.02      0.02        83\n         8.0       0.01      0.01      0.01        87\n         9.0       0.04      0.05      0.04        87\n        10.0       0.01      0.01      0.01        90\n        11.0       0.04      0.05      0.05        82\n        12.0       0.04      0.05      0.04        88\n        13.0       0.08      0.07      0.07        97\n        14.0       0.08      0.08      0.08        99\n        15.0       0.03      0.04      0.03        80\n        16.0       0.04      0.04      0.04        98\n        17.0       0.05      0.05      0.05        84\n        18.0       0.06      0.06      0.06        89\n        19.0       0.02      0.02      0.02       100\n        20.0       0.04      0.05      0.04        87\n        21.0       0.06      0.05      0.06        94\n        22.0       0.10      0.08      0.09       101\n        23.0       0.04      0.03      0.03        97\n        24.0       0.06      0.07      0.06        90\n        25.0       0.00      0.00      0.00        90\n        26.0       0.02      0.02      0.02        88\n\n   micro avg       0.04      0.04      0.04      2340\n   macro avg       0.04      0.04      0.04      2340\nweighted avg       0.04      0.04      0.04      2340\n\nConfusion Matrix for Dummy Classifier (stratified)\n[[ 1  2  2  3  1  2  8  2  3  3  5  1  3  3  7  4  3  3  1  7  5  3  2  3\n   4  3]\n [ 2  4  5  2  8  3  2  4  5  1  2  4  3  2  5  2  5  4  3  5  3  5  6  3\n   3  4]\n [ 5  7  3  2  3  2  4  1  9  1  4  8  1  3  3  3  6  2  4  2  3  3  7  5\n   4  5]\n [ 2  2  3  3  1  3  2  4 13  1  7  4  1  2  4  3  2  3  8  7  2  2  3  5\n   1  3]\n [ 3  4  2  2  2  1  4  3  5  3  2  2  3  3  3  3  1  2  2  4  6  0  1  2\n   4  2]\n [ 4  5  5  3  2  1  3  3  6  6  2  3  4  4  1  7  4  4  4  1  1  0  6  5\n   3  3]\n [ 4  3  2  2  3  1  2  4  3  2  4  4  4  4  1  5  2  4  1  2  2  5  9  3\n   2  5]\n [ 2  3  5  5  5  4  0  1  2  4  8  4  1  4  5  4  5  3  3  3  3  3  1  2\n   2  5]\n [ 1  4  2  5  0  8  3  3  4  4  2  3  1  7  6  4  1  5  2  2  2  1  0  6\n   2  9]\n [ 3  2  3  2  2  3  3  4  4  1  5  3  3  5  3  1  4  4  6  5  4  3  4  4\n   6  3]\n [ 3  5  4  1  5  2  4  0  5  1  4  1  4  5  4  2  2  3  4  5  3  4  2  4\n   3  2]\n [ 5  3  4  4  6  4  2  4  4  2  4  4  2  3  2  4  2  5  2  3  2  4  5  3\n   3  2]\n [ 1  5  3  2  5  6  4  7  4  5  3  4  7  6  6  5  3  3  1  4  2  1  4  2\n   2  2]\n [ 3  4  5  4  3  5  4  3  3  5  5  3  4  8  5  3  2  2  6  2  2  6  3  3\n   2  4]\n [ 0  3  4  4  5  3  5  5  1  4  6  1  2  2  3  2  2  3  2  3  2  4  3  4\n   5  2]\n [ 3  4  3  2  6  4  6  5  2  2  3  4  1  4  5  4  5  4  1  5  4  2  5  2\n   7  5]\n [ 2  2  6  5  4  3  4  6  0  6  3  3  2  3  0  4  4  0  2  5  3  7  1  5\n   3  1]\n [ 2  4  3  3  0  1  4  4  4  5  3  2  5  2  7  5  5  5  1  3  2  2  2  4\n   2  9]\n [ 4  1  2  4  3  0  5  4  2  4  5  3  7  2  5  3  5  6  2  6  5  4  1  7\n   4  6]\n [ 3  6  3  5  4  5  2  1  5  5  2  5  2  3  2  6  3  3  0  4  3  1  2  3\n   5  4]\n [ 4  6  2  3  2  3  2  4  1  3  3  5  5  2  2  4  7  7  6  4  5  4  0  5\n   4  1]\n [ 1  4  0  6  8  8  4  6  2  4  3  0  7  4  2  2  2  2  4  4  4  8  2  3\n   4  7]\n [ 2  2  3  4  5  2  3  0  5  4  2  3  9  5  4  3  2  3  8  4  4  3  3  3\n   4  7]\n [ 9  1  3  3  6  3  4  5  2  2  1  4  3  4  4  3  5  0  3  3  3  3  1  6\n   2  7]\n [ 3  3  5  8  3  4  1  5  3  5  2  7  6  5  4  4  2  3  5  1  2  0  0  6\n   0  3]\n [ 5  2  5  3  2  2  4  4  4  2  1  5  2  2  7  4  2  4  5  3  2  2  5  6\n   3  2]]\n\nAccuracy percentage of this classifier is 3.889 %\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Ουσιαστικά καμία απολύτως βελτίωση για τους dummy classifiers, ο αριθμός των κλάσεων δεν τους επέτρεξε να λειτουργήσουν."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "markdown",
      "source": "Όπως στους dummy, ετσι και στον Naive Bayes Classifier θα δοκιμάσουμε τις default τιμές για το pipeline."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from sklearn.naive_bayes import GaussianNB\ngnb = GaussianNB()\n# κάνουμε εκπαίδευση (fit) δηλαδή ουσιαστικά υπολογίζουμε μέση τιμή και διακύμανση για όλα τα χαρακτηριστικά και κλάσεις στο training set\npipe = Pipeline(steps=[('selector', selector), ('scaler', scaler), ('sampler', ros), ('pca', pca), ('gnb', gnb)])\n\nstart_time = time.time()\npipe.fit(train_dummy,train_labels_dummy)\nprint(\"Για τον Gaussian Naive Bayes: %s seconds\" % (time.time() - start_time))\n\npreds = pipe.predict(test)\n\n#################\nprint ('Classification report for Gaussian Naive Bayes Classifier')\ncr_gnb = classification_report(test_labels, preds)\nprint (cr_gnb)\n\nscores_micro['GNB']=precision_recall_fscore_support(test_labels,preds,average='micro')\nscores_macro['GNB']=precision_recall_fscore_support(test_labels,preds,average='macro')\n\nprint ('Confusion Matrix for Gaussian Naive Bayes Classifier')\nprint (confusion_matrix(test_labels, preds))\n\nacc_gnb = 100*accuracy_score(test_labels,preds)\nprint ('\\nAccuracy percentage of this classifier is %.3f %%' % (acc_gnb))",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Για τον Gaussian Naive Bayes: 1.9278037548065186 seconds\nClassification report for Gaussian Naive Bayes Classifier\n              precision    recall  f1-score   support\n\n         1.0       0.84      0.75      0.79        84\n         2.0       0.76      0.75      0.76        95\n         3.0       0.95      0.82      0.88       100\n         4.0       0.96      0.73      0.83        91\n         5.0       0.84      0.67      0.74        69\n         6.0       0.90      0.90      0.90        90\n         7.0       0.87      0.86      0.86        83\n         8.0       0.93      0.94      0.94        87\n         9.0       0.95      0.84      0.89        87\n        10.0       0.99      0.81      0.89        90\n        11.0       0.91      0.78      0.84        82\n        12.0       0.97      0.84      0.90        88\n        13.0       0.65      0.75      0.70        97\n        14.0       0.80      0.66      0.72        99\n        15.0       0.95      0.89      0.92        80\n        16.0       0.68      0.72      0.70        98\n        17.0       0.71      0.88      0.79        84\n        18.0       0.99      0.85      0.92        89\n        19.0       0.98      0.81      0.89       100\n        20.0       0.82      0.71      0.76        87\n        21.0       0.95      0.80      0.87        94\n        22.0       0.78      0.71      0.75       101\n        23.0       0.37      0.99      0.53        97\n        24.0       1.00      0.96      0.98        90\n        25.0       0.93      0.92      0.93        90\n        26.0       0.87      0.82      0.84        88\n\n   micro avg       0.81      0.81      0.81      2340\n   macro avg       0.86      0.81      0.83      2340\nweighted avg       0.86      0.81      0.82      2340\n\nConfusion Matrix for Gaussian Naive Bayes Classifier\n[[63  2  0  0  0  0  0  1  0  0  1  0  6  5  0  2  0  0  0  0  0  3  1  0\n   0  0]\n [ 0 71  0  1  4  0  0  0  0  0  0  0  0  0  0  4  1  0  0  0  0  8  6  0\n   0  0]\n [ 0  0 82  0  0  0  2  0  0  0  0  0  0  0  0  1  2  0  0  0  0  0 11  0\n   0  2]\n [ 0  3  0 66  3  0  0  1  0  0  0  0  1  0  0  3  2  0  0  2  0  4  6  0\n   0  0]\n [ 3  5  0  0 46  0  0  0  0  0  0  0  0  1  0  8  0  0  0  0  3  2  1  0\n   0  0]\n [ 0  0  0  0  0 81  0  0  0  0  0  0  1  0  0  0  0  0  2  0  0  0  6  0\n   0  0]\n [ 0  0  0  0  0  0 71  0  0  1  0  0  0  0  0  4  0  0  0  4  0  0  1  0\n   0  2]\n [ 1  0  0  0  0  0  0 82  0  0  0  0  0  0  0  0  0  0  0  0  0  0  4  0\n   0  0]\n [ 0  0  0  0  0  0  0  0 73  0  0  0  1  1  1  0  0  1  0  0  0  0  7  0\n   3  0]\n [ 0  0  0  0  0  0  2  0  0 73  0  0  0  2  0  0  8  0  0  1  0  0  4  0\n   0  0]\n [ 2  0  0  0  0  1  0  0  0  0 64  0  0  2  0  3  3  0  0  3  1  0  3  0\n   0  0]\n [ 1  0  0  0  0  0  0  0  0  0  0 74  6  0  1  0  0  0  0  0  0  0  6  0\n   0  0]\n [ 1  0  0  0  0  1  0  0  0  0  0  0 73  5  0  0  0  0  0  0  0  0 17  0\n   0  0]\n [ 0  0  0  0  0  1  0  0  0  0  0  0 21 65  1  0  0  0  0  0  0  0 11  0\n   0  0]\n [ 1  0  0  0  0  1  0  0  1  0  0  1  0  0 71  0  0  0  0  0  0  0  5  0\n   0  0]\n [ 1  6  0  2  1  0  2  1  0  0  2  0  0  0  0 71  1  0  0  3  0  0  8  0\n   0  0]\n [ 1  0  0  0  0  0  1  0  0  0  0  1  0  0  0  0 74  0  0  0  0  0  7  0\n   0  0]\n [ 0  0  0  0  0  1  0  0  3  0  0  0  1  0  1  0  0 76  0  0  0  0  4  0\n   3  0]\n [ 0  0  0  0  0  4  0  1  0  0  0  0  1  0  0  0  0  0 81  0  0  0 13  0\n   0  0]\n [ 0  0  0  0  1  0  4  1  0  0  3  0  0  0  0  5  3  0  0 62  0  0  8  0\n   0  0]\n [ 0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0 10  0  0  0 75  0  8  0\n   0  0]\n [ 0  5  0  0  0  0  0  0  0  0  0  0  0  0  0  4  0  0  0  1  0 72 12  0\n   0  7]\n [ 0  0  0  0  0  0  0  0  0  0  0  0  1  0  0  0  0  0  0  0  0  0 96  0\n   0  0]\n [ 1  0  0  0  0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  2 86\n   0  0]\n [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  7  0\n  83  0]\n [ 0  0  4  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  3  9  0\n   0 72]]\n\nAccuracy percentage of this classifier is 81.325 %\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Χαμηλά αποτελέσματα και για τον gnbc. Συνεχίζουμε με τον kNN τον οποίο θα επιχειρήσουμε να βελτιστοποιήσουμε μέσω gridsearch."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from sklearn import neighbors\n\nclf = neighbors.KNeighborsClassifier(n_jobs=-1) # η παράμετρος n_jobs = 1 χρησιμοποιεί όλους τους πυρήνες του υπολογιστή\npipe = Pipeline(steps=[('selector', selector), ('scaler', scaler), ('sampler', ros), ('pca', pca), ('kNN', clf)])\n\nstart_time = time.time()\npipe.fit(train, train_labels)\nprint(\"Για τον non-optimized kNN : %s seconds\" % (time.time() - start_time))\n\npreds = pipe.predict(test)\n\n\n#################\nprint ('Classification report for non-optimized kNN')\ncr_knn_no = classification_report(test_labels, preds)\nprint (cr_knn_no)\n\nscores_micro['kNN-non-opt']=precision_recall_fscore_support(test_labels,preds,average='micro')\nscores_macro['kNN-non-opt']=precision_recall_fscore_support(test_labels,preds,average='macro')\n\nprint ('Confusion Matrix for non-optimized kNN')\nprint (confusion_matrix(test_labels, preds))\n\nacc_knn_no = 100*accuracy_score(test_labels,preds)\nprint ('\\nAccuracy percentage of this classifier is %.3f %%' % (acc_knn_no))",
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Για τον non-optimized kNN : 1.9779775142669678 seconds\nClassification report for non-optimized kNN\n              precision    recall  f1-score   support\n\n         1.0       0.89      0.90      0.90        84\n         2.0       0.60      0.82      0.69        95\n         3.0       0.88      0.95      0.91       100\n         4.0       0.55      0.71      0.62        91\n         5.0       0.92      0.65      0.76        69\n         6.0       0.86      0.92      0.89        90\n         7.0       0.76      0.88      0.82        83\n         8.0       1.00      0.99      0.99        87\n         9.0       1.00      0.97      0.98        87\n        10.0       0.87      0.97      0.92        90\n        11.0       0.88      0.87      0.87        82\n        12.0       0.91      0.94      0.93        88\n        13.0       0.74      0.75      0.75        97\n        14.0       0.82      0.74      0.78        99\n        15.0       0.93      0.97      0.95        80\n        16.0       0.69      0.70      0.70        98\n        17.0       0.89      1.00      0.94        84\n        18.0       0.97      1.00      0.98        89\n        19.0       0.92      0.87      0.89       100\n        20.0       0.75      0.54      0.63        87\n        21.0       0.99      0.88      0.93        94\n        22.0       0.85      0.59      0.70       101\n        23.0       0.99      0.96      0.97        97\n        24.0       1.00      0.97      0.98        90\n        25.0       1.00      1.00      1.00        90\n        26.0       0.93      0.77      0.84        88\n\n   micro avg       0.86      0.86      0.86      2340\n   macro avg       0.87      0.86      0.86      2340\nweighted avg       0.87      0.86      0.86      2340\n\nConfusion Matrix for non-optimized kNN\n[[76  0  0  1  0  0  0  0  0  0  6  0  0  0  0  1  0  0  0  0  0  0  0  0\n   0  0]\n [ 1 78  0 11  1  0  0  0  0  0  0  0  0  0  0  1  0  0  0  0  0  3  0  0\n   0  0]\n [ 0  0 95  0  0  0  2  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n   0  3]\n [ 0 12  0 65  2  0  4  0  0  0  0  0  0  0  0  0  0  0  0  4  0  4  0  0\n   0  0]\n [ 2 11  0  7 45  0  1  0  0  0  0  0  0  0  0  3  0  0  0  0  0  0  0  0\n   0  0]\n [ 0  0  0  0  0 83  0  0  0  0  0  0  0  0  0  0  0  0  7  0  0  0  0  0\n   0  0]\n [ 0  0  0  5  0  0 73  0  0  1  0  0  0  0  0  0  0  0  0  3  0  1  0  0\n   0  0]\n [ 1  0  0  0  0  0  0 86  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n   0  0]\n [ 0  0  0  0  0  0  0  0 84  0  0  0  0  0  0  0  0  3  0  0  0  0  0  0\n   0  0]\n [ 0  0  0  0  0  0  1  0  0 87  2  0  0  0  0  0  0  0  0  0  0  0  0  0\n   0  0]\n [ 1  0  0  0  0  0  0  0  0 10 71  0  0  0  0  0  0  0  0  0  0  0  0  0\n   0  0]\n [ 0  0  0  0  0  0  0  0  0  0  0 83  0  0  5  0  0  0  0  0  0  0  0  0\n   0  0]\n [ 1  0  0  0  0  0  0  0  0  0  0  6 73 16  0  0  0  0  0  0  0  0  1  0\n   0  0]\n [ 1  0  0  0  0  0  0  0  0  0  0  0 24 73  0  0  0  0  0  0  1  0  0  0\n   0  0]\n [ 0  0  0  0  0  0  0  0  0  0  0  2  0  0 78  0  0  0  0  0  0  0  0  0\n   0  0]\n [ 0 12  0  4  1  0  6  0  0  0  0  0  0  0  0 69  0  0  0  6  0  0  0  0\n   0  0]\n [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 84  0  0  0  0  0  0  0\n   0  0]\n [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 89  0  0  0  0  0  0\n   0  0]\n [ 0  0  0  0  0 13  0  0  0  0  0  0  0  0  0  0  0  0 87  0  0  0  0  0\n   0  0]\n [ 0  0  0 11  0  0  6  0  0  1  2  0  0  0  0 20  0  0  0 47  0  0  0  0\n   0  0]\n [ 0  0  0  0  0  0  0  0  0  0  0  0  1  0  0  0 10  0  0  0 83  0  0  0\n   0  0]\n [ 0 17  0 12  0  0  2  0  0  1  0  0  0  0  0  4  0  0  0  3  0 60  0  0\n   0  2]\n [ 1  0  0  0  0  0  1  0  0  0  0  0  0  0  1  1  0  0  0  0  0  0 93  0\n   0  0]\n [ 1  0  0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0  1  0  0  0  0 87\n   0  0]\n [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n  90  0]\n [ 0  0 13  3  0  0  0  0  0  0  0  0  0  0  0  1  0  0  0  0  0  3  0  0\n   0 68]]\n\nAccuracy percentage of this classifier is 85.769 %\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Επιχειρούμε βελτιστοποίηση. Ξεκινάμε υπολογίζοντας τη διασπορά του δείγματος καθώς ενδέχεται να παίξει ρόλο στην επιλογή του v_threshold."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "train_variance = train.var(axis=0)\nprint(train_variance)\nprint(np.max(train_variance))",
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": "[0.05657589 0.10317914 0.106779   0.0959123  0.21418516 0.32453442\n 0.31188155 0.28697762 0.28188678 0.26532755 0.22931446 0.19714385\n 0.20339876 0.22526384 0.2315474  0.23098169 0.20566921 0.17789607\n 0.1547159  0.13892287 0.13208593 0.1359187  0.14701459 0.15427166\n 0.15111764 0.148334   0.1487317  0.14764926 0.15209474 0.16474763\n 0.18491009 0.21470754 0.04598692 0.0848028  0.0952609  0.08571354\n 0.21025781 0.33308739 0.33826389 0.32471131 0.31266543 0.28697864\n 0.24791497 0.21787527 0.21700228 0.23346832 0.2390233  0.23717964\n 0.2133382  0.17646762 0.14162196 0.12171351 0.1171356  0.12346482\n 0.13491073 0.14164622 0.14672977 0.14623792 0.14591016 0.14370909\n 0.15059707 0.16328869 0.18597631 0.21917925 0.04475434 0.08622898\n 0.10423025 0.08691158 0.20647887 0.33846992 0.35878362 0.34965963\n 0.34611089 0.33101981 0.29360211 0.26117335 0.25407716 0.25847518\n 0.25045781 0.24125884 0.21461927 0.18448904 0.15163604 0.12946094\n 0.12793387 0.13805317 0.14749851 0.15165592 0.1533348  0.15193542\n 0.15191862 0.15288032 0.15990229 0.1722341  0.19358305 0.22326585\n 0.05097592 0.09580093 0.11714546 0.09682153 0.21237187 0.3431785\n 0.36938337 0.35826586 0.35847584 0.35452497 0.3276354  0.30205268\n 0.289504   0.28329629 0.26392563 0.25180668 0.23449611 0.20787528\n 0.1801144  0.15928521 0.16164752 0.17126962 0.17992238 0.18414428\n 0.18087367 0.17933986 0.17844054 0.18274863 0.18796495 0.19306054\n 0.21047442 0.233886   0.06364239 0.11438245 0.12816592 0.10918612\n 0.2307411  0.3547462  0.37448485 0.34369356 0.34075015 0.33598501\n 0.32715187 0.31421546 0.3138286  0.30907911 0.28939914 0.28024275\n 0.26656186 0.24688962 0.22805521 0.20958533 0.20789207 0.21410472\n 0.22531245 0.22639495 0.22046592 0.21589417 0.21725931 0.22473349\n 0.22331225 0.21892329 0.22608398 0.24304762 0.08085389 0.13575717\n 0.13625529 0.12124195 0.25047514 0.35298614 0.34812019 0.30549883\n 0.29452945 0.29442851 0.29665462 0.30112526 0.31354253 0.31467913\n 0.28914745 0.28600683 0.28083202 0.28566004 0.28190379 0.26193562\n 0.25544936 0.26645522 0.27829573 0.2712792  0.25692078 0.24771841\n 0.2490083  0.24829075 0.2394297  0.22639891 0.22101003 0.23127977\n 0.13038646 0.18066045 0.16702739 0.17831575 0.25674805 0.31353339\n 0.30047734 0.26124228 0.25287765 0.25616427 0.25533284 0.25601394\n 0.26889372 0.27514711 0.25626515 0.25254762 0.25426805 0.27539382\n 0.29079261 0.28356581 0.27582543 0.28954714 0.29886087 0.28333021\n 0.25841588 0.25469192 0.24787354 0.24127302 0.22889489 0.20891296\n 0.2024529  0.20252947 0.11922033 0.14161389 0.12462169 0.09929034\n 0.16065687 0.23104304 0.22738758 0.21474654 0.2062219  0.19293449\n 0.16805044 0.14993886 0.15302641 0.17031103 0.17355499 0.17137675\n 0.15957432 0.14301379 0.13226485 0.11762966 0.11520904 0.12027716\n 0.1326063  0.13438075 0.13453936 0.13176016 0.12909851 0.13291371\n 0.13387325 0.14680275 0.15228104 0.17442835 0.08806364 0.11297169\n 0.11310667 0.09394173 0.16056864 0.24495056 0.2452614  0.22403567\n 0.22064573 0.20679933 0.17876626 0.15908539 0.16161125 0.1751529\n 0.18329021 0.18306401 0.16507395 0.14714527 0.12914345 0.11724773\n 0.11436499 0.11637264 0.12760754 0.13239614 0.13329648 0.132245\n 0.12813647 0.13072905 0.13189457 0.14205472 0.15853978 0.18364918\n 0.06669732 0.10105219 0.11818914 0.09927375 0.19880305 0.30555926\n 0.31471541 0.3023079  0.3028406  0.2978699  0.278393   0.26470947\n 0.26005771 0.2539924  0.23715852 0.22740475 0.21743916 0.20614016\n 0.18521384 0.1697075  0.17261833 0.18227739 0.18805233 0.18716309\n 0.18132747 0.17353304 0.17070027 0.17432721 0.17199282 0.17124409\n 0.18307121 0.20315407 0.01742563 0.02613891 0.03613297 0.05237441\n 0.05962219 0.06454202 0.06554653 0.05953517 0.04595788 0.04388611\n 0.07127098 0.10778663 0.09779676 0.08376608 0.08501725 0.09459981\n 0.1185021  0.1396284  0.14863248 0.14376334 0.11829855 0.05338992\n 0.01742287 0.01338493 0.01388502 0.01594416 0.01325022 0.00854355\n 0.00617746 0.00446516 0.0036084  0.00344744 0.00331739 0.03284454\n 0.04982187 0.0725926  0.10483031 0.13434748 0.16427671 0.21477353\n 0.24713058 0.24901271 0.21843859 0.14653586 0.15523893 0.13402788\n 0.10877149 0.10632564 0.12386799 0.16325501 0.19333734 0.19228679\n 0.16284387 0.10771741 0.04072815 0.01477909 0.01558235 0.01800651\n 0.02211089 0.02630866 0.0344196  0.04309851 0.05029971 0.05395112\n 0.05343739 0.05212829 0.08958108 0.12708264 0.1641489  0.19880747\n 0.24487889 0.3107713  0.39308247 0.46331824 0.47081438 0.41461156\n 0.23072415 0.04357402 0.05032122 0.05115393 0.05081157 0.04925655\n 0.04891713 0.04868671 0.04499211 0.04003396 0.03284309 0.0282585\n 0.04867587 0.11151212 0.1649461  0.19040632 0.21955591 0.25451027\n 0.2927065  0.31995826 0.33604268 0.3385308  0.3339297  0.17831703\n 0.20648568 0.23491189 0.2699531  0.36013472 0.49621639 0.5777398\n 0.56154661 0.56974297 0.48034424 0.17966073 0.20152932 0.42172052\n 0.20631665 0.15235078 0.16565554 0.20930125 0.21736728 0.22562321\n 0.33017232 0.43383362 0.44916594 0.42104    0.2833349  0.21324763\n 0.1955413  0.2262087  0.22964709 0.20813467 0.15243338 0.11589206\n 0.10172612 0.10823717 0.3738921  0.37133271 0.36953806 0.36680315\n 0.36474789 0.36324632 0.36328087 0.3639092  0.36404459 0.33092482\n 0.31772151 0.32086332 0.32385654 0.32692299 0.33048532 0.33467876\n 0.34074129 0.35127522 0.36889601 0.36687139 0.1863913  0.14215034\n 0.16221531 0.15516512 0.19270895 0.19349567 0.17812424 0.49852015\n 0.13901675 0.13846579 0.15627084 0.17029732 0.15602731 0.14156088\n 0.13554938 0.13193799 0.12049468 0.10876728 0.0977817  0.08977228\n 0.08610003 0.08055443 0.07665742 0.07263602 0.07494823 0.08195141\n 0.08812449 0.09877232 0.10481736 0.11036626 0.11644258 0.11874123\n 0.12092729 0.11912965 0.11746296 0.11491613 0.11132379 0.10949539\n 0.10279679 0.09989698 0.16750823 0.18550083 0.22782622 0.22499662\n 0.19408395 0.16912331 0.16095272 0.16029731 0.13977515 0.12095429\n 0.10843111 0.09841751 0.09248878 0.08700598 0.08369034 0.07783371\n 0.08194094 0.09019062 0.0955522  0.10672477 0.11659408 0.12696031\n 0.1313959  0.13727382 0.14171527 0.14041807 0.13536482 0.13481886\n 0.13056242 0.12783634 0.1226642  0.13241237 0.15964446 0.1811366\n 0.22113589 0.22063625 0.18327072 0.15448185 0.13967786 0.13994625\n 0.11780718 0.10553039 0.09535154 0.08732311 0.08395035 0.08144415\n 0.07903518 0.07598683 0.08454831 0.09486077 0.10173823 0.11354257\n 0.12274523 0.13111652 0.13867631 0.14102401 0.14266421 0.14122563\n 0.1362801  0.13400164 0.12769865 0.12586015 0.12371819 0.12842601\n 0.53666062 0.60974714 0.46136182 0.8719155  0.21380342 0.1585935\n 0.17324676 0.32287256 0.46351153 0.14547474 0.15154662 0.12301224\n 0.14250939 0.13849852 0.13526254 0.13443065 0.12772052 0.1129906\n 0.11133272 0.10681339 0.10189655 0.09283808 0.09167291 0.08529444\n 0.08546647 0.09185872 0.0966899  0.0980789  0.10730148 0.10791393\n 0.11451926 0.1137004  0.11041732 0.10832788 0.11207204 0.1117519\n 0.11114349 0.11201417 0.11498863 0.11036531 0.12622667]\n0.8719154994875857\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Συμπεραίνουμε ότι λογικά θα χρειαστούμε v_threshold πολύ κοντά στο 0."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from sklearn.model_selection import GridSearchCV\nfrom sklearn import neighbors\n\n### PREPROCESSING\nvthreshold = [0,0.1]\nn_components = [35,50,65,80]\n\n### kNN specific\nk_neighbors = [1,5,7,9,11,17]\nmetric =['euclidean']\nweights =['uniform','distance']\nverbose=10\n\nclf = neighbors.KNeighborsClassifier(n_jobs=-1) # η παράμετρος n_jobs = -1 χρησιμοποιεί όλους τους πυρήνες του υπολογιστή\npipe = Pipeline(steps=[('selector', selector), ('scaler', scaler), ('sampler', ros), ('pca', pca), ('kNN', clf)])\n\nestimator_macro = GridSearchCV(pipe, dict(selector__threshold=vthreshold, pca__n_components=n_components,kNN__n_neighbors=k_neighbors,kNN__weights=weights,kNN__metric=metric),verbose=10,cv=5,scoring='f1_macro', n_jobs=-1)\n\nstart_time = time.time()\nestimator_macro.fit(train, train_labels)\nprint(\"Για τον optimized kNN : %s seconds\" % (time.time() - start_time))\n\npreds = estimator_macro.predict(test)\n\n#estimator_weighted = GridSearchCV(pipe, dict(selector__threshold=vthreshold, pca__n_components=n_components,kNN__n_neighbors=k_neighbors,kNN__weights=weights,kNN__metric=metric), cv=5,scoring='f1_weighted', n_jobs=-1)\n#estimator_weighted.fit(train, train_labels)\n#preds_weighted = estimator_weighted.predict(test)\n\n#################\nprint ('Classification report for optimized kNN')\ncr_knn_opt = classification_report(test_labels, preds)\nprint (cr_knn_opt)\n\nscores_micro['kNN-opt']=precision_recall_fscore_support(test_labels,preds,average='micro')\nscores_macro['kNN-opt']=precision_recall_fscore_support(test_labels,preds,average='macro')\n\nprint ('Confusion Matrix for optimized kNN')\nprint (confusion_matrix(test_labels, preds))\n\nacc_knn_opt = 100*accuracy_score(test_labels,preds)\nprint ('\\nAccuracy percentage of this classifier is %.3f %%' % (acc_knn_opt))\n\n# print best estimator configuration\nprint ('\\nFor kNN the optimal configuration is :')\nprint (estimator_macro.best_estimator_)",
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Fitting 5 folds for each of 96 candidates, totalling 480 fits\n",
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:    6.5s\n[Parallel(n_jobs=-1)]: Done   4 tasks      | elapsed:   11.1s\n[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:   16.0s\n[Parallel(n_jobs=-1)]: Done  14 tasks      | elapsed:   22.0s\n[Parallel(n_jobs=-1)]: Done  21 tasks      | elapsed:   30.6s\n[Parallel(n_jobs=-1)]: Done  28 tasks      | elapsed:   39.6s\n[Parallel(n_jobs=-1)]: Done  37 tasks      | elapsed:   54.0s\n[Parallel(n_jobs=-1)]: Done  46 tasks      | elapsed:  1.1min\n[Parallel(n_jobs=-1)]: Done  57 tasks      | elapsed:  1.3min\n[Parallel(n_jobs=-1)]: Done  68 tasks      | elapsed:  1.5min\n[Parallel(n_jobs=-1)]: Done  81 tasks      | elapsed:  1.7min\n[Parallel(n_jobs=-1)]: Done  94 tasks      | elapsed:  2.2min\n[Parallel(n_jobs=-1)]: Done 109 tasks      | elapsed:  2.8min\n[Parallel(n_jobs=-1)]: Done 124 tasks      | elapsed:  3.5min\n[Parallel(n_jobs=-1)]: Done 141 tasks      | elapsed:  4.2min\n[Parallel(n_jobs=-1)]: Done 158 tasks      | elapsed:  5.2min\n[Parallel(n_jobs=-1)]: Done 177 tasks      | elapsed:  6.0min\n[Parallel(n_jobs=-1)]: Done 196 tasks      | elapsed:  7.0min\n[Parallel(n_jobs=-1)]: Done 217 tasks      | elapsed:  7.8min\n[Parallel(n_jobs=-1)]: Done 238 tasks      | elapsed:  8.8min\n[Parallel(n_jobs=-1)]: Done 261 tasks      | elapsed:  9.6min\n[Parallel(n_jobs=-1)]: Done 284 tasks      | elapsed: 10.9min\n[Parallel(n_jobs=-1)]: Done 309 tasks      | elapsed: 12.1min\n[Parallel(n_jobs=-1)]: Done 334 tasks      | elapsed: 13.3min\n[Parallel(n_jobs=-1)]: Done 361 tasks      | elapsed: 14.6min\n[Parallel(n_jobs=-1)]: Done 388 tasks      | elapsed: 15.6min\n[Parallel(n_jobs=-1)]: Done 417 tasks      | elapsed: 16.8min\n[Parallel(n_jobs=-1)]: Done 446 tasks      | elapsed: 18.4min\n[Parallel(n_jobs=-1)]: Done 477 tasks      | elapsed: 20.2min\n[Parallel(n_jobs=-1)]: Done 480 out of 480 | elapsed: 20.4min finished\n",
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": "Για τον optimized kNN : 1226.2798564434052 seconds\nClassification report for optimized kNN\n              precision    recall  f1-score   support\n\n         1.0       0.96      0.94      0.95        84\n         2.0       0.73      0.88      0.80        95\n         3.0       0.91      0.96      0.93       100\n         4.0       0.79      0.87      0.83        91\n         5.0       0.90      0.67      0.77        69\n         6.0       0.89      0.98      0.93        90\n         7.0       0.87      0.94      0.90        83\n         8.0       1.00      0.99      0.99        87\n         9.0       0.99      1.00      0.99        87\n        10.0       0.89      0.99      0.94        90\n        11.0       0.89      0.90      0.90        82\n        12.0       0.96      0.97      0.96        88\n        13.0       0.75      0.82      0.78        97\n        14.0       0.85      0.72      0.78        99\n        15.0       0.96      0.97      0.97        80\n        16.0       0.77      0.84      0.80        98\n        17.0       0.87      0.99      0.93        84\n        18.0       1.00      0.99      0.99        89\n        19.0       0.98      0.91      0.94       100\n        20.0       0.85      0.71      0.77        87\n        21.0       0.95      0.87      0.91        94\n        22.0       0.89      0.75      0.82       101\n        23.0       0.97      0.99      0.98        97\n        24.0       1.00      0.97      0.98        90\n        25.0       1.00      1.00      1.00        90\n        26.0       0.95      0.82      0.88        88\n\n   micro avg       0.90      0.90      0.90      2340\n   macro avg       0.91      0.90      0.90      2340\nweighted avg       0.90      0.90      0.90      2340\n\nConfusion Matrix for optimized kNN\n[[79  0  0  0  0  0  0  0  0  0  5  0  0  0  0  0  0  0  0  0  0  0  0  0\n   0  0]\n [ 1 84  0  3  3  0  0  0  0  0  0  0  0  0  0  1  0  0  0  0  0  3  0  0\n   0  0]\n [ 0  0 96  0  0  0  2  0  0  0  0  0  0  0  0  0  0  0  0  1  0  0  0  0\n   0  1]\n [ 0  2  0 79  1  0  0  0  0  0  0  0  0  0  0  3  0  0  0  4  0  2  0  0\n   0  0]\n [ 0 11  0  8 46  0  0  0  0  0  0  0  0  0  0  4  0  0  0  0  0  0  0  0\n   0  0]\n [ 0  0  0  0  0 88  0  0  0  0  0  0  0  0  0  0  0  0  2  0  0  0  0  0\n   0  0]\n [ 0  0  0  1  0  0 78  0  0  2  0  0  0  0  0  1  0  0  0  0  0  0  0  0\n   0  1]\n [ 1  0  0  0  0  0  0 86  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n   0  0]\n [ 0  0  0  0  0  0  0  0 87  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n   0  0]\n [ 0  0  0  0  0  0  0  0  0 89  1  0  0  0  0  0  0  0  0  0  0  0  0  0\n   0  0]\n [ 0  0  0  0  0  0  0  0  0  7 74  0  0  0  0  1  0  0  0  0  0  0  0  0\n   0  0]\n [ 0  0  0  0  0  0  0  0  0  0  0 85  0  0  3  0  0  0  0  0  0  0  0  0\n   0  0]\n [ 0  0  0  0  0  0  0  0  0  0  0  2 80 13  0  0  0  0  0  0  1  0  1  0\n   0  0]\n [ 0  0  0  0  0  0  0  0  0  0  1  0 26 71  0  0  0  0  0  0  1  0  0  0\n   0  0]\n [ 0  0  0  0  0  0  0  0  0  0  0  2  0  0 78  0  0  0  0  0  0  0  0  0\n   0  0]\n [ 0  6  0  2  0  0  2  0  0  1  0  0  0  0  0 82  0  0  0  3  0  1  1  0\n   0  0]\n [ 0  0  0  0  0  0  0  0  0  0  0  0  1  0  0  0 83  0  0  0  0  0  0  0\n   0  0]\n [ 0  0  0  0  0  0  0  0  1  0  0  0  0  0  0  0  0 88  0  0  0  0  0  0\n   0  0]\n [ 0  0  0  0  0  9  0  0  0  0  0  0  0  0  0  0  0  0 91  0  0  0  0  0\n   0  0]\n [ 0  0  0  3  0  0  7  0  0  1  1  0  0  0  0 12  0  0  0 62  0  1  0  0\n   0  0]\n [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 12  0  0  0 82  0  0  0\n   0  0]\n [ 0 12  0  4  0  0  1  0  0  0  0  0  0  0  0  3  0  0  0  2  1 76  0  0\n   0  2]\n [ 0  0  0  0  0  0  0  0  0  0  1  0  0  0  0  0  0  0  0  0  0  0 96  0\n   0  0]\n [ 1  0  0  0  0  2  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 87\n   0  0]\n [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n  90  0]\n [ 0  0 10  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1  1  2  1  0\n   0 72]]\n\nAccuracy percentage of this classifier is 90.128 %\n\nFor kNN the optimal configuration is :\nPipeline(memory=None,\n     steps=[('selector', VarianceThreshold(threshold=0)), ('scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('sampler', RandomOverSampler(random_state=None, ratio=None, return_indices=False,\n         sampling_strategy='auto')), ('pca', PCA(copy=True, iterated_power='auto', n_component...an',\n           metric_params=None, n_jobs=-1, n_neighbors=17, p=2,\n           weights='distance'))])\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Φαίνεται να υπάρχει βελτίωση της τάξης του 5 με 6%, αρκετά ικανοποιητικός αριθμός. Έγιναν δοκιμές για διαφορετικές τιμές διαφόρων παραμέτρων αλλά αυτός φαίνεται να είναι ο αποδοτικότερος συνδυασμός, αν και κάπως χρονοβόρος."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Επόμενο βήμα η απόπειρα βελτιστοποίησης του mlp classifier."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "clf = MLPClassifier()\n\npipe = Pipeline(steps=[('selector', selector), ('scaler', scaler), ('sampler', ros), ('pca', pca), ('mlp', clf)])\n\n### PREPROCESSING\nvthreshold = [0]\nn_components = [600]#,50,200,300,500,600]\n\n### MLP specific\nso = ['lbfgs']\nal = [10**float(c) for c in np.arange(-7,-6)]\nhd = [45]#, 25,45]\nmi = [900]#,400,500,600,1000,800]\nlr = ['constant','invscaling','adaptive']\nac = ['identity','logistic','tanh','relu']\n\nestimator_macro = GridSearchCV(pipe, dict(selector__threshold=vthreshold, pca__n_components=n_components,mlp__solver=so, mlp__alpha=al,mlp__hidden_layer_sizes=hd,mlp__max_iter=mi,mlp__learning_rate=lr,mlp__activation=ac),cv=5,scoring='f1_macro', n_jobs=-1)\n\nstart_time = time.time()\nestimator_macro.fit(train, train_labels)\nprint(\"Για τον optimized MLP : %s seconds\" % (time.time() - start_time))\n\npreds = estimator_macro.predict(test)\n\n#estimator_weighted = GridSearchCV(pipe, dict(selector__threshold=vthreshold, pca__n_components=n_components,mlp__solver=so, mlp__alpha=al,mlp__hidden_layer_sizes=hd,mlp__max_iter=mi), cv=5,scoring='f1_weighted', n_jobs=-1)\n#estimator_weighted.fit(train, train_labels)\n#preds_weighted = estimator_weighted.predict(test)\n\n#################\nprint ('Classification report for optimized MLP')\ncr_mlp_opt = classification_report(test_labels, preds)\nprint (cr_mlp_opt)\n\nscores_micro['MLP-opt']=precision_recall_fscore_support(test_labels,preds,average='micro')\nscores_macro['MLP-opt']=precision_recall_fscore_support(test_labels,preds,average='macro')\n\nprint ('Confusion Matrix for optimized MLP')\nprint (confusion_matrix(test_labels, preds))\n\nacc_mlp_opt = 100*accuracy_score(test_labels,preds)\nprint ('Accuracy percentage of this classifier is %.3f %%' % (acc_mlp_opt))\n\nprint ('\\nFor MLP the optimal configuration is :')\nprint (estimator_macro.best_estimator_)",
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Για τον optimized MLP : 214.35696291923523 seconds\nClassification report for optimized MLP\n              precision    recall  f1-score   support\n\n         1.0       0.99      1.00      0.99        84\n         2.0       0.88      0.92      0.90        95\n         3.0       0.95      0.99      0.97       100\n         4.0       0.87      0.90      0.89        91\n         5.0       0.92      0.86      0.89        69\n         6.0       0.95      0.92      0.94        90\n         7.0       0.92      0.94      0.93        83\n         8.0       1.00      0.99      0.99        87\n         9.0       0.99      1.00      0.99        87\n        10.0       0.98      0.96      0.97        90\n        11.0       0.94      0.98      0.96        82\n        12.0       0.96      0.92      0.94        88\n        13.0       0.92      0.91      0.91        97\n        14.0       0.96      0.90      0.93        99\n        15.0       0.92      0.99      0.95        80\n        16.0       0.94      0.90      0.92        98\n        17.0       0.98      0.99      0.98        84\n        18.0       0.98      0.99      0.98        89\n        19.0       0.93      0.98      0.96       100\n        20.0       0.93      0.86      0.89        87\n        21.0       0.97      0.98      0.97        94\n        22.0       0.87      0.89      0.88       101\n        23.0       0.97      0.99      0.98        97\n        24.0       1.00      0.99      0.99        90\n        25.0       0.99      0.99      0.99        90\n        26.0       0.95      0.92      0.94        88\n\n   micro avg       0.95      0.95      0.95      2340\n   macro avg       0.95      0.95      0.95      2340\nweighted avg       0.95      0.95      0.95      2340\n\nConfusion Matrix for optimized MLP\n[[84  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n   0  0]\n [ 0 87  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  7  0  0\n   0  0]\n [ 0  0 99  0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n   0  0]\n [ 0  1  0 82  4  0  2  0  0  0  0  0  0  0  0  0  0  0  0  0  0  2  0  0\n   0  0]\n [ 0  5  0  3 59  0  0  0  0  0  0  0  0  0  0  1  0  0  0  0  0  1  0  0\n   0  0]\n [ 0  0  0  0  0 83  0  0  0  0  0  0  0  0  0  0  0  0  7  0  0  0  0  0\n   0  0]\n [ 0  0  0  0  0  0 78  0  0  1  0  0  0  0  0  1  0  0  0  3  0  0  0  0\n   0  0]\n [ 1  0  0  0  0  0  0 86  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n   0  0]\n [ 0  0  0  0  0  0  0  0 87  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n   0  0]\n [ 0  0  0  0  0  0  0  0  0 86  4  0  0  0  0  0  0  0  0  0  0  0  0  0\n   0  0]\n [ 0  0  0  0  0  0  1  0  0  1 80  0  0  0  0  0  0  0  0  0  0  0  0  0\n   0  0]\n [ 0  0  0  0  0  0  0  0  0  0  0 81  1  0  6  0  0  0  0  0  0  0  0  0\n   0  0]\n [ 0  0  0  0  0  1  0  0  0  0  0  2 88  4  0  0  0  1  0  0  1  0  0  0\n   0  0]\n [ 0  0  0  0  0  0  0  0  0  0  1  1  7 89  0  0  0  0  0  0  1  0  0  0\n   0  0]\n [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0 79  0  0  1  0  0  0  0  0  0\n   0  0]\n [ 0  2  0  1  0  1  0  0  0  0  0  0  0  0  0 88  0  0  0  3  0  2  1  0\n   0  0]\n [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 83  0  0  0  1  0  0  0\n   0  0]\n [ 0  0  0  0  0  0  0  0  1  0  0  0  0  0  0  0  0 88  0  0  0  0  0  0\n   0  0]\n [ 0  0  0  0  0  2  0  0  0  0  0  0  0  0  0  0  0  0 98  0  0  0  0  0\n   0  0]\n [ 0  0  1  4  0  0  3  0  0  0  0  0  0  0  0  3  0  0  0 75  0  0  1  0\n   0  0]\n [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  2  0  0  0 92  0  0  0\n   0  0]\n [ 0  4  0  2  0  0  0  0  0  0  0  0  0  0  0  1  0  0  0  0  0 90  0  0\n   0  4]\n [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  1  0  0  0  0  0  0  0 96  0\n   0  0]\n [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 89\n   1  0]\n [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1  0\n  89  0]\n [ 0  0  4  2  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1  0  0\n   0 81]]\nAccuracy percentage of this classifier is 94.744 %\n\nFor MLP the optimal configuration is :\nPipeline(memory=None,\n     steps=[('selector', VarianceThreshold(threshold=0)), ('scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('sampler', RandomOverSampler(random_state=None, ratio=None, return_indices=False,\n         sampling_strategy='auto')), ('pca', PCA(copy=True, iterated_power='auto', n_component...True, solver='lbfgs', tol=0.0001,\n       validation_fraction=0.1, verbose=False, warm_start=False))])\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Ο mlp ηταν αρκετά ψηλά σε accuracy και δεν πετύχαμε καμία βελτιστοποίηση, πιθανώς λόγω τυχαιότητας να προέρχεται η μικρή διαφορά που έχει πριν και μετά τη βελτιστοποίηση. Ακολουθούν τα διαγράμματα που ζητούνται βάσει των f1_micro και f1_macro, καθώς και πίνακας απόδοσης των ταξινομητών πριν και μετά τη βελτιστοποίηση."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Κάνουμε import την matplotplib\nimport matplotlib.pyplot as plt\n\nf1_scores_micro = [item[2] for item in scores_micro.values()]\nf1_scores_macro = [item[2] for item in scores_macro.values()]\n\ny_pos = np.arange(len(f1_scores_micro))\nplt.barh(y_pos, f1_scores_micro, align='center',color='red')\nplt.yticks(y_pos, scores_micro.keys())\nplt.title('F1_micro average scores')\nplt.show()\n\ny_pos = np.arange(len(f1_scores_macro))\nplt.barh(y_pos, f1_scores_macro, align='center',color='green')\nplt.yticks(y_pos, scores_macro.keys())\nplt.title('F1_macro average scores')\nplt.show()",
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": "<Figure size 432x288 with 1 Axes>"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": "<Figure size 432x288 with 1 Axes>"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Παρακάτω παρουσιάζονται οι πίνακες με τα συγκριτικά αποτελέσματα της απόδοσης των ταξινομητών πριν και μετά τη βελτιστοποίηση (μεταβολή του accuracy)."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "x= {'%DummyUni': [4.231-4.188], '%DummyMostFreq': [3.590-2.949],'%DummyStrat': [3.889-3.761],'%GNBC':[81.325-84.957],'%kNN':[90.128-85.769],'%MLP':[94.744- 95.812]}\ndf1 = pd.DataFrame(data=x)\ndf1",
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 30,
          "data": {
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>%DummyUni</th>\n      <th>%DummyMostFreq</th>\n      <th>%DummyStrat</th>\n      <th>%GNBC</th>\n      <th>%kNN</th>\n      <th>%MLP</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.043</td>\n      <td>0.641</td>\n      <td>0.128</td>\n      <td>-3.632</td>\n      <td>4.359</td>\n      <td>-1.068</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
            "text/plain": "   %DummyUni  %DummyMostFreq  %DummyStrat  %GNBC   %kNN   %MLP\n0      0.043           0.641        0.128 -3.632  4.359 -1.068"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Συμπερασματικά, βλέπουμε ότι σημαντική βελτίωση είχαμε στον ταξινομητή kNN ενώ ο GNBC δεν βελτιστοποιήθηκε αλλά αντιθέτως μειώθηκε το accuracy του. Συνολικά, όπως φαίνεται και από τα f1_micro και f1_macro scores ο πιο αξιόπιστος ταξινομητής για το dataset μας είναι ο MLP ο οποίος, παρ΄όλο που δεν πετύχαμε κάποια βελτίωση κινείται σε αρκετά υψηλά επίπεδα. Μικρές διαφορές πριν και μετά τη βελτιστοποίηση είναι πιθανό να οφείλονται και σε ένα παράγοντα τυχαιότητας που εμπεριέχει ο υπολογισμός των βέλτιστων υπερπαραμέτρων αλλά και η εκτέλεση των ταξινομητών."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python36",
      "display_name": "Python 3.6",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}