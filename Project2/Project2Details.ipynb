{
  "cells": [
    {
      "metadata": {
        "colab_type": "text",
        "id": "hLFXD3IePSyL"
      },
      "cell_type": "markdown",
      "source": "# Εργαστηριακή Άσκηση 2. Μη επιβλεπόμενη μάθηση. \nΗμερομηνία εκφώνησης άσκησης: 3/12/18\n## Σύστημα συστάσεων βασισμένο στο περιεχόμενο\n## Σημασιολογική απεικόνιση δεδομένων με χρήση SOM \n\n"
    },
    {
      "metadata": {
        "id": "S5wbBzIYnird",
        "colab_type": "code",
        "colab": {},
        "trusted": true
      },
      "cell_type": "code",
      "source": "!pip install --upgrade pip\n!pip install --upgrade numpy\n!pip install --upgrade pandas\n!pip install --upgrade nltk\n!pip install --upgrade scikit-learn",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Requirement already up-to-date: pip in /home/nbuser/anaconda3_420/lib/python3.5/site-packages (18.1)\nRequirement already up-to-date: numpy in /home/nbuser/anaconda3_420/lib/python3.5/site-packages (1.15.4)\nCollecting pandas\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5d/d4/6e9c56a561f1d27407bf29318ca43f36ccaa289271b805a30034eb3a8ec4/pandas-0.23.4-cp35-cp35m-manylinux1_x86_64.whl (8.7MB)\n\u001b[K    100% |████████████████████████████████| 8.7MB 94kB/s eta 0:00:011    46% |███████████████                 | 4.1MB 5.2MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied, skipping upgrade: numpy>=1.9.0 in /home/nbuser/anaconda3_420/lib/python3.5/site-packages (from pandas) (1.15.4)\nRequirement already satisfied, skipping upgrade: pytz>=2011k in /home/nbuser/anaconda3_420/lib/python3.5/site-packages (from pandas) (2016.6.1)\nRequirement already satisfied, skipping upgrade: python-dateutil>=2.5.0 in /home/nbuser/anaconda3_420/lib/python3.5/site-packages (from pandas) (2.7.5)\nRequirement already satisfied, skipping upgrade: six>=1.5 in /home/nbuser/anaconda3_420/lib/python3.5/site-packages (from python-dateutil>=2.5.0->pandas) (1.11.0)\nInstalling collected packages: pandas\n  Found existing installation: pandas 0.19.2\n    Uninstalling pandas-0.19.2:\n      Successfully uninstalled pandas-0.19.2\nSuccessfully installed pandas-0.23.4\nCollecting nltk\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6f/ed/9c755d357d33bc1931e157f537721efb5b88d2c583fe593cc09603076cc3/nltk-3.4.zip (1.4MB)\n\u001b[K    100% |████████████████████████████████| 1.4MB 2.3MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied, skipping upgrade: six in /home/nbuser/anaconda3_420/lib/python3.5/site-packages (from nltk) (1.11.0)\nRequirement already satisfied, skipping upgrade: singledispatch in /home/nbuser/anaconda3_420/lib/python3.5/site-packages (from nltk) (3.4.0.3)\nBuilding wheels for collected packages: nltk\n  Running setup.py bdist_wheel for nltk ... \u001b[?25ldone\n\u001b[?25h  Stored in directory: /home/nbuser/.cache/pip/wheels/4b/c8/24/b2343664bcceb7147efeb21c0b23703a05b23fcfeaceaa2a1e\nSuccessfully built nltk\nInstalling collected packages: nltk\n  Found existing installation: nltk 3.2.1\n    Uninstalling nltk-3.2.1:\n      Successfully uninstalled nltk-3.2.1\nSuccessfully installed nltk-3.4\nCollecting scikit-learn\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/18/d9/bea927c86bf78d583d517f24cbc87606cb333bfb3a5c99cb85b547305f0f/scikit_learn-0.20.2-cp35-cp35m-manylinux1_x86_64.whl (5.3MB)\n\u001b[K    100% |████████████████████████████████| 5.3MB 1.4MB/s eta 0:00:01   15% |█████                           | 829kB 5.1MB/s eta 0:00:01    45% |██████████████▋                 | 2.4MB 9.1MB/s eta 0:00:01    96% |██████████████████████████████▉ | 5.2MB 12.3MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied, skipping upgrade: numpy>=1.8.2 in /home/nbuser/anaconda3_420/lib/python3.5/site-packages (from scikit-learn) (1.15.4)\nRequirement already satisfied, skipping upgrade: scipy>=0.13.3 in /home/nbuser/anaconda3_420/lib/python3.5/site-packages (from scikit-learn) (1.1.0)\nInstalling collected packages: scikit-learn\n  Found existing installation: scikit-learn 0.19.1\n    Uninstalling scikit-learn-0.19.1:\n      Successfully uninstalled scikit-learn-0.19.1\nSuccessfully installed scikit-learn-0.20.2\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "aViHqlQcPSyP"
      },
      "cell_type": "markdown",
      "source": "## Εισαγωγή του Dataset"
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "2ZVmdDExPSyQ"
      },
      "cell_type": "markdown",
      "source": "Το σύνολο δεδομένων με το οποίο θα δουλέψουμε είναι βασισμένο στο [Carnegie Mellon Movie Summary Corpus](http://www.cs.cmu.edu/~ark/personas/). Πρόκειται για ένα dataset με περίπου 40.000 περιγραφές ταινιών. Η περιγραφή κάθε ταινίας αποτελείται από τον τίτλο της, μια ή περισσότερες ετικέτες που χαρακτηρίζουν το είδος της ταινίας και τέλος τη σύνοψη της υπόθεσής της. Αρχικά εισάγουμε το dataset (χρησιμοποιήστε αυτούσιο τον κώδικα, δεν χρειάζεστε το αρχείο csv) στο dataframe `df_data_1`: "
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "62SOj46gPSyS",
        "colab": {},
        "trusted": true
      },
      "cell_type": "code",
      "source": "import pandas as pd\n\ndataset_url = \"https://drive.google.com/uc?export=download&id=1PdkVDENX12tQliCk_HtUnAUbfxXvnWuG\"\n# make direct link for drive docs this way https://www.labnol.org/internet/direct-links-for-google-drive/28356/\ndf_data_1 = pd.read_csv(dataset_url, sep='\\t',  header=None, quoting=3, error_bad_lines=False,encoding='utf-8')",
      "execution_count": 1,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "7TAEZGdIPSyW"
      },
      "cell_type": "markdown",
      "source": "Κάθε ομάδα θα δουλέψει σε ένα μοναδικό υποσύνολο 5.000 ταινιών (διαφορετικό dataset για κάθε ομάδα) ως εξής\n\n1. Κάθε ομάδα μπορεί να βρει [εδώ](https://docs.google.com/spreadsheets/d/12AmxMqvjrc0ruNmZYTBNxvnEktbec1DRG64LW7SX4HA/edit?usp=sharing) τον μοναδικό  αριθμό της \"Seed\" από 1 έως 128. \n\n2. Το data frame `df_data_2` έχει 128 γραμμές (ομάδες) και 5.000 στήλες. Σε κάθε ομάδα αντιστοιχεί η γραμμή του πίνακα με το `team_seed_number` της. Η γραμμή αυτή θα περιλαμβάνει 5.000 διαφορετικούς αριθμούς που αντιστοιχούν σε ταινίες του αρχικού dataset. \n\n3. Στο επόμενο κελί αλλάξτε τη μεταβλητή `team_seed_number` με το Seed της ομάδας σας από το Google Sheet.\n\n4. Τρέξτε τον κώδικα. Θα προκύψουν τα μοναδικά για κάθε ομάδα  titles, categories, catbins, summaries και corpus με τα οποία θα δουλέψετε."
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "2POlqDjkPSyY",
        "colab": {},
        "trusted": true
      },
      "cell_type": "code",
      "source": "import numpy as np\n\n# βάλτε το seed που αντιστοιχεί στην ομάδα σας\nteam_seed_number = 0\n\nmovie_seeds_url = \"https://drive.google.com/uc?export=download&id=1NkzL6rqv4DYxGY-XTKkmPqEoJ8fNbMk_\"\ndf_data_2 = pd.read_csv(movie_seeds_url, header=None, error_bad_lines=False)\n\n# επιλέγεται \nmy_index = df_data_2.iloc[team_seed_number,:].values\n\ntitles = df_data_1.iloc[:, [2]].values[my_index] # movie titles (string)\ncategories = df_data_1.iloc[:, [3]].values[my_index] # movie categories (string)\nbins = df_data_1.iloc[:, [4]]\ncatbins = bins[4].str.split(',', expand=True).values.astype(np.float)[my_index] # movie categories in binary form (1 feature per category)\nsummaries =  df_data_1.iloc[:, [5]].values[my_index] # movie summaries (string)\ncorpus = summaries[:,0].tolist() # list form of summaries",
      "execution_count": 3,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "If66lkwxPSyb"
      },
      "cell_type": "markdown",
      "source": "- Ο πίνακας **titles** περιέχει τους τίτλους των ταινιών. Παράδειγμα: 'Sid and Nancy'.\n- O πίνακας **categories** περιέχει τις κατηγορίες (είδη) της ταινίας υπό τη μορφή string. Παράδειγμα: '\"Tragedy\",  \"Indie\",  \"Punk rock\",  \"Addiction Drama\",  \"Cult\",  \"Musical\",  \"Drama\",  \"Biopic \\[feature\\]\",  \"Romantic drama\",  \"Romance Film\",  \"Biographical film\"'. Παρατηρούμε ότι είναι μια comma separated λίστα strings, με κάθε string να είναι μια κατηγορία.\n- Ο πίνακας **catbins** περιλαμβάνει πάλι τις κατηγορίες των ταινιών αλλά σε δυαδική μορφή ([one hot encoding](https://hackernoon.com/what-is-one-hot-encoding-why-and-when-do-you-have-to-use-it-e3c6186d008f)). Έχει διαστάσεις 5.000 x 322 (όσες οι διαφορετικές κατηγορίες). Αν η ταινία ανήκει στο συγκεκριμένο είδος η αντίστοιχη στήλη παίρνει την τιμή 1, αλλιώς παίρνει την τιμή 0.\n- Ο πίνακας **summaries** και η λίστα **corpus** περιλαμβάνουν τις συνόψεις των ταινιών (η corpus είναι απλά ο summaries σε μορφή λίστας). Κάθε σύνοψη είναι ένα (συνήθως μεγάλο) string. Παράδειγμα: *'The film is based on the real story of a Soviet Internal Troops soldier who killed his entire unit  as a result of Dedovschina. The plot unfolds mostly on board of the prisoner transport rail car guarded by a unit of paramilitary conscripts.'*\n- Θεωρούμε ως **ID** της κάθε ταινίας τον αριθμό γραμμής της ή το αντίστοιχο στοιχείο της λίστας. Παράδειγμα: για να τυπώσουμε τη σύνοψη της ταινίας με `ID=99` (την εκατοστή) θα γράψουμε `print(corpus[99])`."
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "k_7A3KXLp0qS",
        "colab": {},
        "trusted": true
      },
      "cell_type": "code",
      "source": "ID = 99\nprint(titles[ID])\nprint(categories[ID])\nprint(catbins[ID])\nprint(corpus[ID])",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": "['Gangs of Wasseypur']\n['\"Thriller\",  \"Crime Fiction\",  \"Action\",  \"Drama\",  \"Bollywood\"']\n[0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n A gang of heavily armed men scour and finally narrow down on a house in Wasseypur. They surround the house and unleash a wave of bullets and grenades on it with the intention of killing the family inside it. After heavy firing on the house, they retreat away from the crime scene in a vehicle, convinced they have killed everyone within. The leader of the gang then calls one JP Singh  on his cell phone and reports that the family have been successfully executed but he is double crossed by JP Singh as a fire fight erupts between them and a police check post blocking their escape route. The scene cuts abruptly for a prologue by the narrator, Nasir.  The narration describes the history and nature of Wasseypur. The village has been historically ruled by the Qureshi Muslims, a sub-caste of animal butchers who are feared by the non-Qureshi Muslims in Wasseypur and Dhanbad by extension. During the British Raj, the British seized the farm lands of Dhanbad for coal which began the business of coal mining in Dhanbad. The region was a hotbed of the local faceless dacoit Sultana Qureshi who robbed British trains in the night and thus holds some patriotic value for the locals.  Shahid Khan , a Pathan, takes advantage of the mysteriousness of the faceless dacoit Sultana, a Qureshi, by impersonating his identity to rob British ferry trains. The Qureshi clans eventually find out and order the banishment of Shahid Khan and his family from Wasseypur. They settle down in Dhanbad where Shahid begins work as a labourer in a coal mine. He is unable to be at his wife's side who dies during childbirth. The enraged Shahid kills the coal-mines muscleman who had denied him leave on that day. In 1947, Independent India begins to assert its authority over itself. The British coal mines are sold to Indian industrialists and Ramadhir Singh  receives a few coal-mines in the Dhanbad region. He hires Shahid as the new muscle-man of the coal mine who terrorizes the local population to seize their lands and extract compliance. On a rainy day, Ramadhir Singh overhears Shahid's ambitions of taking over the coal mines from him. Singh tricks Shahid to travel to Varanasi for business but instead, he is murdered. Nasir finds Ramadhir's umbrella with his initials near the door, concluding that Ramadhir eavesdropped on their conversation. He flees from the place with Shahid's son Sardar in the nick of time as Ehsaan Qureshi , another associate of Ramaadhir Singh shows up too late for killing them. An unsuccessful Ehsaan lies to Singh that Shahid's family has been murdered and buried. Sardar grows along with his cousin Asghar  and learns the truth about his fathers death upon which he shaves his head vowing not to grow it until Ramadhir singh is dead.  Ramadhir Singh establishes himself in the coal mining by misusing his position and power as a trade union leader by turning it into a mafia organisation threatening labourers into giving a substantial portion of their income to Ramaadhir's henchmen. Singh encourages the labourers to dip the coal ores into water to fake production output.  The coal mines are nationalised. A mature Sardar Khan  and his kins start hijacking Ramaadhir's coal trucks mid transit but Ramadhir Singh suspects the foul play on SP Sinha, a Coal India official and murders him. Post Sinha's murder, Ramaadhir starts being feared by everyone in Dhanbad. Sardar marries Nagma Khatoon . The pregnant Khatoon confronts Sardar Khan and a prostitute inside a brothel and chases him away. Later, Nagma gives birth to Danish but gets pregnant immediately afterwards. Unable to have sex with a pregnant Nagma, Sardar confesses his sexual frustrations with his kins. At dinner, Nagma gives her consent to Sardar to sleep with other women but with the condition that he won't bring them home or dishonour the family name. Sardar and Asghar start working for JP Singh. They misuse their employment by secretly selling the company petrol in the black market. Later, they rob a petrol pump and a train bogey belonging to the Singh family. They usurp Singh's land leading which forces the two families to confront each other for talks. The meeting ends in a scuffle with Ramaadhir Singh realising that Sardar Khan is in fact the son of Shahid khan who he had murdered in the late 40s. Sardar and Asghar are sent for jail time for hitting JP Singh.  Sardar and Asghar escape from the jail. While hiding in Wasseyur, Sardar marries a Bengali Hindu girl named Durga . Asghar informs Nagma that Sardar has taken a second wife leaving Nagma helpless at the situation. Meanwhile, Wasseypur has merged with Dhanbad where the Qureshi goons terrorize the non-Qureshi Muslims. The locals approach Sardar Khan for help. During Muharram, both Shias and Sunnis are out mourning including the Qureshi clan. Sardar uses the opportunity to launch a major bomb attack on many Qureshi shops and houses. The word spreads about Sardar's raids and the people start fearing Sardar more than the Qureshis. Eventually, Sardar returns home to Nagma and she gets pregnant again. Sardar tries to initiate sex with a pregnant Nagma but she refuses which prompts an angry Sardar to leave her. He goes to stay with his second wife, Durga where she gives birth to his son, Definite. Ramaadhir Singh, noticing that Sardar has abandoned his first family, tries to reach out through Danish by giving him money. An enraged Nagma beats Danish for taking the money while she breaks down in front of Nasir. A thirsty Faizal wakes up in the middle of the night to find Nagma and Nasir about to have sex. Angry, he storms out of the house and becomes a junkie. Nasir reveals that the desires were never consummated. Faizal and Nasir never see eye to eye again.  Sensing Sardar's increasing clout, Ramaadhir calls his old associate Ehsaan Qureshi who brokers a meeting between Sultan Qureshi and Ramaadhir Singh where the two decide to become allies against their common enemy, Sardar Khan. Sultan asks Ramaadhir for modern automatic weapons which the latter promises to give.  Sardar becomes the most feared man in Wasseypur and shifts his business to stealing iron ore. Danish Khan joins in the family business. A failed attack from Sultan Qureshi leaves Danish with a minor injury and causes reconciliation between Sardar and Nagma. Sardar finds Ramaadhir and warns him of terrible consequences if anything ever happens to his family. A mature Faizal is seriously affected by Bollywood movies as he starts behaving, talking and dressing like Bollywood characters. Faizal is caught by the police for buying guns and jailed. Upon release, he kills the gun-seller who had implicated him to police earlier. Meanwhile, Sardar seizes a lake belonging to a local temple and charges commission on fish sellers who make a catch in that lake. An uneasy peace is maintained between the Qureshi and Khan families when Danish Khan marries Shama Parveen, the sister of Sultan Qureshi. Faizal reveals to a friend that his father Sardar would be travelling without security the next day. Late night, while Faizal is still asleep, his friend who turns out to be a spy calls up the Qureshis and passes this bit of information to them. Sardar leaves home alone and reaches the Durga household to give them their expense allowance. Durga turns out to be yet another Qureshi spy. The Qureshi men follow Sardars car and when the latter stops at a petrol pump to refuel they start shooting while Sardar ducks in the car for cover. The Qureshi men put several close rounds through the car window ensuring a precise & unmistakable hit after which they escape. A shocked Sardar opens the car door and stands up to reveal multiple bullet wounds, one directly on his head. He steps out with his gun drawn trying to locate the shooters but he eventually collapses to his death on a ferry cycle.  \n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "UTNgwBfjPSyc"
      },
      "cell_type": "markdown",
      "source": "# Εφαρμογή 1. Υλοποίηση συστήματος συστάσεων ταινιών βασισμένο στο περιεχόμενο\n<img src=\"http://clture.org/wp-content/uploads/2015/12/Netflix-Streaming-End-of-Year-Posts.jpg\" width=\"50%\">"
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "rnA2RP8GPSyf"
      },
      "cell_type": "markdown",
      "source": "Η πρώτη εφαρμογή που θα αναπτύξετε θα είναι ένα [σύστημα συστάσεων](https://en.wikipedia.org/wiki/Recommender_system) ταινιών βασισμένο στο περιεχόμενο (content based recommender system). Τα συστήματα συστάσεων στοχεύουν στο να προτείνουν αυτόματα στο χρήστη αντικείμενα από μια συλλογή τα οποία ιδανικά θέλουμε να βρει ενδιαφέροντα ο χρήστης. Η κατηγοριοποίηση των συστημάτων συστάσεων βασίζεται στο πώς γίνεται η επιλογή (filtering) των συστηνόμενων αντικειμένων. Οι δύο κύριες κατηγορίες είναι η συνεργατική διήθηση (collaborative filtering) όπου το σύστημα προτείνει στο χρήστη αντικείμενα που έχουν αξιολογηθεί θετικά από χρήστες που έχουν παρόμοιο με αυτόν ιστορικό αξιολογήσεων και η διήθηση με βάση το περιεχόμενο (content based filtering), όπου προτείνονται στο χρήστη αντικείμενα με παρόμοιο περιεχόμενο (με βάση κάποια χαρακτηριστικά) με αυτά που έχει προηγουμένως αξιολογήσει θετικά.\n\nΤο σύστημα συστάσεων που θα αναπτύξετε θα βασίζεται στο **περιεχόμενο** και συγκεκριμένα στις συνόψεις των ταινιών (corpus). \n"
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "DD5KuSKrxQ8I"
      },
      "cell_type": "markdown",
      "source": "## Μετατροπή σε TFIDF\n\nΤο πρώτο βήμα θα είναι λοιπόν να μετατρέψετε το corpus σε αναπαράσταση tf-idf:"
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "s5YP6XCZPSyh",
        "colab": {},
        "trusted": true
      },
      "cell_type": "code",
      "source": "from sklearn.feature_extraction.text import TfidfVectorizer\nvectorizer = TfidfVectorizer()\nvectorizer.fit(corpus)\ncorpus_tf_idf = vectorizer.transform(corpus)",
      "execution_count": 5,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "H-uRZK3EPSyl"
      },
      "cell_type": "markdown",
      "source": "Η συνάρτηση [TfidfVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) όπως καλείται εδώ **δεν είναι βελτιστοποιημένη**. Οι επιλογές των μεθόδων και παραμέτρων της μπορεί να έχουν **δραματική επίδραση στην ποιότητα των συστάσεων** και είναι διαφορετικές για κάθε dataset. Επίσης, οι επιλογές αυτές έχουν πολύ μεγάλη επίδραση και στη **διαστατικότητα και όγκο των δεδομένων**. Η διαστατικότητα των δεδομένων με τη σειρά της θα έχει πολύ μεγάλη επίδραση στους **χρόνους εκπαίδευσης**, ιδιαίτερα στη δεύτερη εφαρμογή της άσκησης. Ανατρέξτε στα notebooks του εργαστηρίου και στο [FAQ](https://docs.google.com/document/d/1jL4gRag_LHbVCYIt5XVJ53iJPb6RZWi02rT5mPXiqEU/edit?usp=sharing) των ασκήσεων.\n"
    },
    {
      "metadata": {
        "id": "y_Cw0brpnisF",
        "colab_type": "code",
        "colab": {},
        "trusted": true
      },
      "cell_type": "code",
      "source": "print(corpus_tf_idf.shape)",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": "(5000, 48684)\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "3LsmvSyVykTU"
      },
      "cell_type": "markdown",
      "source": "## Υλοποίηση του συστήματος συστάσεων\n\nΤο σύστημα συστάσεων που θα παραδώσετε θα είναι μια συνάρτηση `content_recommender` με δύο ορίσματα `target_movie` και `max_recommendations`. Στην `target_movie` περνάμε το ID μιας ταινίας-στόχου για την οποία μας ενδιαφέρει να βρούμε παρόμοιες ως προς το περιεχόμενο (τη σύνοψη) ταινίες, `max_recommendations` στο πλήθος.\nΥλοποιήστε τη συνάρτηση ως εξής: \n- για την ταινία-στόχο, από το `corpus_tf_idf` υπολογίστε την [ομοιότητα συνημιτόνου](https://en.wikipedia.org/wiki/Cosine_similarity) της με όλες τις ταινίες της συλλογής σας\n- με βάση την ομοιότητα συνημιτόνου που υπολογίσατε, δημιουργήστε ταξινομημένο πίνακα από το μεγαλύτερο στο μικρότερο, με τα indices (`ID`) των ταινιών. Παράδειγμα: αν η ταινία με index 1 έχει ομοιότητα συνημιτόνου με 3 ταινίες \\[0.2 1 0.6\\] (έχει ομοιότητα 1 με τον εαύτό της) ο ταξινομημένος αυτός πίνακας indices θα είναι \\[1 2 0\\].\n- Για την ταινία-στόχο εκτυπώστε: id, τίτλο, σύνοψη, κατηγορίες (categories)\n- Για τις `max_recommendations` ταινίες (πλην της ίδιας της ταινίας-στόχου που έχει cosine similarity 1 με τον εαυτό της) με τη μεγαλύτερη ομοιότητα συνημιτόνου (σε φθίνουσα σειρά), τυπώστε σειρά σύστασης (1 πιο κοντινή, 2 η δεύτερη πιο κοντινή κλπ), id, τίτλο, σύνοψη, κατηγορίες (categories)\n"
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "8IvHkTUHyu78"
      },
      "cell_type": "markdown",
      "source": "## Βελτιστοποίηση\n\nΑφού υλοποιήσετε τη συνάρτηση `content_recommender` χρησιμοποιήστε τη για να βελτιστοποιήσετε την `TfidfVectorizer`. Συγκεκριμένα, αρχικά μπορείτε να δείτε τι επιστρέφει το σύστημα για τυχαίες ταινίες-στόχους και για ένα μικρό `max_recommendations` (2 ή 3). Αν σε κάποιες ταινίες το σύστημα μοιάζει να επιστρέφει σημασιολογικά κοντινές ταινίες σημειώστε το `ID` τους. Δοκιμάστε στη συνέχεια να βελτιστοποιήσετε την `TfidfVectorizer` για τα συγκεκριμένα `ID` ώστε να επιστρέφονται σημασιολογικά κοντινές ταινίες για μεγαλύτερο αριθμό `max_recommendations`. Παράλληλα, όσο βελτιστοποιείτε την `TfidfVectorizer`, θα πρέπει να λαμβάνετε καλές συστάσεις για μεγαλύτερο αριθμό τυχαίων ταινιών. Μπορείτε επίσης να βελτιστοποιήσετε τη συνάρτηση παρατηρώντας πολλά φαινόμενα που το σύστημα εκλαμβάνει ως ομοιότητα περιεχομένου ενώ επί της ουσίας δεν είναι επιθυμητό να συνυπολογίζονται (δείτε σχετικά το [FAQ](https://docs.google.com/document/d/1jL4gRag_LHbVCYIt5XVJ53iJPb6RZWi02rT5mPXiqEU/edit?usp=sharing)). Ταυτόχρονα, μια άλλη κατεύθυνση της βελτιστοποίησης είναι να χρησιμοποιείτε τις παραμέτρους του `TfidfVectorizer` έτσι ώστε να μειώνονται οι διαστάσεις του Vector Space Model μέχρι το σημείο που θα αρχίσει να εμφανίζονται επιπτώσεις στην ποιότητα των συστάσεων. \n\n\n"
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "NPVK7Z5c1p5F"
      },
      "cell_type": "markdown",
      "source": "## Επεξήγηση επιλογών και ποιοτική ερμηνεία\n\nΣε markdown περιγράψτε πώς προχωρήσατε στις επιλογές σας για τη βελτιστοποίηση της `TfidfVectorizer`. Επίσης σε markdown δώστε 10 παραδείγματα (IDs) από τη συλλογή σας που επιστρέφουν καλά αποτελέσματα μέχρι `max_recommendations` (5 και παραπάνω) και σημειώστε συνοπτικά ποια είναι η θεματική που ενώνει τις ταινίες.\n\nΔείτε [εδώ](https://pastebin.com/raw/ZEvg5t3z) ένα παράδειγμα εξόδου του βελτιστοποιημένου συστήματος συστάσεων για την ταίνία [\"Q Planes\"](https://en.wikipedia.org/wiki/Q_Planes) με την κλήση της συνάρτησης για κάποιο seed `content_recommender(529,3)`. Είναι φανερό ότι η κοινή θεματική των ταινιών είναι τα αεροπλάνα, οι πτήσεις, οι πιλότοι, ο πόλεμος."
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "4irg4K-IPSym"
      },
      "cell_type": "markdown",
      "source": "## Tip: persistence αντικειμένων με joblib.dump\n\nH βιβλιοθήκη [joblib](https://pypi.python.org/pypi/joblib) της Python δίνει κάποιες εξαιρετικά χρήσιμες ιδιότητες στην ανάπτυξη κώδικα: pipelining, παραλληλισμό, caching και variable persistence. Τις τρεις πρώτες ιδιότητες τις είδαμε στην πρώτη άσκηση. Στην παρούσα άσκηση θα μας φανεί χρήσιμη η τέταρτη, το persistence των αντικειμένων. Συγκεκριμένα μπορούμε με:\n\n```python\nfrom sklearn.externals import joblib  \njoblib.dump(my_object, 'my_object.pkl') \n```\n\nνα αποθηκεύσουμε οποιοδήποτε αντικείμενο-μεταβλητή (εδώ το `my_object`) απευθείας πάνω στο filesystem ως αρχείο, το οποίο στη συνέχεια μπορούμε να ανακαλέσουμε ως εξής:\n\n```python\nmy_object = joblib.load('my_object.pkl')\n```\n\nΜπορούμε έτσι να ανακαλέσουμε μεταβλητές ακόμα και αφού κλείσουμε και ξανανοίξουμε το notebook, χωρίς να χρειαστεί να ακολουθήσουμε ξανά όλα τα βήματα ένα - ένα για την παραγωγή τους, κάτι ιδιαίτερα χρήσιμο αν αυτή η διαδικασία είναι χρονοβόρα. Προσοχή: αυτό ισχύει μόνο στα Azure και Kaggle, στο Colab και στο IBM τα αρχεία εξαφανίζονται όταν ανακυκλώνεται ο πυρήνας και θα πρέπει να τα αποθηκεύετε τοπικά. Περισσότερα στο [FAQ](https://docs.google.com/document/d/1jL4gRag_LHbVCYIt5XVJ53iJPb6RZWi02rT5mPXiqEU/edit?usp=sharing).\n\nΑς αποθηκεύσουμε το `corpus_tf_idf` και στη συνέχεια ας το ανακαλέσουμε."
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "aESOPYQaPSyo",
        "scrolled": true,
        "colab": {},
        "trusted": true
      },
      "cell_type": "code",
      "source": "from sklearn.externals import joblib\njoblib.dump(corpus_tf_idf, 'corpus_tf_idf.pkl') ",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 7,
          "data": {
            "text/plain": "['corpus_tf_idf.pkl']"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "7_rAEj5ZPSy1"
      },
      "cell_type": "markdown",
      "source": "\n\nΜπορείτε με ένα απλό `!ls` να δείτε ότι το αρχείο `corpus_tf_idf.pkl` υπάρχει στο filesystem σας (== persistence):"
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "ZhwXmTEIPSy3",
        "scrolled": true,
        "colab": {},
        "trusted": true
      },
      "cell_type": "code",
      "source": "!ls -lh",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": "total 0\r\n-rw-r--r-- 1 nbuser nbuser 8.9M Jan  4  2019 corpus_tf_idf.pkl\r\n-rw-r--r-- 1 nbuser nbuser    0 Jan  4 16:01 README.md\r\n-rw-r--r-- 1 nbuser nbuser  54K Jan  4 16:07 Άσκηση 2 Εκφώνηση (1).ipynb\r\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "cey5AbkO475S"
      },
      "cell_type": "markdown",
      "source": "και μπορούμε να τα διαβάσουμε με `joblib.load`"
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "DSJPTKY8PSyu",
        "colab": {},
        "trusted": true
      },
      "cell_type": "code",
      "source": "corpus_tf_idf = joblib.load('corpus_tf_idf.pkl')",
      "execution_count": 9,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "zHOQtO83PSy9"
      },
      "cell_type": "markdown",
      "source": "# Εφαρμογή 2.  Σημασιολογική απεικόνιση της συλλογής ταινιών με χρήση SOM\n<img src=\"http://visual-memory.co.uk/daniel/Documents/intgenre/Images/film-genres.jpg\" width=\"35%\">"
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "UB_clmizPSy-"
      },
      "cell_type": "markdown",
      "source": "## Δημιουργία dataset\nΣτη δεύτερη εφαρμογή θα βασιστούμε στις τοπολογικές ιδιότητες των Self Organizing Maps (SOM) για να φτιάξουμε ενά χάρτη (grid) δύο διαστάσεων όπου θα απεικονίζονται όλες οι ταινίες της συλλογής της ομάδας με τρόπο χωρικά συνεκτικό ως προς το περιεχόμενο και κυρίως το είδος τους. \n\nΗ `build_final_set` αρχικά μετατρέπει την αραιή αναπαράσταση tf-idf της εξόδου της `TfidfVectorizer()` σε πυκνή (η [αραιή αναπαράσταση](https://en.wikipedia.org/wiki/Sparse_matrix) έχει τιμές μόνο για τα μη μηδενικά στοιχεία). \n\nΣτη συνέχεια ενώνει την πυκνή `dense_tf_idf` αναπαράσταση και τις binarized κατηγορίες `catbins` των ταινιών ως επιπλέον στήλες (χαρακτηριστικά). Συνεπώς, κάθε ταινία αναπαρίσταται στο Vector Space Model από τα χαρακτηριστικά του TFIDF και τις κατηγορίες της.\n\nΤέλος, δέχεται ένα ορισμα για το πόσες ταινίες να επιστρέψει, με default τιμή όλες τις ταινίες (5000). Αυτό είναι χρήσιμο για να μπορείτε αν θέλετε να φτιάχνετε μικρότερα σύνολα δεδομένων ώστε να εκπαιδεύεται ταχύτερα το SOM.\n\nΣημειώστε ότι το IBM Watson δείνει \"Kernel dead\" εάν δεν έχετε βελτιστοποιήσει το tfidf και μικρύνει τις διαστάσεις του dataset (πιθανότατα κάποια υπέρβαση μνήμης)."
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "U-FDDOkQPSzA",
        "colab": {},
        "trusted": true
      },
      "cell_type": "code",
      "source": "def build_final_set(doc_limit = 5000, tf_idf_only=False):\n    # convert sparse tf_idf to dense tf_idf representation\n    dense_tf_idf = corpus_tf_idf.toarray()[0:doc_limit,:]\n    if tf_idf_only:\n        # use only tf_idf\n        final_set = dense_tf_idf\n    else:\n        # append the binary categories features horizontaly to the (dense) tf_idf features\n        final_set = np.hstack((dense_tf_idf, catbins[0:doc_limit,:]))\n        # η somoclu θέλει δεδομ΄ένα σε float32\n    return np.array(final_set, dtype=np.float32)",
      "execution_count": 10,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "aF1B62UbPSzF",
        "colab": {},
        "trusted": true
      },
      "cell_type": "code",
      "source": "final_set = build_final_set()",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "error",
          "ename": "MemoryError",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-e940e0403bb1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfinal_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_final_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-10-ab310670d2ac>\u001b[0m in \u001b[0;36mbuild_final_set\u001b[0;34m(doc_limit, tf_idf_only)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;31m# append the binary categories features horizontaly to the (dense) tf_idf features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mfinal_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdense_tf_idf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatbins\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mdoc_limit\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0;31m# η somoclu θέλει δεδομ΄ένα σε float32\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3_420/lib/python3.5/site-packages/numpy/core/shape_base.py\u001b[0m in \u001b[0;36mhstack\u001b[0;34m(tup)\u001b[0m\n\u001b[1;32m    286\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_nx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_nx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMemoryError\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "KjvPPENS_dYL"
      },
      "cell_type": "markdown",
      "source": "Τυπώνουμε τις διαστάσεις του τελικού dataset μας. Χωρίς βελτιστοποίηση του TFIDF θα έχουμε περίπου 50.000 χαρακτηριστικά."
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "fvEgNn-L-jEw",
        "colab": {},
        "trusted": true
      },
      "cell_type": "code",
      "source": "final_set.shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "Om7PoyDVCqis"
      },
      "cell_type": "markdown",
      "source": "Με βάση την εμπειρία σας στην προετοιμασί των δεδομένων στην επιβλεπόμενη μάθηση, υπάρχει κάποιο βήμα προεπεξεργασίας που θα μπορούσε να εφαρμοστεί σε αυτό το dataset; "
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "8tikdip0PSzQ"
      },
      "cell_type": "markdown",
      "source": "## Εκπαίδευση χάρτη SOM\n\nΘα δουλέψουμε με τη βιβλιοθήκη SOM [\"Somoclu\"](http://somoclu.readthedocs.io/en/stable/index.html). Εισάγουμε τις somoclu και matplotlib και λέμε στη matplotlib να τυπώνει εντός του notebook (κι όχι σε pop up window)."
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "oX9rzxGSPSzR",
        "colab": {},
        "trusted": true
      },
      "cell_type": "code",
      "source": "# install somoclu\n!pip install --upgrade somoclu\n# import sompoclu, matplotlib\nimport somoclu\nimport matplotlib\n# we will plot inside the notebook and not in separate window\n%matplotlib inline",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "EqBfn0ijPSzX"
      },
      "cell_type": "markdown",
      "source": "Καταρχάς διαβάστε το [function reference](http://somoclu.readthedocs.io/en/stable/reference.html) του somoclu. Θα δoυλέψουμε με χάρτη τύπου planar, παραλληλόγραμμου σχήματος νευρώνων με τυχαία αρχικοποίηση (όλα αυτά είναι default). Μπορείτε να δοκιμάσετε διάφορα μεγέθη χάρτη ωστόσο όσο ο αριθμός των νευρώνων μεγαλώνει, μεγαλώνει και ο χρόνος εκπαίδευσης. Για το training δεν χρειάζεται να ξεπεράσετε τα 100 epochs. Σε γενικές γραμμές μπορούμε να βασιστούμε στις default παραμέτρους μέχρι να έχουμε τη δυνατότητα να οπτικοποιήσουμε και να αναλύσουμε ποιοτικά τα αποτελέσματα. Ξεκινήστε με ένα χάρτη 10 x 10, 100 epochs training και ένα υποσύνολο των ταινιών (π.χ. 2000). Χρησιμοποιήστε την `time` για να έχετε μια εικόνα των χρόνων εκπαίδευσης. Ενδεικτικά, με σωστή κωδικοποίηση tf-idf, μικροί χάρτες για λίγα δεδομένα (1000-2000) παίρνουν γύρω στο ένα λεπτό ενώ μεγαλύτεροι χάρτες με όλα τα δεδομένα μπορούν να πάρουν 10-15 λεπτά ή και περισσότερο.\n"
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "ntd2GE9SaHiS"
      },
      "cell_type": "markdown",
      "source": "\n## Best matching units\n\nΜετά από κάθε εκπαίδευση αποθηκεύστε σε μια μεταβλητή τα best matching units (bmus) για κάθε ταινία. Τα bmus μας δείχνουν σε ποιο νευρώνα ανήκει η κάθε ταινία. Προσοχή: η σύμβαση των συντεταγμένων των νευρώνων είναι (στήλη, γραμμή) δηλαδή το ανάποδο από την Python. Με χρήση της [np.unique](https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.unique.html) (μια πολύ χρήσιμη συνάρτηση στην άσκηση) αποθηκεύστε τα μοναδικά best matching units και τους δείκτες τους (indices) προς τις ταινίες. Σημειώστε ότι μπορεί να έχετε λιγότερα μοναδικά bmus από αριθμό νευρώνων γιατί μπορεί σε κάποιους νευρώνες να μην έχουν ανατεθεί ταινίες. Ως αριθμό νευρώνα θα θεωρήσουμε τον αριθμό γραμμής στον πίνακα μοναδικών bmus.\n"
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "grzqcyHyaKdg"
      },
      "cell_type": "markdown",
      "source": "\n## Ομαδοποίηση (clustering)\n\nΤυπικά, η ομαδοποίηση σε ένα χάρτη SOM προκύπτει από το unified distance matrix (U-matrix): για κάθε κόμβο υπολογίζεται η μέση απόστασή του από τους γειτονικούς κόμβους. Εάν χρησιμοποιηθεί μπλε χρώμα στις περιοχές του χάρτη όπου η τιμή αυτή είναι χαμηλή (μικρή απόσταση) και κόκκινο εκεί που η τιμή είναι υψηλή (μεγάλη απόσταση), τότε μπορούμε να πούμε ότι οι μπλε περιοχές αποτελούν clusters και οι κόκκινες αποτελούν σύνορα μεταξύ clusters.\n\nTo somoclu δίνει την επιπρόσθετη δυνατότητα να κάνουμε ομαδοποίηση των νευρώνων χρησιμοποιώντας οποιονδήποτε αλγόριθμο ομαδοποίησης του scikit-learn. Στην άσκηση θα χρησιμοποιήσουμε τον k-Means. Για τον αρχικό σας χάρτη δοκιμάστε ένα k=20 ή 25. Οι δύο προσεγγίσεις ομαδοποίησης είναι διαφορετικές, οπότε περιμένουμε τα αποτελέσματα να είναι κοντά αλλά όχι τα ίδια.\n"
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "2nupuqcuaMe3"
      },
      "cell_type": "markdown",
      "source": "\n## Αποθήκευση του SOM\n\nΕπειδή η αρχικοποίηση του SOM γίνεται τυχαία και το clustering είναι και αυτό στοχαστική διαδικασία, οι θέσεις και οι ετικέτες των νευρώνων και των clusters θα είναι διαφορετικές κάθε φορά που τρέχετε τον χάρτη, ακόμα και με τις ίδιες παραμέτρους. Για να αποθηκεύσετε ένα συγκεκριμένο som και clustering χρησιμοποιήστε και πάλι την `joblib`. Μετά την ανάκληση ενός SOM θυμηθείτε να ακολουθήσετε τη διαδικασία για τα bmus.\n"
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "ejX0Qs18aRHU"
      },
      "cell_type": "markdown",
      "source": "\n## Οπτικοποίηση U-matrix, clustering και μέγεθος clusters\n\nΓια την εκτύπωση του U-matrix χρησιμοποιήστε τη `view_umatrix` με ορίσματα `bestmatches=True` και `figsize=(15, 15)` ή `figsize=(20, 20)`. Τα διαφορετικά χρώματα που εμφανίζονται στους κόμβους αντιπροσωπεύουν τα διαφορετικά clusters που προκύπτουν από τον k-Means. Μπορείτε να εμφανίσετε τη λεζάντα του U-matrix με το όρισμα `colorbar`. Μην τυπώνετε τις ετικέτες (labels) των δειγμάτων, είναι πολύ μεγάλος ο αριθμός τους.\n\nΓια μια δεύτερη πιο ξεκάθαρη οπτικοποίηση του clustering τυπώστε απευθείας τη μεταβλητή `clusters`.\n\nΤέλος, χρησιμοποιώντας πάλι την `np.unique` (με διαφορετικό όρισμα) και την `np.argsort` (υπάρχουν και άλλοι τρόποι υλοποίησης) εκτυπώστε τις ετικέτες των clusters (αριθμοί από 0 έως k-1) και τον αριθμό των νευρώνων σε κάθε cluster, με φθίνουσα ή αύξουσα σειρά ως προς τον αριθμό των νευρώνων. Ουσιαστικά είναι ένα εργαλείο για να βρίσκετε εύκολα τα μεγάλα και μικρά clusters. \n\nΑκολουθεί ένα μη βελτιστοποιημένο παράδειγμα για τις τρεις προηγούμενες εξόδους:\n\n<img src=\"https://image.ibb.co/i0tsfR/umatrix_s.jpg\" width=\"35%\">\n<img src=\"https://image.ibb.co/nLgHEm/clusters.png\" width=\"35%\">\n\n"
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "fMO_KcQYaTv-"
      },
      "cell_type": "markdown",
      "source": "\n## Σημασιολογική ερμηνεία των clusters\n\nΠροκειμένου να μελετήσουμε τις τοπολογικές ιδιότητες του SOM και το αν έχουν ενσωματώσει σημασιολογική πληροφορία για τις ταινίες διαμέσου της διανυσματικής αναπαράστασης με το tf-idf και των κατηγοριών, χρειαζόμαστε ένα κριτήριο ποιοτικής επισκόπησης των clusters. Θα υλοποιήσουμε το εξής κριτήριο: Λαμβάνουμε όρισμα έναν αριθμό (ετικέτα) cluster. Για το cluster αυτό βρίσκουμε όλους τους νευρώνες που του έχουν ανατεθεί από τον k-Means. Για όλους τους νευρώνες αυτούς βρίσκουμε όλες τις ταινίες που τους έχουν ανατεθεί (για τις οποίες αποτελούν bmus). Για όλες αυτές τις ταινίες τυπώνουμε ταξινομημένη τη συνολική στατιστική όλων των ειδών (κατηγοριών) και τις συχνότητές τους. Αν το cluster διαθέτει καλή συνοχή και εξειδίκευση, θα πρέπει κάποιες κατηγορίες να έχουν σαφώς μεγαλύτερη συχνότητα από τις υπόλοιπες. Θα μπορούμε τότε να αναθέσουμε αυτήν/ές την/τις κατηγορία/ες ως ετικέτες κινηματογραφικού είδους στο cluster.\n\nΜπορείτε να υλοποιήσετε τη συνάρτηση αυτή όπως θέλετε. Μια πιθανή διαδικασία θα μπορούσε να είναι η ακόλουθη:\n\n1. Ορίζουμε συνάρτηση `print_categories_stats` που δέχεται ως είσοδο λίστα με ids ταινιών. Δημιουργούμε μια κενή λίστα συνολικών κατηγοριών. Στη συνέχεια, για κάθε ταινία επεξεργαζόμαστε το string `categories` ως εξής: δημιουργούμε μια λίστα διαχωρίζοντας το string κατάλληλα με την `split` και αφαιρούμε τα whitespaces μεταξύ ετικετών με την `strip`. Προσθέτουμε τη λίστα αυτή στη συνολική λίστα κατηγοριών με την `extend`. Τέλος χρησιμοποιούμε πάλι την `np.unique` για να μετρήσουμε συχνότητα μοναδικών ετικετών κατηγοριών και ταξινομούμε με την `np.argsort`. Τυπώνουμε τις κατηγορίες και τις συχνότητες εμφάνισης ταξινομημένα. Χρήσιμες μπορεί να σας φανούν και οι `np.ravel`, `np.nditer`, `np.array2string` και `zip`.\n\n2. Ορίζουμε τη βασική μας συνάρτηση `print_cluster_neurons_movies_report` που δέχεται ως όρισμα τον αριθμό ενός cluster. Με τη χρήση της `np.where` μπορούμε να βρούμε τις συντεταγμένες των bmus που αντιστοιχούν στο cluster και με την `column_stack` να φτιάξουμε έναν πίνακα bmus για το cluster. Προσοχή στη σειρά (στήλη - σειρά) στον πίνακα bmus. Για κάθε bmu αυτού του πίνακα ελέγχουμε αν υπάρχει στον πίνακα μοναδικών bmus που έχουμε υπολογίσει στην αρχή συνολικά και αν ναι προσθέτουμε το αντίστοιχο index του νευρώνα σε μια λίστα. Χρήσιμες μπορεί να είναι και οι `np.rollaxis`, `np.append`, `np.asscalar`. Επίσης πιθανώς να πρέπει να υλοποιήσετε ένα κριτήριο ομοιότητας μεταξύ ενός bmu και ενός μοναδικού bmu από τον αρχικό πίνακα bmus.\n\n3. Υλοποιούμε μια βοηθητική συνάρτηση `neuron_movies_report`. Λαμβάνει ένα σύνολο νευρώνων από την `print_cluster_neurons_movies_report` και μέσω της `indices` φτιάχνει μια λίστα με το σύνολο ταινιών που ανήκουν σε αυτούς τους νευρώνες. Στο τέλος καλεί με αυτή τη λίστα την `print_categories_stats` που τυπώνει τις στατιστικές των κατηγοριών.\n\nΜπορείτε βέβαια να προσθέσετε οποιαδήποτε επιπλέον έξοδο σας βοηθάει. Μια χρήσιμη έξοδος είναι πόσοι νευρώνες ανήκουν στο cluster και σε πόσους και ποιους από αυτούς έχουν ανατεθεί ταινίες.\n\nΘα επιτελούμε τη σημασιολογική ερμηνεία του χάρτη καλώντας την `print_cluster_neurons_movies_report` με τον αριθμός ενός cluster που μας ενδιαφέρει. \n\nΠαράδειγμα εξόδου για ένα cluster (μη βελτιστοποιημένος χάρτης, ωστόσο βλέπετε ότι οι μεγάλες κατηγορίες έχουν σημασιολογική  συνάφεια):\n\n```\nOverall Cluster Genres stats:  \n[('\"Horror\"', 86), ('\"Science Fiction\"', 24), ('\"B-movie\"', 16), ('\"Monster movie\"', 10), ('\"Creature Film\"', 10), ('\"Indie\"', 9), ('\"Zombie Film\"', 9), ('\"Slasher\"', 8), ('\"World cinema\"', 8), ('\"Sci-Fi Horror\"', 7), ('\"Natural horror films\"', 6), ('\"Supernatural\"', 6), ('\"Thriller\"', 6), ('\"Cult\"', 5), ('\"Black-and-white\"', 5), ('\"Japanese Movies\"', 4), ('\"Short Film\"', 3), ('\"Drama\"', 3), ('\"Psychological thriller\"', 3), ('\"Crime Fiction\"', 3), ('\"Monster\"', 3), ('\"Comedy\"', 2), ('\"Western\"', 2), ('\"Horror Comedy\"', 2), ('\"Archaeology\"', 2), ('\"Alien Film\"', 2), ('\"Teen\"', 2), ('\"Mystery\"', 2), ('\"Adventure\"', 2), ('\"Comedy film\"', 2), ('\"Combat Films\"', 1), ('\"Chinese Movies\"', 1), ('\"Action/Adventure\"', 1), ('\"Gothic Film\"', 1), ('\"Costume drama\"', 1), ('\"Disaster\"', 1), ('\"Docudrama\"', 1), ('\"Film adaptation\"', 1), ('\"Film noir\"', 1), ('\"Parody\"', 1), ('\"Period piece\"', 1), ('\"Action\"', 1)]```\n   "
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "lq4QrImhaa7E"
      },
      "cell_type": "markdown",
      "source": "\n## Tips για το SOM και το clustering\n\n- Για την ομαδοποίηση ένα U-matrix καλό είναι να εμφανίζει και μπλε-πράσινες περιοχές (clusters) και κόκκινες περιοχές (ορίων). Παρατηρήστε ποια σχέση υπάρχει μεταξύ αριθμού ταινιών στο final set, μεγέθους grid και ποιότητας U-matrix.\n- Για το k του k-Means προσπαθήστε να προσεγγίζει σχετικά τα clusters του U-matrix (όπως είπαμε είναι διαφορετικοί μέθοδοι clustering). Μικρός αριθμός k δεν θα σέβεται τα όρια. Μεγάλος αριθμός θα δημιουργεί υπο-clusters εντός των clusters που φαίνονται στο U-matrix. Το τελευταίο δεν είναι απαραίτητα κακό, αλλά μεγαλώνει τον αριθμό clusters που πρέπει να αναλυθούν σημασιολογικά.\n- Σε μικρούς χάρτες και με μικρά final sets δοκιμάστε διαφορετικές παραμέτρους για την εκπαίδευση του SOM. Σημειώστε τυχόν παραμέτρους που επηρεάζουν την ποιότητα του clustering για το dataset σας ώστε να τις εφαρμόσετε στους μεγάλους χάρτες.\n- Κάποια τοπολογικά χαρακτηριστικά εμφανίζονται ήδη σε μικρούς χάρτες. Κάποια άλλα χρειάζονται μεγαλύτερους χάρτες. Δοκιμάστε μεγέθη 20x20, 25x25 ή και 30x30 και αντίστοιχη προσαρμογή των k. Όσο μεγαλώνουν οι χάρτες, μεγαλώνει η ανάλυση του χάρτη αλλά μεγαλώνει και ο αριθμός clusters που πρέπει να αναλυθούν.\n"
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "x4IUl8O8ayVf"
      },
      "cell_type": "markdown",
      "source": "\n\n## Ανάλυση τοπολογικών ιδιοτήτων χάρτη SOM\n\nΜετά το πέρας της εκπαίδευσης και του clustering θα έχετε ένα χάρτη με τοπολογικές ιδιότητες ως προς τα είδη των ταίνιών της συλλογής σας, κάτι αντίστοιχο με την εικόνα στην αρχή της Εφαρμογής 2 αυτού του notebook (η συγκεκριμένη εικόνα είναι μόνο για εικονογράφιση, δεν έχει καμία σχέση με τη συλλογή δεδομένων και τις κατηγορίες μας).\n\nΓια τον τελικό χάρτη SOM που θα παράξετε για τη συλλογή σας, αναλύστε σε markdown με συγκεκριμένη αναφορά σε αριθμούς clusters και τη σημασιολογική ερμηνεία τους τις εξής τρεις τοπολογικές ιδιότητες του SOM: \n\n1. Δεδομένα που έχουν μεγαλύτερη πυκνότητα πιθανότητας στο χώρο εισόδου τείνουν να απεικονίζονται με περισσότερους νευρώνες στο χώρο μειωμένης διαστατικότητας. Δώστε παραδείγματα από συχνές και λιγότερο συχνές κατηγορίες ταινιών. Χρησιμοποιήστε τις στατιστικές των κατηγοριών στη συλλογή σας και τον αριθμό κόμβων που χαρακτηρίζουν.\n2. Μακρινά πρότυπα εισόδου τείνουν να απεικονίζονται απομακρυσμένα στο χάρτη. Υπάρχουν χαρακτηριστικές κατηγορίες ταινιών που ήδη από μικρούς χάρτες τείνουν να τοποθετούνται σε διαφορετικά ή απομονωμένα σημεία του χάρτη.\n3. Κοντινά πρότυπα εισόδου τείνουν να απεικονίζονται κοντά στο χάρτη. Σε μεγάλους χάρτες εντοπίστε είδη ταινιών και κοντινά τους υποείδη.\n\nΠροφανώς τοποθέτηση σε 2 διαστάσεις που να σέβεται μια απόλυτη τοπολογία δεν είναι εφικτή, αφενός γιατί δεν υπάρχει κάποια απόλυτη εξ ορισμού για τα κινηματογραφικά είδη ακόμα και σε πολλές διαστάσεις, αφετέρου γιατί πραγματοποιούμε μείωση διαστατικότητας.\n\nΕντοπίστε μεγάλα clusters και μικρά clusters που δεν έχουν σαφή χαρακτηριστικά. Εντοπίστε clusters συγκεκριμένων ειδών που μοιάζουν να μην έχουν τοπολογική συνάφεια με γύρω περιοχές. Προτείνετε πιθανές ερμηνείες.\n\n\n\nΤέλος, εντοπίστε clusters που έχουν κατά την άποψή σας ιδιαίτερο ενδιαφέρον στη συλλογή της ομάδας σας (data exploration / discovery value) και σχολιάστε.\n"
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "tYjxGR5DawIy"
      },
      "cell_type": "markdown",
      "source": "\n# Τελική παράδοση άσκησης\n\n- Θα παραδώσετε στο mycourses το παρόν notebook επεξεργασμένο με τις απαντήσεις σας για τα ζητούμενα και των δύο εφαρμογών. \n- Προσέξτε ώστε να **απαντήσετε σε όλα τα σημεία όπου απαιτείται**.\n- Μαζί Θα παραδώσετε τα joblib dumps των τελικών `corpus_tf_idf.pkl` και `som.pkl` (μεγάλος χάρτης - 5000 ταινίες) που θα χρησιμοποιήσετε. Θυμηθείτε ότι η ανάλυση του χάρτη στο markdown με αναφορά σε αριθμούς clusters πρέπει να αναφέρεται στο dump του χάρτη που θα μας παραδώσετε αλλιώς ο χάρτης που θα προκύψει θα είναι διαφορετικός και τα labels των clusters δεν θα αντιστοιχούν στην ανάλυσή σας. Οδηγίες για την αποθήκευση των dumps θα βρείτε στο [FAQ](https://docs.google.com/document/d/1jL4gRag_LHbVCYIt5XVJ53iJPb6RZWi02rT5mPXiqEU/edit?usp=sharing). \n- Μην ξεχάσετε στην αρχή ένα κελί markdown με **τα στοιχεία της ομάδας σας**.\n- Στο **zip** που θα παραδώσετε πρέπει να βρίσκονται **4 αρχεία (το .ipynb και το .py του notebook καθώς και τα δύο .pkl)**\n- **Το maximum upload filesize στο mycourses είναι 29MB**, εάν το zip σας είναι μεγαλύτερο, δεν έχετε βελτιστοποιήσει το tfidf."
    },
    {
      "metadata": {
        "id": "UHhCkvxjnitd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": "<table>\n  <tr>\n    <td bgcolor=\"#FCF8E3\"><font size=\"4\">ΞΑΝΑ: Παρακαλούμε διατρέξτε βήμα-βήμα το notebook για να μην ξεχάσετε παραδοτέα!!</font>\n</td>\n  </tr>\n</table>\n"
    },
    {
      "metadata": {
        "id": "a-FvmQai5S9m",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": "## Ημερομηνία παράδοσης ##\n\nΠέμπτη 17 Ιανουαρίου 2019 αυστηρά (παρακαλούμε όχι αιτήματα για παράταση).\n\nΚαλή επιτυχία!\n"
    }
  ],
  "metadata": {
    "colab": {
      "name": "Άσκηση 2 Εκφώνηση.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}